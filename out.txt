diff --git a/pkg/base/config.go b/pkg/base/config.go
index 0971e25c09f..c01bc6e4494 100644
--- a/pkg/base/config.go
+++ b/pkg/base/config.go
@@ -146,7 +146,7 @@ var (
 	// both a network roundtrip and a TCP retransmit, but we don't need to
 	// tolerate more than 1 retransmit per connection attempt, so
 	// 2 * NetworkTimeout is sufficient.
-	DialTimeout = envutil.EnvOrDefaultDuration("COCKROACH_RPC_DIAL_TIMEOUT", 2*NetworkTimeout)
+	DialTimeout = 2 * NetworkTimeout
 
 	// PingInterval is the interval between network heartbeat pings. It is used
 	// both for RPC heartbeat intervals and gRPC server keepalive pings. It is
@@ -212,8 +212,7 @@ var (
 	// heartbeats and reduce this to NetworkTimeout (plus DialTimeout for the
 	// initial heartbeat), see:
 	// https://github.com/cockroachdb/cockroach/issues/93397.
-	DefaultRPCHeartbeatTimeout = envutil.EnvOrDefaultDuration(
-		"COCKROACH_RPC_HEARTBEAT_TIMEOUT", 3*NetworkTimeout)
+	DefaultRPCHeartbeatTimeout = 3 * NetworkTimeout
 
 	// defaultRaftTickInterval is the default resolution of the Raft timer.
 	defaultRaftTickInterval = envutil.EnvOrDefaultDuration(
diff --git a/pkg/ccl/backupccl/full_cluster_backup_restore_test.go b/pkg/ccl/backupccl/full_cluster_backup_restore_test.go
index 7ccc2d0f7c5..8314e49698f 100644
--- a/pkg/ccl/backupccl/full_cluster_backup_restore_test.go
+++ b/pkg/ccl/backupccl/full_cluster_backup_restore_test.go
@@ -132,23 +132,22 @@ CREATE TABLE data2.foo (a int);
 
 	// Setup the system systemTablesToVerify to ensure that they are copied to the new cluster.
 	// Populate system.users.
-	numBatches := 100
+	numUsers := 1000
 	if util.RaceEnabled {
-		numBatches = 1
+		numUsers = 10
 	}
-	usersPerBatch := 10
-	userID := 0
-	for b := 0; b < numBatches; b++ {
-		sqlDB.RunWithRetriableTxn(t, func(txn *gosql.Tx) error {
-			for u := 0; u < usersPerBatch; u++ {
-				if _, err := txn.Exec(fmt.Sprintf("CREATE USER maxroach%d WITH CREATEDB", userID)); err != nil {
-					return err
-				}
-				userID++
+
+	sqlDB.RunWithRetriableTxn(t, func(txn *gosql.Tx) error {
+		for i := 0; i < numUsers; i++ {
+			if _, err := txn.Exec(fmt.Sprintf("CREATE USER maxroach%d", i)); err != nil {
+				return err
 			}
-			return nil
-		})
-	}
+			if _, err := txn.Exec(fmt.Sprintf("ALTER USER maxroach%d CREATEDB", i)); err != nil {
+				return err
+			}
+		}
+		return nil
+	})
 
 	// Populate system.zones.
 	sqlDB.Exec(t, `ALTER TABLE data.bank CONFIGURE ZONE USING gc.ttlseconds = 3600`)
@@ -609,7 +608,7 @@ func TestClusterRestoreFailCleanup(t *testing.T) {
 	defer leaktest.AfterTest(t)()
 	defer log.Scope(t).Close(t)
 
-	skip.UnderStressRace(t, "too slow under stress race")
+	skip.UnderRace(t, "takes >1 min under race")
 	params := base.TestServerArgs{}
 	// Disable GC job so that the final check of crdb_internal.tables is
 	// guaranteed to not be cleaned up. Although this was never observed by a
@@ -632,23 +631,14 @@ func TestClusterRestoreFailCleanup(t *testing.T) {
 
 	// Setup the system systemTablesToVerify to ensure that they are copied to the new cluster.
 	// Populate system.users.
-	numBatches := 100
-	if util.RaceEnabled {
-		numBatches = 1
-	}
-	usersPerBatch := 10
-	userID := 0
-	for b := 0; b < numBatches; b++ {
-		sqlDB.RunWithRetriableTxn(t, func(txn *gosql.Tx) error {
-			for u := 0; u < usersPerBatch; u++ {
-				if _, err := txn.Exec(fmt.Sprintf("CREATE USER maxroach%d", userID)); err != nil {
-					return err
-				}
-				userID++
+	sqlDB.RunWithRetriableTxn(t, func(txn *gosql.Tx) error {
+		for i := 0; i < 1000; i++ {
+			if _, err := txn.Exec(fmt.Sprintf("CREATE USER maxroach%d", i)); err != nil {
+				return err
 			}
-			return nil
-		})
-	}
+		}
+		return nil
+	})
 	sqlDB.Exec(t, `BACKUP TO 'nodelocal://1/missing-ssts'`)
 
 	// Bugger the backup by removing the SST files. (Note this messes up all of
diff --git a/pkg/ccl/sqlproxyccl/proxy_handler_test.go b/pkg/ccl/sqlproxyccl/proxy_handler_test.go
index 1811d133c64..f8b737c2838 100644
--- a/pkg/ccl/sqlproxyccl/proxy_handler_test.go
+++ b/pkg/ccl/sqlproxyccl/proxy_handler_test.go
@@ -1004,16 +1004,8 @@ func TestInsecureProxy(t *testing.T) {
 	te.TestConnect(ctx, t, url, func(conn *pgx.Conn) {
 		require.NoError(t, runTestQuery(ctx, conn))
 	})
-	testutils.SucceedsSoon(t, func() error {
-		if s.metrics.AuthFailedCount.Count() != 1 ||
-			s.metrics.SuccessfulConnCount.Count() != 1 {
-			return errors.Newf("expected metrics to update, got: "+
-				"AuthFailedCount=%d, SuccessfulConnCount=%d",
-				s.metrics.AuthFailedCount.Count(), s.metrics.SuccessfulConnCount.Count(),
-			)
-		}
-		return nil
-	})
+	require.Equal(t, int64(1), s.metrics.AuthFailedCount.Count())
+	require.Equal(t, int64(1), s.metrics.SuccessfulConnCount.Count())
 	count, _ := s.metrics.ConnectionLatency.Total()
 	require.Equal(t, int64(1), count)
 }
diff --git a/pkg/ccl/streamingccl/streamproducer/replication_stream_test.go b/pkg/ccl/streamingccl/streamproducer/replication_stream_test.go
index 8339ab69df9..995694df8eb 100644
--- a/pkg/ccl/streamingccl/streamproducer/replication_stream_test.go
+++ b/pkg/ccl/streamingccl/streamproducer/replication_stream_test.go
@@ -942,7 +942,6 @@ USE d;`)
 		require.True(t, source.mu.rows.Next())
 		source.mu.codec.decode()
 		if codec.e.Batch != nil {
-			require.Greater(t, 0, len(codec.e.Batch.SpanConfigs), "a non empty batch had zero span config updates")
 			for _, cfg := range codec.e.Batch.SpanConfigs {
 				if receivedSpanConfigs.maybeAddNewRecord(cfg.SpanConfig, cfg.Timestamp.WallTime) {
 					updateCount++
diff --git a/pkg/ccl/streamingccl/streamproducer/span_config_event_stream.go b/pkg/ccl/streamingccl/streamproducer/span_config_event_stream.go
index 57900ee126b..52005686cd7 100644
--- a/pkg/ccl/streamingccl/streamproducer/span_config_event_stream.go
+++ b/pkg/ccl/streamingccl/streamproducer/span_config_event_stream.go
@@ -256,6 +256,7 @@ func (s *spanConfigEventStream) streamLoop(ctx context.Context) error {
 				if err := s.flushEvent(ctx, &streampb.StreamEvent{Checkpoint: &frontier.checkpoint}); err != nil {
 					return err
 				}
+
 				batcher.reset()
 			}
 		}
diff --git a/pkg/cmd/roachtest/roachtestutil/mixedversion/helper.go b/pkg/cmd/roachtest/roachtestutil/mixedversion/helper.go
index f8d5b18e47a..38e2c499074 100644
--- a/pkg/cmd/roachtest/roachtestutil/mixedversion/helper.go
+++ b/pkg/cmd/roachtest/roachtestutil/mixedversion/helper.go
@@ -20,9 +20,7 @@ import (
 	"sync/atomic"
 
 	"github.com/cockroachdb/cockroach/pkg/cmd/roachtest/option"
-	"github.com/cockroachdb/cockroach/pkg/cmd/roachtest/roachtestutil/clusterupgrade"
 	"github.com/cockroachdb/cockroach/pkg/roachprod/logger"
-	"github.com/cockroachdb/cockroach/pkg/util/version"
 )
 
 func (h *Helper) RandomNode(prng *rand.Rand, nodes option.NodeListOption) int {
@@ -121,34 +119,6 @@ func (h *Helper) ExpectDeaths(n int) {
 	h.runner.monitor.ExpectDeaths(n)
 }
 
-// LowestBinaryVersion returns a parsed `version.Version` object
-// corresponding to the lowest binary version used in the current
-// upgrade. The {Major, Minor} information in the version returned
-// provides a lower bound on the cluster version active when this
-// function is called. Test authors can use this information to
-// determine whether a certain feature is available.
-func (h *Helper) LowestBinaryVersion() *version.Version {
-	tc := h.Context()
-
-	var lowestVersion string
-	if tc.FromVersion == clusterupgrade.MainVersion {
-		lowestVersion = tc.ToVersion
-	} else if tc.ToVersion == clusterupgrade.MainVersion {
-		lowestVersion = tc.FromVersion
-	} else {
-		fromVersion := version.MustParse("v" + tc.FromVersion)
-		toVersion := version.MustParse("v" + tc.ToVersion)
-
-		if fromVersion.Compare(toVersion) < 0 {
-			lowestVersion = tc.FromVersion
-		} else {
-			lowestVersion = tc.ToVersion
-		}
-	}
-
-	return version.MustParse("v" + lowestVersion)
-}
-
 // loggerFor creates a logger instance to be used by background
 // functions (created by calling `Background` on the helper
 // instance). It is similar to the logger instances created for
diff --git a/pkg/cmd/roachtest/roachtestutil/mixedversion/mixedversion.go b/pkg/cmd/roachtest/roachtestutil/mixedversion/mixedversion.go
index 96c21d9e60f..f0e34946862 100644
--- a/pkg/cmd/roachtest/roachtestutil/mixedversion/mixedversion.go
+++ b/pkg/cmd/roachtest/roachtestutil/mixedversion/mixedversion.go
@@ -107,12 +107,6 @@ const (
 	// finalized.
 	runWhileMigratingProbability = 0.5
 
-	// rollbackIntermediateUpgradesProbability is the probability that
-	// an "intermediate" upgrade (i.e., an upgrade to a version older
-	// than the one being tested) will also go through a rollback during
-	// a test run.
-	rollbackIntermediateUpgradesProbability = 0.3
-
 	// numNodesInFixtures is the number of nodes expected to exist in a
 	// cluster that can use the test fixtures in
 	// `pkg/cmd/roachtest/fixtures`.
@@ -138,8 +132,6 @@ var (
 		// detect bugs, especially in migrations.
 		useFixturesProbability: 0.7,
 		upgradeTimeout:         clusterupgrade.DefaultUpgradeTimeout,
-		minUpgrades:            1,
-		maxUpgrades:            3,
 	}
 )
 
@@ -238,8 +230,6 @@ type (
 	testOptions struct {
 		useFixturesProbability float64
 		upgradeTimeout         time.Duration
-		minUpgrades            int
-		maxUpgrades            int
 	}
 
 	customOption func(*testOptions)
@@ -275,7 +265,7 @@ type (
 		_buildVersion *version.Version
 		// test-only field, allows us to have deterministic tests even as
 		// the predecessor data changes.
-		predecessorFunc func(*rand.Rand, *version.Version, int) ([]string, error)
+		predecessorFunc func(*rand.Rand, *version.Version) (string, error)
 	}
 
 	shouldStop chan struct{}
@@ -311,31 +301,6 @@ func UpgradeTimeout(timeout time.Duration) customOption {
 	}
 }
 
-// MinUpgrades allows callers to set a minimum number of upgrades each
-// test run should exercise.
-func MinUpgrades(n int) customOption {
-	return func(opts *testOptions) {
-		opts.minUpgrades = n
-	}
-}
-
-// MaxUpgrades allows callers to set a maximum number of upgrades to
-// be performed during a test run.
-func MaxUpgrades(n int) customOption {
-	return func(opts *testOptions) {
-		opts.maxUpgrades = n
-	}
-}
-
-// NumUpgrades allows callers to specify the exact number of upgrades
-// every test run should perform.
-func NumUpgrades(n int) customOption {
-	return func(opts *testOptions) {
-		opts.minUpgrades = n
-		opts.maxUpgrades = n
-	}
-}
-
 // NewTest creates a Test struct that users can use to create and run
 // a mixed-version roachtest.
 func NewTest(
@@ -371,7 +336,7 @@ func NewTest(
 		prng:            prng,
 		seed:            seed,
 		hooks:           &testHooks{prng: prng, crdbNodes: crdbNodes},
-		predecessorFunc: release.RandomPredecessorHistory,
+		predecessorFunc: release.RandomPredecessor,
 	}
 
 	assertValidTest(test, t.Fatal)
@@ -521,19 +486,19 @@ func (t *Test) run(plan *TestPlan) error {
 }
 
 func (t *Test) plan() (*TestPlan, error) {
-	previousReleases, err := t.predecessorFunc(t.prng, t.buildVersion(), t.numUpgrades())
+	previousRelease, err := t.predecessorFunc(t.prng, t.buildVersion())
 	if err != nil {
 		return nil, err
 	}
 
 	planner := testPlanner{
-		versions:  append(previousReleases, clusterupgrade.MainVersion),
-		options:   t.options,
-		rt:        t.rt,
-		crdbNodes: t.crdbNodes,
-		hooks:     t.hooks,
-		prng:      t.prng,
-		bgChans:   t.bgChans,
+		initialVersion: previousRelease,
+		options:        t.options,
+		rt:             t.rt,
+		crdbNodes:      t.crdbNodes,
+		hooks:          t.hooks,
+		prng:           t.prng,
+		bgChans:        t.bgChans,
 	}
 
 	return planner.Plan(), nil
@@ -554,15 +519,6 @@ func (t *Test) runCommandFunc(nodes option.NodeListOption, cmd string) userFunc
 	}
 }
 
-// numUpgrades returns the number of upgrades that will be performed
-// in this test run. Returns a number in the [minUpgrades, maxUpgrades]
-// range.
-func (t *Test) numUpgrades() int {
-	return t.prng.Intn(
-		t.options.maxUpgrades-t.options.minUpgrades+1,
-	) + t.options.minUpgrades
-}
-
 // installFixturesStep is the step that copies the fixtures from
 // `pkg/cmd/roachtest/fixtures` for a specific version into the nodes'
 // store dir.
@@ -576,7 +532,7 @@ func (s installFixturesStep) ID() int                { return s.id }
 func (s installFixturesStep) Background() shouldStop { return nil }
 
 func (s installFixturesStep) Description() string {
-	return fmt.Sprintf("install fixtures for version %q", s.version)
+	return fmt.Sprintf("installing fixtures for version %q", s.version)
 }
 
 func (s installFixturesStep) Run(
@@ -598,7 +554,7 @@ func (s startStep) ID() int                { return s.id }
 func (s startStep) Background() shouldStop { return nil }
 
 func (s startStep) Description() string {
-	return fmt.Sprintf("start cluster at version %q", s.version)
+	return fmt.Sprintf("starting cluster at version %q", s.version)
 }
 
 // Run uploads the binary associated with the given version and starts
@@ -656,7 +612,7 @@ func (s preserveDowngradeOptionStep) ID() int                { return s.id }
 func (s preserveDowngradeOptionStep) Background() shouldStop { return nil }
 
 func (s preserveDowngradeOptionStep) Description() string {
-	return "prevent auto-upgrades by setting `preserve_downgrade_option`"
+	return "preventing auto-upgrades by setting `preserve_downgrade_option`"
 }
 
 func (s preserveDowngradeOptionStep) Run(
@@ -923,12 +879,4 @@ func assertValidTest(test *Test, fatalFunc func(...interface{})) {
 		)
 		fatalFunc(errors.Wrap(err, "mixedversion.NewTest"))
 	}
-
-	if test.options.minUpgrades > test.options.maxUpgrades {
-		err := fmt.Errorf(
-			"invalid test options: maxUpgrades (%d) must be greater than minUpgrades (%d)",
-			test.options.maxUpgrades, test.options.minUpgrades,
-		)
-		fatalFunc(errors.Wrap(err, "mixedversion.NewTest"))
-	}
 }
diff --git a/pkg/cmd/roachtest/roachtestutil/mixedversion/mixedversion_test.go b/pkg/cmd/roachtest/roachtestutil/mixedversion/mixedversion_test.go
index e7717472501..87e5d37bb82 100644
--- a/pkg/cmd/roachtest/roachtestutil/mixedversion/mixedversion_test.go
+++ b/pkg/cmd/roachtest/roachtestutil/mixedversion/mixedversion_test.go
@@ -30,10 +30,9 @@ func Test_assertValidTest(t *testing.T) {
 		}
 	}
 
-	// Validating that number of nodes matches what is encoded in the
-	// fixtures if using them.
 	notEnoughNodes := option.NodeListOption{1, 2, 3}
 	tooManyNodes := option.NodeListOption{1, 2, 3, 5, 6}
+
 	for _, crdbNodes := range []option.NodeListOption{notEnoughNodes, tooManyNodes} {
 		mvt := newTest()
 		mvt.crdbNodes = crdbNodes
@@ -48,15 +47,4 @@ func Test_assertValidTest(t *testing.T) {
 		assertValidTest(mvt, fatalFunc())
 		require.NoError(t, fatalErr)
 	}
-
-	// Validating number of upgrades specified by the test.
-	mvt := newTest(MinUpgrades(10))
-	assertValidTest(mvt, fatalFunc())
-	require.Error(t, fatalErr)
-	require.Contains(t, fatalErr.Error(), "mixedversion.NewTest: invalid test options: maxUpgrades (3) must be greater than minUpgrades (10)")
-
-	mvt = newTest(MaxUpgrades(0))
-	assertValidTest(mvt, fatalFunc())
-	require.Error(t, fatalErr)
-	require.Contains(t, fatalErr.Error(), "mixedversion.NewTest: invalid test options: maxUpgrades (0) must be greater than minUpgrades (1)")
 }
diff --git a/pkg/cmd/roachtest/roachtestutil/mixedversion/planner.go b/pkg/cmd/roachtest/roachtestutil/mixedversion/planner.go
index 4700c727e82..7ad48a9d38d 100644
--- a/pkg/cmd/roachtest/roachtestutil/mixedversion/planner.go
+++ b/pkg/cmd/roachtest/roachtestutil/mixedversion/planner.go
@@ -21,12 +21,14 @@ import (
 )
 
 type (
-	// TestPlan is the output of planning a mixed-version test. The list
-	// of `versions` are just for presentation purposes, as the plan is
-	// defined by the sequence of steps that it contains.
+	// TestPlan is the output of planning a mixed-version test. The
+	// initialVersion and finalVersion fields are just for presentation
+	// purposes, as the plan is defined by the sequence of steps that it
+	// contains.
 	TestPlan struct {
-		versions       []string
-		startClusterID int // step ID after which the cluster should be ready to receive connections
+		initialVersion string
+		finalVersion   string
+		startClusterID int
 		steps          []testStep
 	}
 
@@ -35,7 +37,7 @@ type (
 	testPlanner struct {
 		stepCount      int
 		startClusterID int
-		versions       []string
+		initialVersion string
 		crdbNodes      option.NodeListOption
 		rt             test.Test
 		options        testOptions
@@ -52,129 +54,99 @@ const (
 	lastBranchPadding  = "   "
 )
 
-// Plan returns the TestPlan used to upgrade the cluster from the
-// first to the final version in the `versions` field. The test plan
-// roughly translates to the sequence of steps below:
+// Plan returns the TestPlan generated using the `prng` in the
+// testPlanner field. Currently, the test will always follow the
+// following high level outline:
 //
-//  1. start all nodes in the cluster at the initial version, maybe
-//     using fixtures.
-//  2. run startup hooks.
-//  3. for each cluster upgrade:
-//     - set `preserve_downgrade_option`.
-//     - upgrade all nodes to the next cockroach version (running
+//   - start all nodes in the cluster from a random predecessor version,
+//     maybe using fixtures.
+//   - set `preserve_downgrade_option`.
+//   - run startup hooks.
+//   - upgrade all nodes to the current cockroach version (running
 //     mixed-version hooks at times determined by the planner).
-//     - maybe downgrade all nodes back to the previous version
-//     (running mixed-version hooks again).
-//     - if a rollback was performed, upgrade all nodes back to the
-//     next version one more time (running mixed-version hooks).
-//     - reset `preserve_downgrade_option`, allowing the cluster
-//     to upgrade. Mixed-version hooks may be executed while
+//   - downgrade all nodes back to the predecessor version (running
+//     mixed-version hooks again).
+//   - upgrade all nodes back to the current cockroach version one
+//     more time (running mixed-version hooks).
+//   - finally, reset `preserve_downgrade_option`, allowing the
+//     cluster to upgrade. Mixed-version hooks may be executed while
 //     this is happening.
-//     - run after-upgrade hooks.
+//   - run after-test hooks.
+//
+// TODO(renato): further opportunities for random exploration:
+// - going back multiple releases instead of just one
+// - inserting arbitrary delays (`sleep` calls) during the test.
 func (p *testPlanner) Plan() *TestPlan {
 	var steps []testStep
+	addSteps := func(ss []testStep) { steps = append(steps, ss...) }
 
-	steps = append(steps, p.testSetupSteps()...)
-	steps = append(steps, p.hooks.BackgroundSteps(p.nextID, p.longRunningContext(), p.bgChans)...)
-
-	for prevVersionIdx := 0; prevVersionIdx+1 < len(p.versions); prevVersionIdx++ {
-		fromVersion := p.versions[prevVersionIdx]
-		toVersion := p.versions[prevVersionIdx+1]
-
-		upgradeStep := sequentialRunStep{
-			label: fmt.Sprintf("upgrade cluster from %q to %q", versionMsg(fromVersion), versionMsg(toVersion)),
-		}
-		addUpgradeSteps := func(ss []testStep) {
-			upgradeStep.steps = append(upgradeStep.steps, ss...)
-		}
+	addSteps(p.initSteps())
+	addSteps(p.hooks.BackgroundSteps(p.nextID, p.initialContext(), p.bgChans))
 
-		addUpgradeSteps(p.initUpgradeSteps(fromVersion, toVersion))
-
-		// previous -> next
-		addUpgradeSteps(p.upgradeSteps(fromVersion, toVersion))
-		if p.shouldRollback(toVersion) {
-			// next -> previous (rollback)
-			addUpgradeSteps(p.downgradeSteps(toVersion, fromVersion))
-
-			// previous -> current
-			addUpgradeSteps(p.upgradeSteps(fromVersion, toVersion))
-		}
-
-		// finalize
-		addUpgradeSteps(p.finalizeUpgradeSteps(fromVersion, toVersion))
-
-		// wait for upgrade to finalize
-		addUpgradeSteps(p.finalUpgradeSteps(fromVersion, toVersion))
-
-		steps = append(steps, upgradeStep)
-	}
+	// previous -> current
+	addSteps(p.upgradeSteps(p.initialVersion, clusterupgrade.MainVersion))
+	// current -> previous (rollback)
+	addSteps(p.downgradeSteps(clusterupgrade.MainVersion, p.initialVersion))
+	// previous -> current
+	addSteps(p.upgradeSteps(p.initialVersion, clusterupgrade.MainVersion))
+	// finalize
+	addSteps(p.finalizeUpgradeSteps())
 
+	addSteps(p.finalSteps())
 	return &TestPlan{
-		versions:       p.versions,
+		initialVersion: p.initialVersion,
+		finalVersion:   versionMsg(clusterupgrade.MainVersion),
 		startClusterID: p.startClusterID,
 		steps:          steps,
 	}
 }
 
-func (p *testPlanner) finalContext(fromVersion, toVersion string, finalizing bool) Context {
+func (p *testPlanner) initialContext() Context {
 	return Context{
-		FromVersion:    fromVersion,
-		ToVersion:      toVersion,
-		ToVersionNodes: p.crdbNodes,
-		Finalizing:     finalizing,
+		FromVersion:      p.initialVersion,
+		ToVersion:        clusterupgrade.MainVersion,
+		FromVersionNodes: p.crdbNodes,
 	}
 }
 
-// longRunningContext is the test context passed to long running tasks
-// (background functions and the like). In these scenarios,
-// `FromVersion` and `ToVersion` correspond to, respectively, the
-// initial version the cluster is started at, and the final version
-// once the test finishes.
-func (p *testPlanner) longRunningContext() Context {
+func (p *testPlanner) finalContext(finalizing bool) Context {
 	return Context{
-		FromVersion: p.versions[0],
-		ToVersion:   p.versions[len(p.versions)-1],
-		Finalizing:  false,
+		FromVersion:    p.initialVersion,
+		ToVersion:      clusterupgrade.MainVersion,
+		ToVersionNodes: p.crdbNodes,
+		Finalizing:     finalizing,
 	}
 }
 
-func (p *testPlanner) testSetupSteps() []testStep {
-	initialVersion := p.versions[0]
-
+// initSteps returns the sequence of steps that should be executed
+// before we start changing binaries on nodes in the process of
+// upgrading/downgrading. It will also run any startup hooks the user
+// may have provided.
+func (p *testPlanner) initSteps() []testStep {
 	var steps []testStep
 	if p.prng.Float64() < p.options.useFixturesProbability {
-		steps = []testStep{installFixturesStep{id: p.nextID(), version: initialVersion, crdbNodes: p.crdbNodes}}
+		steps = []testStep{installFixturesStep{id: p.nextID(), version: p.initialVersion, crdbNodes: p.crdbNodes}}
 	}
-
 	p.startClusterID = p.nextID()
-	steps = append(steps,
-		startStep{id: p.startClusterID, version: initialVersion, rt: p.rt, crdbNodes: p.crdbNodes},
-		waitForStableClusterVersionStep{id: p.nextID(), nodes: p.crdbNodes, timeout: p.options.upgradeTimeout},
-	)
+	steps = append(steps, startStep{id: p.startClusterID, version: p.initialVersion, rt: p.rt, crdbNodes: p.crdbNodes})
 
 	return append(
-		steps,
-		p.hooks.StartupSteps(p.nextID, p.longRunningContext())...,
+		append(steps,
+			waitForStableClusterVersionStep{id: p.nextID(), nodes: p.crdbNodes, timeout: p.options.upgradeTimeout},
+			preserveDowngradeOptionStep{id: p.nextID(), prng: p.newRNG(), crdbNodes: p.crdbNodes},
+		),
+		p.hooks.StartupSteps(p.nextID, p.initialContext())...,
 	)
 }
 
-// initUpgradeSteps returns the sequence of steps that should be
-// executed before we start changing binaries on nodes in the process
-// of upgrading/downgrading.
-func (p *testPlanner) initUpgradeSteps(fromVersion, toVersion string) []testStep {
-	return []testStep{
-		preserveDowngradeOptionStep{id: p.nextID(), prng: p.newRNG(), crdbNodes: p.crdbNodes},
-	}
-}
-
-// finalUpgradeSteps are the steps to be run once the nodes have been
+// finalSteps are the steps to be run once the nodes have been
 // upgraded/downgraded. It will wait for the cluster version on all
 // nodes to be the same and then run any after-finalization hooks the
 // user may have provided.
-func (p *testPlanner) finalUpgradeSteps(fromVersion, toVersion string) []testStep {
+func (p *testPlanner) finalSteps() []testStep {
 	return append([]testStep{
 		waitForStableClusterVersionStep{id: p.nextID(), nodes: p.crdbNodes, timeout: p.options.upgradeTimeout},
-	}, p.hooks.AfterUpgradeFinalizedSteps(p.nextID, p.finalContext(fromVersion, toVersion, false /* finalizing */))...)
+	}, p.hooks.AfterUpgradeFinalizedSteps(p.nextID, p.finalContext(false /* finalizing */))...)
 }
 
 func (p *testPlanner) upgradeSteps(from, to string) []testStep {
@@ -223,24 +195,10 @@ func (p *testPlanner) changeVersionSteps(from, to, label string) []testStep {
 // finalizeUpgradeSteps finalizes the upgrade by resetting the
 // `preserve_downgrade_option` and potentially running mixed-version
 // hooks while the cluster version is changing.
-func (p *testPlanner) finalizeUpgradeSteps(fromVersion, toVersion string) []testStep {
+func (p *testPlanner) finalizeUpgradeSteps() []testStep {
 	return append([]testStep{
 		finalizeUpgradeStep{id: p.nextID(), prng: p.newRNG(), crdbNodes: p.crdbNodes},
-	}, p.hooks.MixedVersionSteps(p.finalContext(fromVersion, toVersion, true /* finalizing */), p.nextID)...)
-}
-
-// shouldRollback returns whether the test will attempt a rollback. If
-// we are upgrading to the current version being tested, we always
-// rollback, as we want to expose that upgrade to complex upgrade
-// scenarios. If this is an intermediate upgrade in a multi-upgrade
-// test, then rollback with a probability. This stops tests from
-// having excessively long running times.
-func (p *testPlanner) shouldRollback(toVersion string) bool {
-	if toVersion == clusterupgrade.MainVersion {
-		return true
-	}
-
-	return p.prng.Float64() < rollbackIntermediateUpgradesProbability
+	}, p.hooks.MixedVersionSteps(p.finalContext(true /* finalizing */), p.nextID)...)
 }
 
 func (p *testPlanner) nextID() int {
@@ -263,14 +221,9 @@ func (plan *TestPlan) PrettyPrint() string {
 		plan.prettyPrintStep(&out, step, treeBranchString(i, len(plan.steps)))
 	}
 
-	formattedVersions := make([]string, 0, len(plan.versions))
-	for _, v := range plan.versions {
-		formattedVersions = append(formattedVersions, fmt.Sprintf("%q", versionMsg(v)))
-	}
-
 	return fmt.Sprintf(
-		"mixed-version test plan for upgrading from %s:\n%s",
-		strings.Join(formattedVersions, " to "), out.String(),
+		"mixed-version test plan for upgrading from %s to %s:\n%s",
+		plan.initialVersion, plan.finalVersion, out.String(),
 	)
 }
 
@@ -285,7 +238,7 @@ func (plan *TestPlan) prettyPrintStep(out *strings.Builder, step testStep, prefi
 		}
 	}
 
-	// writeSingle is the function that generates the description for
+	// writeRunnable is the function that generates the description for
 	// a singleStep. It can include extra information, such as whether
 	// there's a delay associated with the step (in the case of
 	// concurrent execution), and what database node the step is
diff --git a/pkg/cmd/roachtest/roachtestutil/mixedversion/planner_test.go b/pkg/cmd/roachtest/roachtestutil/mixedversion/planner_test.go
index 7c8000b8574..d17dc9765d6 100644
--- a/pkg/cmd/roachtest/roachtestutil/mixedversion/planner_test.go
+++ b/pkg/cmd/roachtest/roachtestutil/mixedversion/planner_test.go
@@ -69,50 +69,44 @@ func TestTestPlanner(t *testing.T) {
 
 	plan, err := mvt.plan()
 	require.NoError(t, err)
-	require.Len(t, plan.steps, 6)
+	require.Len(t, plan.steps, 10)
 
 	// Assert on the pretty-printed version of the test plan as that
 	// asserts the ordering of the steps we want to take, and as a bonus
 	// tests the printing function itself.
 	expectedPrettyPlan := fmt.Sprintf(`
-mixed-version test plan for upgrading from "%[1]s" to "<current>":
-├── install fixtures for version "%[1]s" (1)
-├── start cluster at version "%[1]s" (2)
-├── wait for nodes :1-4 to all have the same cluster version (same as binary version of node 1) (3)
+mixed-version test plan for upgrading from %[1]s to <current>:
+├── starting cluster at version "%[1]s" (1)
+├── wait for nodes :1-4 to all have the same cluster version (same as binary version of node 1) (2)
+├── preventing auto-upgrades by setting `+"`preserve_downgrade_option`"+` (3)
 ├── run "initialize bank workload" (4)
 ├── start background hooks concurrently
 │   ├── run "bank workload", after 50ms delay (5)
 │   ├── run "rand workload", after 200ms delay (6)
 │   └── run "csv server", after 500ms delay (7)
-└── upgrade cluster from "%[1]s" to "<current>"
-   ├── prevent auto-upgrades by setting `+"`preserve_downgrade_option`"+` (8)
-   ├── upgrade nodes :1-4 from "%[1]s" to "<current>"
-   │   ├── restart node 1 with binary version <current> (9)
-   │   ├── restart node 3 with binary version <current> (10)
-   │   ├── run "mixed-version 2" (11)
-   │   ├── restart node 2 with binary version <current> (12)
-   │   ├── run "mixed-version 1" (13)
-   │   └── restart node 4 with binary version <current> (14)
-   ├── downgrade nodes :1-4 from "<current>" to "%[1]s"
-   │   ├── restart node 2 with binary version %[1]s (15)
-   │   ├── run "mixed-version 1" (16)
-   │   ├── restart node 1 with binary version %[1]s (17)
-   │   ├── run "mixed-version 2" (18)
-   │   ├── restart node 3 with binary version %[1]s (19)
-   │   └── restart node 4 with binary version %[1]s (20)
-   ├── upgrade nodes :1-4 from "%[1]s" to "<current>"
-   │   ├── restart node 4 with binary version <current> (21)
-   │   ├── restart node 3 with binary version <current> (22)
-   │   ├── restart node 1 with binary version <current> (23)
-   │   ├── run mixed-version hooks concurrently
-   │   │   ├── run "mixed-version 1", after 0s delay (24)
-   │   │   └── run "mixed-version 2", after 0s delay (25)
-   │   └── restart node 2 with binary version <current> (26)
-   ├── finalize upgrade by resetting `+"`preserve_downgrade_option`"+` (27)
-   ├── run mixed-version hooks concurrently
-   │   ├── run "mixed-version 1", after 100ms delay (28)
-   │   └── run "mixed-version 2", after 0s delay (29)
-   └── wait for nodes :1-4 to all have the same cluster version (same as binary version of node 1) (30)
+├── upgrade nodes :1-4 from "%[1]s" to "<current>"
+│   ├── restart node 1 with binary version <current> (8)
+│   ├── run "mixed-version 1" (9)
+│   ├── restart node 4 with binary version <current> (10)
+│   ├── restart node 3 with binary version <current> (11)
+│   ├── run "mixed-version 2" (12)
+│   └── restart node 2 with binary version <current> (13)
+├── downgrade nodes :1-4 from "<current>" to "%[1]s"
+│   ├── restart node 4 with binary version %[1]s (14)
+│   ├── run "mixed-version 2" (15)
+│   ├── restart node 2 with binary version %[1]s (16)
+│   ├── restart node 3 with binary version %[1]s (17)
+│   ├── restart node 1 with binary version %[1]s (18)
+│   └── run "mixed-version 1" (19)
+├── upgrade nodes :1-4 from "%[1]s" to "<current>"
+│   ├── restart node 4 with binary version <current> (20)
+│   ├── run "mixed-version 1" (21)
+│   ├── restart node 1 with binary version <current> (22)
+│   ├── restart node 2 with binary version <current> (23)
+│   ├── run "mixed-version 2" (24)
+│   └── restart node 3 with binary version <current> (25)
+├── finalize upgrade by resetting `+"`preserve_downgrade_option`"+` (26)
+└── wait for nodes :1-4 to all have the same cluster version (same as binary version of node 1) (27)
 `, predecessorVersion)
 
 	expectedPrettyPlan = expectedPrettyPlan[1:] // remove leading newline
@@ -129,89 +123,15 @@ mixed-version test plan for upgrading from "%[1]s" to "<current>":
 	requireConcurrentHooks(t, plan.steps[3], "startup 1", "startup 2")
 
 	// Assert that AfterUpgradeFinalized hooks are scheduled to run in
-	// the last step of the upgrade.
+	// the last step of the test.
 	mvt = newTest()
 	mvt.AfterUpgradeFinalized("finalizer 1", dummyHook)
 	mvt.AfterUpgradeFinalized("finalizer 2", dummyHook)
 	mvt.AfterUpgradeFinalized("finalizer 3", dummyHook)
 	plan, err = mvt.plan()
 	require.NoError(t, err)
-	require.Len(t, plan.steps, 4)
-	upgradeSteps := plan.steps[3].(sequentialRunStep)
-	require.Len(t, upgradeSteps.steps, 7)
-	requireConcurrentHooks(t, upgradeSteps.steps[6], "finalizer 1", "finalizer 2", "finalizer 3")
-}
-
-// TestMultipleUpgrades tests the generation of test plans that
-// involve multiple upgrades.
-func TestMultipleUpgrades(t *testing.T) {
-	mvt := newTest(NumUpgrades(3))
-	mvt.predecessorFunc = func(rng *rand.Rand, v *version.Version, n int) ([]string, error) {
-		return []string{"22.1.8", "22.2.3", "23.1.4"}, nil
-	}
-
-	mvt.InMixedVersion("mixed-version 1", dummyHook)
-	initBank := roachtestutil.NewCommand("./cockroach workload init bank")
-	runBank := roachtestutil.NewCommand("./cockroach workload run bank")
-	mvt.Workload("bank", nodes, initBank, runBank)
-
-	plan, err := mvt.plan()
-	require.NoError(t, err)
-
-	expectedPrettyPlan := fmt.Sprintf(`
-mixed-version test plan for upgrading from "%[1]s" to "%[2]s" to "%[3]s" to "<current>":
-├── start cluster at version "%[1]s" (1)
-├── wait for nodes :1-4 to all have the same cluster version (same as binary version of node 1) (2)
-├── run "initialize bank workload" (3)
-├── run "bank workload" (4)
-├── upgrade cluster from "%[1]s" to "%[2]s"
-│   ├── prevent auto-upgrades by setting `+"`preserve_downgrade_option`"+` (5)
-│   ├── upgrade nodes :1-4 from "%[1]s" to "%[2]s"
-│   │   ├── restart node 2 with binary version %[2]s (6)
-│   │   ├── restart node 4 with binary version %[2]s (7)
-│   │   ├── restart node 1 with binary version %[2]s (8)
-│   │   ├── run "mixed-version 1" (9)
-│   │   └── restart node 3 with binary version %[2]s (10)
-│   ├── finalize upgrade by resetting `+"`preserve_downgrade_option`"+` (11)
-│   └── wait for nodes :1-4 to all have the same cluster version (same as binary version of node 1) (12)
-├── upgrade cluster from "%[2]s" to "%[3]s"
-│   ├── prevent auto-upgrades by setting `+"`preserve_downgrade_option`"+` (13)
-│   ├── upgrade nodes :1-4 from "%[2]s" to "%[3]s"
-│   │   ├── restart node 3 with binary version %[3]s (14)
-│   │   ├── restart node 1 with binary version %[3]s (15)
-│   │   ├── run "mixed-version 1" (16)
-│   │   ├── restart node 4 with binary version %[3]s (17)
-│   │   └── restart node 2 with binary version %[3]s (18)
-│   ├── finalize upgrade by resetting `+"`preserve_downgrade_option`"+` (19)
-│   ├── run "mixed-version 1" (20)
-│   └── wait for nodes :1-4 to all have the same cluster version (same as binary version of node 1) (21)
-└── upgrade cluster from "%[3]s" to "<current>"
-   ├── prevent auto-upgrades by setting `+"`preserve_downgrade_option`"+` (22)
-   ├── upgrade nodes :1-4 from "%[3]s" to "<current>"
-   │   ├── restart node 4 with binary version <current> (23)
-   │   ├── run "mixed-version 1" (24)
-   │   ├── restart node 1 with binary version <current> (25)
-   │   ├── restart node 2 with binary version <current> (26)
-   │   └── restart node 3 with binary version <current> (27)
-   ├── downgrade nodes :1-4 from "<current>" to "23.1.4"
-   │   ├── restart node 1 with binary version %[3]s (28)
-   │   ├── restart node 3 with binary version %[3]s (29)
-   │   ├── restart node 4 with binary version %[3]s (30)
-   │   ├── restart node 2 with binary version %[3]s (31)
-   │   └── run "mixed-version 1" (32)
-   ├── upgrade nodes :1-4 from "%[3]s" to "<current>"
-   │   ├── restart node 2 with binary version <current> (33)
-   │   ├── run "mixed-version 1" (34)
-   │   ├── restart node 3 with binary version <current> (35)
-   │   ├── restart node 1 with binary version <current> (36)
-   │   └── restart node 4 with binary version <current> (37)
-   ├── finalize upgrade by resetting `+"`preserve_downgrade_option`"+` (38)
-   ├── run "mixed-version 1" (39)
-   └── wait for nodes :1-4 to all have the same cluster version (same as binary version of node 1) (40)
-`, "22.1.8", "22.2.3", "23.1.4")
-
-	expectedPrettyPlan = expectedPrettyPlan[1:] // remove leading newline
-	require.Equal(t, expectedPrettyPlan, plan.PrettyPrint())
+	require.Len(t, plan.steps, 9)
+	requireConcurrentHooks(t, plan.steps[8], "finalizer 1", "finalizer 2", "finalizer 3")
 }
 
 // TestDeterministicTestPlan tests that generating a test plan with
@@ -276,19 +196,17 @@ func TestDeterministicHookSeeds(t *testing.T) {
 		plan, err := mvt.plan()
 		require.NoError(t, err)
 
-		upgradeStep := plan.steps[3].(sequentialRunStep)
-
 		// We can hardcode these paths since we are using a fixed seed in
 		// these tests.
-		firstRun := upgradeStep.steps[1].(sequentialRunStep).steps[3].(runHookStep)
+		firstRun := plan.steps[3].(sequentialRunStep).steps[4].(runHookStep)
 		require.Equal(t, "do something", firstRun.hook.name)
 		require.NoError(t, firstRun.Run(ctx, nilLogger, nilCluster, emptyHelper))
 
-		secondRun := upgradeStep.steps[2].(sequentialRunStep).steps[2].(runHookStep)
+		secondRun := plan.steps[4].(sequentialRunStep).steps[1].(runHookStep)
 		require.Equal(t, "do something", secondRun.hook.name)
 		require.NoError(t, secondRun.Run(ctx, nilLogger, nilCluster, emptyHelper))
 
-		thirdRun := upgradeStep.steps[3].(sequentialRunStep).steps[2].(runHookStep)
+		thirdRun := plan.steps[5].(sequentialRunStep).steps[3].(runHookStep)
 		require.Equal(t, "do something", thirdRun.hook.name)
 		require.NoError(t, thirdRun.Run(ctx, nilLogger, nilCluster, emptyHelper))
 
@@ -297,9 +215,9 @@ func TestDeterministicHookSeeds(t *testing.T) {
 	}
 
 	expectedData := [][]int{
-		{37, 94, 58, 5, 22},
-		{56, 88, 23, 85, 45},
-		{99, 37, 96, 23, 63},
+		{97, 94, 35, 65, 21},
+		{40, 30, 46, 88, 46},
+		{96, 91, 48, 85, 76},
 	}
 	const numRums = 50
 	for j := 0; j < numRums; j++ {
@@ -395,8 +313,8 @@ func newTest(options ...customOption) *Test {
 // Always use the same predecessor version to make this test
 // deterministic even as changes continue to happen in the
 // cockroach_releases.yaml file.
-func testPredecessorFunc(rng *rand.Rand, v *version.Version, n int) ([]string, error) {
-	return []string{predecessorVersion}, nil
+func testPredecessorFunc(rng *rand.Rand, v *version.Version) (string, error) {
+	return predecessorVersion, nil
 }
 
 // requireConcurrentHooks asserts that the given step is a concurrent
diff --git a/pkg/cmd/roachtest/tests/backup.go b/pkg/cmd/roachtest/tests/backup.go
index e50d467e942..f80d939025c 100644
--- a/pkg/cmd/roachtest/tests/backup.go
+++ b/pkg/cmd/roachtest/tests/backup.go
@@ -224,7 +224,7 @@ func registerBackupNodeShutdown(r registry.Registry) {
 			nodeToShutdown := 3
 			dest := loadBackupData(ctx, t, c)
 			backupQuery := `BACKUP bank.bank TO 'nodelocal://1/` + dest + `' WITH DETACHED`
-			startBackup := func(c cluster.Cluster, t test.Test) (jobID jobspb.JobID, err error) {
+			startBackup := func(c cluster.Cluster, t test.Test) (jobID string, err error) {
 				gatewayDB := c.Conn(ctx, t.L(), gatewayNode)
 				defer gatewayDB.Close()
 
@@ -246,7 +246,7 @@ func registerBackupNodeShutdown(r registry.Registry) {
 			nodeToShutdown := 2
 			dest := loadBackupData(ctx, t, c)
 			backupQuery := `BACKUP bank.bank TO 'nodelocal://1/` + dest + `' WITH DETACHED`
-			startBackup := func(c cluster.Cluster, t test.Test) (jobID jobspb.JobID, err error) {
+			startBackup := func(c cluster.Cluster, t test.Test) (jobID string, err error) {
 				gatewayDB := c.Conn(ctx, t.L(), gatewayNode)
 				defer gatewayDB.Close()
 
diff --git a/pkg/cmd/roachtest/tests/cluster_to_cluster.go b/pkg/cmd/roachtest/tests/cluster_to_cluster.go
index ef1e5e919df..cd671acfa68 100644
--- a/pkg/cmd/roachtest/tests/cluster_to_cluster.go
+++ b/pkg/cmd/roachtest/tests/cluster_to_cluster.go
@@ -362,9 +362,6 @@ type replicationDriver struct {
 	// beforeWorkloadHook is called before the main workload begins.
 	beforeWorkloadHook func()
 
-	// cutoverStarted closes once the driver issues a cutover commmand.
-	cutoverStarted chan struct{}
-
 	// replicationStartHook is called as soon as the replication job begins.
 	replicationStartHook func(ctx context.Context, sp *replicationDriver)
 
@@ -459,7 +456,6 @@ func (rd *replicationDriver) setupC2C(ctx context.Context, t test.Test, c cluste
 	rd.metrics = &c2cMetrics{}
 	rd.replicationStartHook = func(ctx context.Context, sp *replicationDriver) {}
 	rd.beforeWorkloadHook = func() {}
-	rd.cutoverStarted = make(chan struct{})
 
 	if !c.IsLocal() {
 		// TODO(msbutler): pass a proper cluster replication dashboard and figure out why we need to
@@ -594,7 +590,6 @@ func (rd *replicationDriver) stopReplicationStream(
 			rd.setup.dst.name, cutoverTime).Scan(&cutoverStr)
 	}
 	actualCutoverTime = DecimalTimeToHLC(rd.t, cutoverStr)
-	close(rd.cutoverStarted)
 	err := retry.ForDuration(rd.rs.cutoverTimeout, func() error {
 		var status string
 		var payloadBytes []byte
@@ -1123,45 +1118,28 @@ func getPhase(rd *replicationDriver, dstJobID jobspb.JobID) c2cPhase {
 	return phaseCutover
 }
 
-func waitForTargetPhase(
-	ctx context.Context, rd *replicationDriver, dstJobID jobspb.JobID, targetPhase c2cPhase,
-) error {
-	ticker := time.NewTicker(5 * time.Second)
-	defer ticker.Stop()
+func waitForTargetPhase(rd *replicationDriver, dstJobID jobspb.JobID, targetPhase c2cPhase) error {
 	for {
 		currentPhase := getPhase(rd, dstJobID)
-		rd.t.L().Printf("current Phase %s", currentPhase.String())
-		select {
-		case <-rd.cutoverStarted:
-			require.Equal(rd.t, phaseCutover, getPhase(rd, dstJobID), "the replication job is not yet in the cutover phase")
-			rd.t.L().Printf("cutover phase discovered via channel")
+		rd.t.L().Printf("Current Phase %s", currentPhase.String())
+		switch {
+		case currentPhase < targetPhase:
+			time.Sleep(5 * time.Second)
+		case currentPhase == targetPhase:
+			rd.t.L().Printf("In target phase %s", currentPhase.String())
 			return nil
-		case <-ticker.C:
-			switch {
-			case currentPhase < targetPhase:
-			case currentPhase == targetPhase:
-				rd.t.L().Printf("In target phase %s", currentPhase.String())
-				return nil
-			default:
-				return errors.New("c2c job past target phase")
-			}
-		case <-ctx.Done():
-			return ctx.Err()
+		default:
+			return errors.New("c2c job past target phase")
 		}
 	}
 }
 
-func sleepBeforeResiliencyEvent(rd *replicationDriver, phase c2cPhase) {
+func sleepBeforeResiliencyEvent(rd *replicationDriver) {
 	// Assuming every C2C phase lasts at least 10 seconds, introduce some waiting
 	// before a resiliency event (e.g. a node shutdown) to ensure the event occurs
 	// once we're fully settled into the target phase (e.g. the stream ingestion
 	// processors have observed the cutover signal).
-	baseSleep := 1
-	if phase == phaseCutover {
-		// Cutover can sometimes be fast, so sleep for less time.
-		baseSleep = 0
-	}
-	randomSleep := time.Duration(baseSleep+rd.rng.Intn(2)) * time.Second
+	randomSleep := time.Duration(1+rd.rng.Intn(2)) * time.Second
 	rd.t.L().Printf("Take a %s power nap", randomSleep)
 	time.Sleep(randomSleep)
 }
@@ -1275,10 +1253,10 @@ func registerClusterReplicationResilience(r registry.Registry) {
 				// DR scenario the src cluster may have gone belly up during a
 				// successful c2c replication execution.
 				shutdownStarter := func() jobStarter {
-					return func(c cluster.Cluster, t test.Test) (jobspb.JobID, error) {
-						require.NoError(t, waitForTargetPhase(ctx, rrd.replicationDriver, rrd.dstJobID, rrd.phase))
-						sleepBeforeResiliencyEvent(rrd.replicationDriver, rrd.phase)
-						return rrd.dstJobID, nil
+					return func(c cluster.Cluster, t test.Test) (string, error) {
+						require.NoError(t, waitForTargetPhase(rrd.replicationDriver, rrd.dstJobID, rrd.phase))
+						sleepBeforeResiliencyEvent(rrd.replicationDriver)
+						return fmt.Sprintf("%d", rrd.dstJobID), nil
 					}
 				}
 				destinationWatcherNode := rrd.watcherNode
@@ -1339,8 +1317,8 @@ func registerClusterReplicationDisconnect(r registry.Registry) {
 		dstJobID := jobspb.JobID(getIngestionJobID(t, rd.setup.dst.sysSQL, rd.setup.dst.name))
 
 		// TODO(msbutler): disconnect nodes during a random phase
-		require.NoError(t, waitForTargetPhase(ctx, rd, dstJobID, phaseSteadyState))
-		sleepBeforeResiliencyEvent(rd, phaseSteadyState)
+		require.NoError(t, waitForTargetPhase(rd, dstJobID, phaseSteadyState))
+		sleepBeforeResiliencyEvent(rd)
 		ingestionProgress := getJobProgress(t, rd.setup.dst.sysSQL, dstJobID).GetStreamIngest()
 
 		srcDestConnections := getSrcDestNodePairs(rd, ingestionProgress)
diff --git a/pkg/cmd/roachtest/tests/import.go b/pkg/cmd/roachtest/tests/import.go
index 13529111a30..8ff8b9640b4 100644
--- a/pkg/cmd/roachtest/tests/import.go
+++ b/pkg/cmd/roachtest/tests/import.go
@@ -25,7 +25,6 @@ import (
 	"github.com/cockroachdb/cockroach/pkg/cmd/roachtest/roachtestutil/clusterupgrade"
 	"github.com/cockroachdb/cockroach/pkg/cmd/roachtest/spec"
 	"github.com/cockroachdb/cockroach/pkg/cmd/roachtest/test"
-	"github.com/cockroachdb/cockroach/pkg/jobs/jobspb"
 	"github.com/cockroachdb/cockroach/pkg/roachprod/install"
 	"github.com/cockroachdb/cockroach/pkg/testutils/release"
 	"github.com/cockroachdb/cockroach/pkg/util/log"
@@ -44,8 +43,7 @@ func readCreateTableFromFixture(fixtureURI string, gatewayDB *gosql.DB) (string,
 
 func registerImportNodeShutdown(r registry.Registry) {
 	getImportRunner := func(ctx context.Context, t test.Test, gatewayNode int) jobStarter {
-		startImport := func(c cluster.Cluster, t test.Test) (jobspb.JobID, error) {
-			var jobID jobspb.JobID
+		startImport := func(c cluster.Cluster, t test.Test) (jobID string, err error) {
 			// partsupp is 11.2 GiB.
 			tableName := "partsupp"
 			if c.IsLocal() {
@@ -71,7 +69,7 @@ func registerImportNodeShutdown(r registry.Registry) {
 			createStmt, err := readCreateTableFromFixture(
 				fmt.Sprintf("gs://cockroach-fixtures/tpch-csv/schema/%s.sql?AUTH=implicit", tableName), gatewayDB)
 			if err != nil {
-				return jobID, err
+				return "", err
 			}
 
 			// Create the table to be imported into.
@@ -80,7 +78,7 @@ func registerImportNodeShutdown(r registry.Registry) {
 			}
 
 			err = gatewayDB.QueryRowContext(ctx, importStmt).Scan(&jobID)
-			return jobID, err
+			return
 		}
 
 		return startImport
diff --git a/pkg/cmd/roachtest/tests/jobs.go b/pkg/cmd/roachtest/tests/jobs.go
index 51c9cf9b5e9..d1e972975f5 100644
--- a/pkg/cmd/roachtest/tests/jobs.go
+++ b/pkg/cmd/roachtest/tests/jobs.go
@@ -12,7 +12,6 @@ package tests
 
 import (
 	"context"
-	gosql "database/sql"
 	"fmt"
 	"time"
 
@@ -22,15 +21,15 @@ import (
 	"github.com/cockroachdb/cockroach/pkg/jobs"
 	"github.com/cockroachdb/cockroach/pkg/jobs/jobspb"
 	"github.com/cockroachdb/cockroach/pkg/roachprod/install"
-	"github.com/cockroachdb/cockroach/pkg/testutils"
 	"github.com/cockroachdb/cockroach/pkg/testutils/sqlutils"
 	"github.com/cockroachdb/cockroach/pkg/util/protoutil"
 	"github.com/cockroachdb/cockroach/pkg/util/randutil"
+	"github.com/cockroachdb/cockroach/pkg/util/timeutil"
 	"github.com/cockroachdb/errors"
 	"github.com/stretchr/testify/require"
 )
 
-type jobStarter func(c cluster.Cluster, t test.Test) (jobspb.JobID, error)
+type jobStarter func(c cluster.Cluster, t test.Test) (string, error)
 
 // jobSurvivesNodeShutdown is a helper that tests that a given job,
 // running on the specified gatewayNode will still complete successfully
@@ -49,22 +48,18 @@ func jobSurvivesNodeShutdown(
 	ctx context.Context, t test.Test, c cluster.Cluster, nodeToShutdown int, startJob jobStarter,
 ) {
 	cfg := nodeShutdownConfig{
-		shutdownNode:         nodeToShutdown,
-		watcherNode:          1 + (nodeToShutdown)%c.Spec().NodeCount,
-		crdbNodes:            c.All(),
-		waitFor3XReplication: true,
-		sleepBeforeShutdown:  30 * time.Second,
+		shutdownNode: nodeToShutdown,
+		watcherNode:  1 + (nodeToShutdown)%c.Spec().NodeCount,
+		crdbNodes:    c.All(),
 	}
 	executeNodeShutdown(ctx, t, c, cfg, startJob)
 }
 
 type nodeShutdownConfig struct {
-	shutdownNode         int
-	watcherNode          int
-	crdbNodes            option.NodeListOption
-	restartSettings      []install.ClusterSettingOption
-	waitFor3XReplication bool
-	sleepBeforeShutdown  time.Duration
+	shutdownNode    int
+	watcherNode     int
+	crdbNodes       option.NodeListOption
+	restartSettings []install.ClusterSettingOption
 }
 
 func executeNodeShutdown(
@@ -74,29 +69,35 @@ func executeNodeShutdown(
 	t.L().Printf("test has chosen shutdown target node %d, and watcher node %d",
 		cfg.shutdownNode, cfg.watcherNode)
 
-	watcherDB := c.Conn(ctx, t.L(), cfg.watcherNode)
-	defer watcherDB.Close()
+	jobIDCh := make(chan string, 1)
+
+	m := c.NewMonitor(ctx, cfg.crdbNodes)
+	m.Go(func(ctx context.Context) error {
+		defer close(jobIDCh)
+
+		watcherDB := c.Conn(ctx, t.L(), cfg.watcherNode)
+		defer watcherDB.Close()
 
-	if cfg.waitFor3XReplication {
 		// Wait for 3x replication to ensure that the cluster
 		// is in a healthy state before we start bringing any
 		// nodes down.
 		t.Status("waiting for cluster to be 3x replicated")
 		err := WaitFor3XReplication(ctx, t, watcherDB)
 		require.NoError(t, err)
-	}
 
-	t.Status("running job")
-	jobID, err := startJob(c, t)
-	require.NoError(t, err)
-	t.L().Printf("started running job with ID %s", jobID)
-	WaitForRunning(t, watcherDB, jobID, time.Minute)
+		t.Status("running job")
+		var jobID string
+		jobID, err = startJob(c, t)
+		if err != nil {
+			return errors.Wrap(err, "starting the job")
+		}
+		t.L().Printf("started running job with ID %s", jobID)
+		jobIDCh <- jobID
 
-	m := c.NewMonitor(ctx, cfg.crdbNodes)
-	m.ExpectDeath()
-	m.Go(func(ctx context.Context) error {
-		ticker := time.NewTicker(1 * time.Second)
+		pollInterval := 5 * time.Second
+		ticker := time.NewTicker(pollInterval)
 		defer ticker.Stop()
+
 		var status string
 		for {
 			select {
@@ -121,19 +122,69 @@ func executeNodeShutdown(
 			}
 		}
 	})
-	time.Sleep(cfg.sleepBeforeShutdown)
-	rng, _ := randutil.NewTestRand()
-	shouldUseSigKill := rng.Float64() > 0.5
-	if shouldUseSigKill {
-		t.L().Printf(`stopping node (using SIGKILL) %s`, target)
-		require.NoError(t, c.StopE(ctx, t.L(), option.DefaultStopOpts(), target), "could not stop node %s", target)
-	} else {
-		t.L().Printf(`stopping node gracefully %s`, target)
-		require.NoError(t, c.StopCockroachGracefullyOnNode(ctx, t.L(), cfg.shutdownNode), "could not stop node %s", target)
-	}
-	t.L().Printf("stopped node %s", target)
 
+	m.Go(func(ctx context.Context) error {
+		jobID, ok := <-jobIDCh
+		if !ok {
+			return errors.New("job never created")
+		}
+
+		// Check once a second to see if the job has started running.
+		watcherDB := c.Conn(ctx, t.L(), cfg.watcherNode)
+		defer watcherDB.Close()
+		timeToWait := time.Second
+		timer := timeutil.Timer{}
+		jobRunning := false
+		for {
+			var status string
+			err := watcherDB.QueryRowContext(ctx, `SELECT status FROM [SHOW JOBS] WHERE job_id=$1`, jobID).Scan(&status)
+			if err != nil {
+				return errors.Wrap(err, "getting the job status")
+			}
+			switch jobs.Status(status) {
+			case jobs.StatusPending:
+			case jobs.StatusRunning:
+				jobRunning = true
+			default:
+				return errors.Newf("job too fast! job got to state %s before the target node could be shutdown",
+					status)
+			}
+			t.L().Printf(`status %s`, status)
+			timer.Reset(timeToWait)
+			select {
+			case <-ctx.Done():
+				return errors.Wrapf(ctx.Err(), "stopping test, did not shutdown node")
+			case <-timer.C:
+				timer.Read = true
+			}
+			// Break a second after confirming the job is running to ensure the node shutdown
+			// "in the middle" of running the job, right after the job began running.
+			if jobRunning {
+				break
+			}
+		}
+
+		rng, _ := randutil.NewTestRand()
+		shouldUseSigKill := rng.Float64() > 0.5
+		if shouldUseSigKill {
+			t.L().Printf(`stopping node (using SIGKILL) %s`, target)
+			if err := c.StopE(ctx, t.L(), option.DefaultStopOpts(), target); err != nil {
+				return errors.Wrapf(err, "could not stop node %s", target)
+			}
+		} else {
+			t.L().Printf(`stopping node gracefully %s`, target)
+			if err := c.StopCockroachGracefullyOnNode(ctx, t.L(), cfg.shutdownNode); err != nil {
+				return errors.Wrapf(err, "could not stop node %s", target)
+			}
+		}
+		t.L().Printf("stopped node %s", target)
+
+		return nil
+	})
+
+	m.ExpectDeath()
 	m.Wait()
+
 	// NB: the roachtest harness checks that at the end of the test, all nodes
 	// that have data also have a running process.
 	t.Status(fmt.Sprintf("restarting %s (node restart test is done)\n", target))
@@ -145,22 +196,6 @@ func executeNodeShutdown(
 	}
 }
 
-func WaitForRunning(t test.Test, db *gosql.DB, jobID jobspb.JobID, maxWait time.Duration) {
-	sqlDB := sqlutils.MakeSQLRunner(db)
-	testutils.SucceedsWithin(t, func() error {
-		var status jobs.Status
-		sqlDB.QueryRow(t, "SELECT status FROM crdb_internal.system_jobs WHERE id = $1", jobID).Scan(&status)
-		switch status {
-		case jobs.StatusPending:
-		case jobs.StatusRunning:
-		default:
-			return errors.Newf("job too fast! job got to state %s before the target node could be shutdown",
-				status)
-		}
-		return nil
-	}, maxWait)
-}
-
 func getJobProgress(t test.Test, db *sqlutils.SQLRunner, jobID jobspb.JobID) *jobspb.Progress {
 	ret := &jobspb.Progress{}
 	var buf []byte
diff --git a/pkg/cmd/roachtest/tests/mixed_version_backup.go b/pkg/cmd/roachtest/tests/mixed_version_backup.go
index e3ea03a69dd..5e368d2c11f 100644
--- a/pkg/cmd/roachtest/tests/mixed_version_backup.go
+++ b/pkg/cmd/roachtest/tests/mixed_version_backup.go
@@ -86,8 +86,13 @@ var (
 		MaxRetries:     80,
 	}
 
-	v231 = version.MustParse("v23.1.0")
-	v222 = version.MustParse("v22.2.0")
+	v231 = func() *version.Version {
+		v, err := version.Parse("v23.1.0")
+		if err != nil {
+			panic(fmt.Sprintf("failure parsing version: %v", err))
+		}
+		return v
+	}()
 
 	// systemTablesInFullClusterBackup includes all system tables that
 	// are included as part of a full cluster backup. It should include
@@ -104,21 +109,6 @@ var (
 		"privileges", "external_connections",
 	}
 
-	// systemTableVersionRestrictions maps a subset of
-	// `systemTablesInFullClusterBackups` to the minimum version that
-	// should be active in the cluster for the system tables to exist.
-	systemTableVersionRestrictions = map[string]*version.Version{
-		"external_connections": v222,
-		"privileges":           v222,
-		// Even though the `tenant_settings` table exists in 22.1, there
-		// is a bug when using `SHOW COLUMNS` to access information about
-		// this table after a cluster restore in 22.1 (error: `relation
-		// "system.tenant_settings" does not exist`). For that reason, we
-		// do not make any assertions about this table for cluster versions
-		// older than 22.2.
-		"tenant_settings": v222,
-	}
-
 	// showSystemQueries maps system table names to `SHOW` statements
 	// that reflect their contents in a more human readable format. When
 	// the contents of a system table after restore does not match the
@@ -192,8 +182,19 @@ func sanitizeVersionForBackup(v string) string {
 // have the `crdb_internal.system_jobs` vtable in the mixed-version
 // context passed. If so, it should be used instead of `system.jobs`
 // when querying job status.
-func hasInternalSystemJobs(h *mixedversion.Helper) bool {
-	return h.LowestBinaryVersion().AtLeast(v231)
+func hasInternalSystemJobs(tc *mixedversion.Context) bool {
+	lowestVersion := tc.FromVersion // upgrades
+	if tc.FromVersion == clusterupgrade.MainVersion {
+		lowestVersion = tc.ToVersion // downgrades
+	}
+
+	// Add 'v' prefix expected by `version` package.
+	lowestVersion = "v" + lowestVersion
+	sv, err := version.Parse(lowestVersion)
+	if err != nil {
+		panic(fmt.Errorf("internal error: test context version (%s) expected to be parseable: %w", lowestVersion, err))
+	}
+	return sv.AtLeast(v231)
 }
 
 func aostFor(timestamp string) string {
@@ -534,28 +535,14 @@ func (dbb *databaseBackup) TargetTables() []string {
 	return tableNamesWithDB(dbb.db, dbb.tables)
 }
 
-func newClusterBackup(
-	rng *rand.Rand, dbs []string, tables [][]string, lowest *version.Version,
-) *clusterBackup {
+func newClusterBackup(rng *rand.Rand, dbs []string, tables [][]string) *clusterBackup {
 	dbBackups := make([]*databaseBackup, 0, len(dbs))
 	for j, db := range dbs {
 		dbBackups = append(dbBackups, newDatabaseBackup(rng, []string{db}, [][]string{tables[j]}))
 	}
-
-	// Only include system tables that exist in the current version.
-	var systemTables []string
-	for _, t := range systemTablesInFullClusterBackup {
-		minVersion, ok := systemTableVersionRestrictions[t]
-		if ok && !lowest.AtLeast(minVersion) {
-			continue
-		}
-
-		systemTables = append(systemTables, t)
-	}
-
 	return &clusterBackup{
 		dbBackups:    dbBackups,
-		systemTables: systemTables,
+		systemTables: systemTablesInFullClusterBackup,
 	}
 }
 
@@ -1044,11 +1031,11 @@ func newMixedVersionBackup(
 
 // newBackupType chooses a random backup type (table, database,
 // cluster) with equal probability.
-func (mvb *mixedVersionBackup) newBackupType(rng *rand.Rand, h *mixedversion.Helper) backupType {
+func (mvb *mixedVersionBackup) newBackupType(rng *rand.Rand) backupType {
 	possibleTypes := []backupType{
 		newTableBackup(rng, mvb.dbs, mvb.tables),
 		newDatabaseBackup(rng, mvb.dbs, mvb.tables),
-		newClusterBackup(rng, mvb.dbs, mvb.tables, h.LowestBinaryVersion()),
+		newClusterBackup(rng, mvb.dbs, mvb.tables),
 	}
 
 	return possibleTypes[rng.Intn(len(possibleTypes))]
@@ -1365,7 +1352,7 @@ func (mvb *mixedVersionBackup) waitForJobSuccess(
 	l.Printf("querying job status through node %d", node)
 
 	jobsQuery := "system.jobs WHERE id = $1"
-	if hasInternalSystemJobs(h) {
+	if hasInternalSystemJobs(h.Context()) {
 		jobsQuery = fmt.Sprintf("(%s)", jobutils.InternalSystemJobsBaseQuery)
 	}
 	for r := retry.StartWithCtx(ctx, backupCompletionRetryOptions); r.Next(); {
@@ -1529,7 +1516,7 @@ func (mvb *mixedVersionBackup) runBackup(
 	var collection backupCollection
 	switch b := bType.(type) {
 	case fullBackup:
-		btype := mvb.newBackupType(rng, h)
+		btype := mvb.newBackupType(rng)
 		name := mvb.backupName(mvb.nextBackupID(), h, b.label, btype)
 		createOptions := newBackupOptions(rng)
 		collection = newBackupCollection(name, btype, createOptions)
@@ -1848,16 +1835,10 @@ func (mvb *mixedVersionBackup) planAndRunBackups(
 }
 
 // checkFiles uses the `check_files` option of `SHOW BACKUP` to verify
-// that the latest backup in the collection passed is valid. This step
-// is skipped if the feature is not available.
+// that the latest backup in the collection passed is valid.
 func (mvb *mixedVersionBackup) checkFiles(
-	rng *rand.Rand, l *logger.Logger, collection *backupCollection, h *mixedversion.Helper,
+	rng *rand.Rand, collection *backupCollection, h *mixedversion.Helper,
 ) error {
-	if !h.LowestBinaryVersion().AtLeast(v231) {
-		l.Printf("skipping check_files as it is not supported")
-		return nil
-	}
-
 	options := []string{"check_files"}
 	if opt := collection.encryptionOption(); opt != nil {
 		options = append(options, opt.String())
@@ -1938,7 +1919,7 @@ func (mvb *mixedVersionBackup) verifyBackupCollection(
 
 	// As a sanity check, make sure that a `check_files` check passes
 	// before attempting a restore.
-	if err := mvb.checkFiles(rng, l, collection, h); err != nil {
+	if err := mvb.checkFiles(rng, collection, h); err != nil {
 		return fmt.Errorf("%s: backup %s: check_files failed: %w", v, collection.name, err)
 	}
 
@@ -2072,7 +2053,7 @@ func (mvb *mixedVersionBackup) verifyAllBackups(
 	}
 
 	verify(h.Context().FromVersion)
-	verify(h.Context().ToVersion)
+	verify(clusterupgrade.MainVersion)
 
 	// If the context was canceled (most likely due to a test timeout),
 	// return early. In these cases, it's likely that `restoreErrors`
@@ -2098,13 +2079,6 @@ func (mvb *mixedVersionBackup) verifyAllBackups(
 		return fmt.Errorf("%d errors during restore:\n%s", len(restoreErrors), strings.Join(msgs, "\n"))
 	}
 
-	// Reset collections -- if this test run is performing multiple
-	// upgrades, we just want to test restores from the previous version
-	// to the current one.
-	//
-	// TODO(renato): it would be nice if this automatically followed
-	// `binaryMinSupportedVersion` instead.
-	mvb.collections = nil
 	return nil
 }
 
diff --git a/pkg/cmd/roachtest/tests/mixed_version_cdc.go b/pkg/cmd/roachtest/tests/mixed_version_cdc.go
index a869085c556..9d9eeadd30e 100644
--- a/pkg/cmd/roachtest/tests/mixed_version_cdc.go
+++ b/pkg/cmd/roachtest/tests/mixed_version_cdc.go
@@ -57,6 +57,8 @@ var (
 	targetDB    = "bank"
 	targetTable = "bank"
 
+	timeout = 30 * time.Minute
+
 	// teamcityAgentZone is the zone used in this test. Since this test
 	// runs a lot of queries from the TeamCity agent to CRDB nodes, we
 	// make sure to create roachprod nodes that are in the same region
@@ -74,10 +76,10 @@ func registerCDCMixedVersions(r registry.Registry) {
 	}
 	r.Add(registry.TestSpec{
 		Name:  "cdc/mixed-versions",
-		Owner: registry.OwnerCDC,
+		Owner: registry.OwnerTestEng,
 		// N.B. ARM64 is not yet supported, see https://github.com/cockroachdb/cockroach/issues/103888.
 		Cluster:         r.MakeClusterSpec(5, spec.Zones(zones), spec.Arch(vm.ArchAMD64)),
-		Timeout:         60 * time.Minute,
+		Timeout:         timeout,
 		RequiresLicense: true,
 		Run: func(ctx context.Context, t test.Test, c cluster.Cluster) {
 			runCDCMixedVersions(ctx, t, c)
@@ -416,18 +418,7 @@ func runCDCMixedVersions(ctx context.Context, t test.Test, c cluster.Cluster) {
 
 	// NB: We rely on the testing framework to choose a random predecessor
 	// to upgrade from.
-	mvt := mixedversion.NewTest(
-		ctx, t, t.L(), c, tester.crdbNodes,
-		// For now, we perform at most 2 upgrades in this test so that the
-		// lowest version we start from is 22.2. The reason for this is
-		// that, in 22.1, node draining errors were common and led to
-		// changefeeds failing. See #106878.
-		//
-		// TODO(renato): remove this restriction by not failing the test
-		// if the changefeed failed due to "node draining" errors while
-		// still in 22.1 or older.
-		mixedversion.MaxUpgrades(2),
-	)
+	mvt := mixedversion.NewTest(ctx, t, t.L(), c, tester.crdbNodes)
 
 	cleanupKafka := tester.StartKafka(t, c)
 	defer cleanupKafka()
diff --git a/pkg/cmd/roachtest/tests/restore.go b/pkg/cmd/roachtest/tests/restore.go
index deb296af6ef..fd24c538abf 100644
--- a/pkg/cmd/roachtest/tests/restore.go
+++ b/pkg/cmd/roachtest/tests/restore.go
@@ -57,8 +57,9 @@ func registerRestoreNodeShutdown(r registry.Registry) {
 
 	makeRestoreStarter := func(ctx context.Context, t test.Test, c cluster.Cluster,
 		gatewayNode int, rd restoreDriver) jobStarter {
-		return func(c cluster.Cluster, t test.Test) (jobspb.JobID, error) {
-			return rd.runDetached(ctx, "DATABASE tpce", gatewayNode)
+		return func(c cluster.Cluster, t test.Test) (string, error) {
+			jobID, err := rd.runDetached(ctx, "DATABASE tpce", gatewayNode)
+			return fmt.Sprintf("%d", jobID), err
 		}
 	}
 
diff --git a/pkg/jobs/adopt.go b/pkg/jobs/adopt.go
index ffaff7411be..c9221790902 100644
--- a/pkg/jobs/adopt.go
+++ b/pkg/jobs/adopt.go
@@ -327,7 +327,15 @@ func (r *Registry) resumeJob(
 				return err
 			}
 			if !exists {
-				return errors.Wrap(&JobNotFoundError{jobID: jobID}, "job payload not found in system.job_info")
+				// 23.1.3 could finalize an upgrade but leave some jobs behind with rows
+				// not copied to info table. If we get here, try backfilling the info
+				// table for this job in the txn and proceed if it succeeds.
+				fixedPayload, err := infoStorage.BackfillLegacyPayload(ctx)
+				if err != nil {
+					return errors.Wrap(err, "job payload not found in system.job_info")
+				}
+				log.Infof(ctx, "fixed missing payload info for job %d", jobID)
+				payloadBytes = fixedPayload
 			}
 			if err := protoutil.Unmarshal(payloadBytes, payload); err != nil {
 				return err
@@ -338,7 +346,12 @@ func (r *Registry) resumeJob(
 				return err
 			}
 			if !exists {
-				return errors.Wrap(&JobNotFoundError{jobID: jobID}, "job progress not found in system.job_info")
+				fixedProgress, err := infoStorage.BackfillLegacyProgress(ctx)
+				if err != nil {
+					return errors.Wrap(err, "job progress not found in system.job_info")
+				}
+				log.Infof(ctx, "fixed missing progress info for job %d", jobID)
+				progressBytes = fixedProgress
 			}
 			return protoutil.Unmarshal(progressBytes, progress)
 		}); err != nil {
diff --git a/pkg/jobs/job_info_storage.go b/pkg/jobs/job_info_storage.go
index d0d0566b1ec..5fdad0af0c9 100644
--- a/pkg/jobs/job_info_storage.go
+++ b/pkg/jobs/job_info_storage.go
@@ -13,6 +13,7 @@ package jobs
 import (
 	"bytes"
 	"context"
+	"fmt"
 
 	"github.com/cockroachdb/cockroach/pkg/jobs/jobspb"
 	"github.com/cockroachdb/cockroach/pkg/roachpb"
@@ -23,6 +24,7 @@ import (
 	"github.com/cockroachdb/cockroach/pkg/util/log"
 	"github.com/cockroachdb/cockroach/pkg/util/tracing"
 	"github.com/cockroachdb/errors"
+	"github.com/cockroachdb/redact"
 )
 
 // InfoStorage can be used to read and write rows to system.job_info table. All
@@ -318,3 +320,38 @@ func (i InfoStorage) GetLegacyProgress(ctx context.Context) ([]byte, bool, error
 func (i InfoStorage) WriteLegacyProgress(ctx context.Context, progress []byte) error {
 	return i.Write(ctx, LegacyProgressKey, progress)
 }
+
+// BackfillLegacyPayload copies a legacy payload from system.jobs. #104798.
+func (i InfoStorage) BackfillLegacyPayload(ctx context.Context) ([]byte, error) {
+	return i.backfillMissing(ctx, "payload")
+}
+
+// BackfillLegacyProgress copies a legacy progress from system.jobs. #104798.
+func (i InfoStorage) BackfillLegacyProgress(ctx context.Context) ([]byte, error) {
+	return i.backfillMissing(ctx, "progress")
+}
+
+func (i InfoStorage) backfillMissing(ctx context.Context, kind string) ([]byte, error) {
+	row, err := i.txn.QueryRowEx(
+		ctx, fmt.Sprintf("job-info-fix-%s", kind), i.txn.KV(),
+		sessiondata.NodeUserSessionDataOverride,
+		`INSERT INTO system.job_info (job_id, info_key, value) 
+			SELECT id, 'legacy_`+kind+`', `+kind+` FROM system.jobs WHERE id = $1 AND `+kind+` IS NOT NULL
+			RETURNING value`,
+		i.j.ID(),
+	)
+
+	if err != nil {
+		return nil, err
+	}
+
+	if row == nil {
+		return nil, errors.Wrapf(&JobNotFoundError{jobID: i.j.ID()}, "job %s not found in system.jobs", redact.SafeString(kind))
+	}
+
+	value, ok := row[0].(*tree.DBytes)
+	if !ok {
+		return nil, errors.AssertionFailedf("job info: expected value to be DBytes (was %T)", row[0])
+	}
+	return []byte(*value), nil
+}
diff --git a/pkg/jobs/jobs_test.go b/pkg/jobs/jobs_test.go
index 65a3cc0efaa..69fab5b7e2d 100644
--- a/pkg/jobs/jobs_test.go
+++ b/pkg/jobs/jobs_test.go
@@ -1618,7 +1618,7 @@ func TestJobLifecycle(t *testing.T) {
 			}, registry.MakeJobID(), txn)
 			return errors.New("boom")
 		}))
-		if err := job.Started(ctx); !testutils.IsError(err, "not found in system.jobs table") {
+		if err := job.Started(ctx); !testutils.IsError(err, "job payload not found in system.jobs") {
 			t.Fatalf("unexpected error %v", err)
 		}
 	})
diff --git a/pkg/jobs/registry.go b/pkg/jobs/registry.go
index fa420d9bd15..833ae134a16 100644
--- a/pkg/jobs/registry.go
+++ b/pkg/jobs/registry.go
@@ -1861,6 +1861,13 @@ func (r *Registry) MarkIdle(job *Job, isIdle bool) {
 	}
 }
 
+// TestingForgetJob causes the registry to forget it has adopted a job.
+func (r *Registry) TestingForgetJob(id jobspb.JobID) {
+	r.mu.Lock()
+	defer r.mu.Unlock()
+	delete(r.mu.adoptedJobs, id)
+}
+
 func (r *Registry) cancelAllAdoptedJobs() {
 	r.mu.Lock()
 	defer r.mu.Unlock()
diff --git a/pkg/jobs/update.go b/pkg/jobs/update.go
index 55b61aaf2c8..a2b37fe4365 100644
--- a/pkg/jobs/update.go
+++ b/pkg/jobs/update.go
@@ -108,7 +108,33 @@ func (u Updater) update(ctx context.Context, useReadLock bool, updateFn UpdateFn
 		return err
 	}
 	if row == nil {
-		return errors.Errorf("not found in system.jobs table")
+		// Maybe try to fix a row missed by 23.1.3 backfill and re-read. #104798.
+		if u.j.registry.settings.Version.IsActive(ctx, clusterversion.V23_1JobInfoTableIsBackfilled) {
+			i := j.InfoStorage(u.txn)
+
+			_, err := i.BackfillLegacyPayload(ctx)
+			if err != nil {
+				return errors.Wrap(err, "failed to backfill job info payload during update")
+			}
+
+			_, err = i.BackfillLegacyProgress(ctx)
+			if err != nil {
+				return errors.Wrap(err, "failed to backfill job info progress during update")
+			}
+
+			row, err = u.txn.QueryRowEx(
+				ctx, "select-job", u.txn.KV(),
+				sessiondata.RootUserSessionDataOverride,
+				getSelectStmtForJobUpdate(ctx, j.session != nil, useReadLock, u.j.registry.settings.Version), j.ID(),
+			)
+
+			if err != nil {
+				return err
+			}
+		}
+		if row == nil {
+			return errors.Errorf("not found in system.jobs table")
+		}
 	}
 
 	if status, err = unmarshalStatus(row[0]); err != nil {
diff --git a/pkg/kv/kvserver/batcheval/cmd_export_test.go b/pkg/kv/kvserver/batcheval/cmd_export_test.go
index 13c49375f83..1317cbf62e3 100644
--- a/pkg/kv/kvserver/batcheval/cmd_export_test.go
+++ b/pkg/kv/kvserver/batcheval/cmd_export_test.go
@@ -530,14 +530,11 @@ func exportUsingGoIterator(
 		return nil, nil
 	}
 
-	iter, err := storage.NewMVCCIncrementalIterator(reader, storage.MVCCIncrementalIterOptions{
+	iter := storage.NewMVCCIncrementalIterator(reader.(storage.ReaderWithMustIterators), storage.MVCCIncrementalIterOptions{
 		EndKey:    endKey,
 		StartTime: startTime,
 		EndTime:   endTime,
 	})
-	if err != nil {
-		return nil, err
-	}
 	defer iter.Close()
 	for iter.SeekGE(storage.MakeMVCCMetadataKey(startKey)); ; iterFn(iter) {
 		ok, err := iter.Valid()
diff --git a/pkg/kv/kvserver/batcheval/cmd_refresh_range.go b/pkg/kv/kvserver/batcheval/cmd_refresh_range.go
index 26d2de08beb..a3d3bf22562 100644
--- a/pkg/kv/kvserver/batcheval/cmd_refresh_range.go
+++ b/pkg/kv/kvserver/batcheval/cmd_refresh_range.go
@@ -77,7 +77,7 @@ func refreshRange(
 	// Construct an incremental iterator with the desired time bounds. Incremental
 	// iterators will emit MVCC tombstones by default and will emit intents when
 	// configured to do so (see IntentPolicy).
-	iter, err := storage.NewMVCCIncrementalIterator(reader, storage.MVCCIncrementalIterOptions{
+	iter := storage.NewMVCCIncrementalIterator(reader.(storage.ReaderWithMustIterators), storage.MVCCIncrementalIterOptions{
 		KeyTypes:     storage.IterKeyTypePointsAndRanges,
 		StartKey:     span.Key,
 		EndKey:       span.EndKey,
@@ -85,9 +85,6 @@ func refreshRange(
 		EndTime:      refreshTo,   // inclusive
 		IntentPolicy: storage.MVCCIncrementalIterIntentPolicyEmit,
 	})
-	if err != nil {
-		return err
-	}
 	defer iter.Close()
 
 	var meta enginepb.MVCCMetadata
diff --git a/pkg/kv/kvserver/kvflowcontrol/kvflowcontroller/kvflowcontroller.go b/pkg/kv/kvserver/kvflowcontrol/kvflowcontroller/kvflowcontroller.go
index b263edff077..75a1ffb8221 100644
--- a/pkg/kv/kvserver/kvflowcontrol/kvflowcontroller/kvflowcontroller.go
+++ b/pkg/kv/kvserver/kvflowcontrol/kvflowcontroller/kvflowcontroller.go
@@ -57,6 +57,7 @@ const regular, elastic = admissionpb.RegularWorkClass, admissionpb.ElasticWorkCl
 type Controller struct {
 	mu struct {
 		syncutil.Mutex
+
 		// Token limit per work class, tracking
 		// kvadmission.flow_controller.{regular,elastic}_tokens_per_stream.
 		limit tokensPerWorkClass
@@ -68,7 +69,7 @@ type Controller struct {
 		// streams get closed permanently (tenants get deleted, nodes removed)
 		// or when completely inactive (no tokens deducted/returned over 30+
 		// minutes), clear these out.
-		buckets map[kvflowcontrol.Stream]*bucket
+		buckets map[kvflowcontrol.Stream]bucket
 	}
 	metrics  *metrics
 	clock    *hlc.Clock
@@ -90,7 +91,7 @@ func New(registry *metric.Registry, settings *cluster.Settings, clock *hlc.Clock
 		regular: regularTokens,
 		elastic: elasticTokens,
 	}
-	c.mu.buckets = make(map[kvflowcontrol.Stream]*bucket)
+	c.mu.buckets = make(map[kvflowcontrol.Stream]bucket)
 	regularTokensPerStream.SetOnChange(&settings.SV, func(ctx context.Context) {
 		c.mu.Lock()
 		defer c.mu.Unlock()
@@ -109,10 +110,8 @@ func New(registry *metric.Registry, settings *cluster.Settings, clock *hlc.Clock
 		}
 		c.mu.limit = now
 		for _, b := range c.mu.buckets {
-			b.mu.Lock()
-			b.mu.tokens[regular] += adjustment[regular]
-			b.mu.tokens[elastic] += adjustment[elastic]
-			b.mu.Unlock()
+			b.tokens[regular] += adjustment[regular]
+			b.tokens[elastic] += adjustment[elastic]
 			c.metrics.onTokenAdjustment(adjustment)
 			if adjustment[regular] > 0 || adjustment[elastic] > 0 {
 				b.signal() // signal a waiter, if any
@@ -145,9 +144,9 @@ func (c *Controller) Admit(
 	for {
 		c.mu.Lock()
 		b := c.getBucketLocked(connection.Stream())
+		tokens := b.tokens[class]
 		c.mu.Unlock()
 
-		tokens := b.tokens(class)
 		if tokens > 0 ||
 			// In addition to letting requests through when there are tokens
 			// being available, we'll also let them through if we're not
@@ -244,14 +243,12 @@ func (c *Controller) Inspect(ctx context.Context) []kvflowinspectpb.Stream {
 
 	var streams []kvflowinspectpb.Stream
 	for stream, b := range c.mu.buckets {
-		b.mu.Lock()
 		streams = append(streams, kvflowinspectpb.Stream{
 			TenantID:               stream.TenantID,
 			StoreID:                stream.StoreID,
-			AvailableRegularTokens: int64(b.tokensLocked(regular)),
-			AvailableElasticTokens: int64(b.tokensLocked(elastic)),
+			AvailableRegularTokens: int64(b.tokens[regular]),
+			AvailableElasticTokens: int64(b.tokens[elastic]),
 		})
-		b.mu.Unlock()
 	}
 	sort.Slice(streams, func(i, j int) bool { // for determinism
 		if streams[i].TenantID != streams[j].TenantID {
@@ -284,8 +281,9 @@ func (c *Controller) adjustTokens(
 	class := admissionpb.WorkClassFromPri(pri)
 
 	c.mu.Lock()
+	defer c.mu.Unlock()
+
 	b := c.getBucketLocked(stream)
-	c.mu.Unlock()
 	adjustment, unaccounted := b.adjust(ctx, class, delta, c.mu.limit)
 	c.metrics.onTokenAdjustment(adjustment)
 	c.metrics.onUnaccounted(unaccounted)
@@ -294,14 +292,12 @@ func (c *Controller) adjustTokens(
 	}
 
 	if log.ExpensiveLogEnabled(ctx, 2) {
-		b.mu.Lock()
 		log.Infof(ctx, "adjusted flow tokens (pri=%s stream=%s delta=%s): regular=%s elastic=%s",
-			pri, stream, delta, b.tokensLocked(regular), b.tokensLocked(elastic))
-		b.mu.Unlock()
+			pri, stream, delta, b.tokens[regular], b.tokens[elastic])
 	}
 }
 
-func (c *Controller) getBucketLocked(stream kvflowcontrol.Stream) *bucket {
+func (c *Controller) getBucketLocked(stream kvflowcontrol.Stream) bucket {
 	b, ok := c.mu.buckets[stream]
 	if !ok {
 		b = newBucket(c.mu.limit)
@@ -314,11 +310,7 @@ func (c *Controller) getBucketLocked(stream kvflowcontrol.Stream) *bucket {
 // kvflowcontrol.Stream. It's used to synchronize handoff between threads
 // returning and waiting for flow tokens.
 type bucket struct {
-	mu struct {
-		syncutil.Mutex
-		tokens tokensPerWorkClass
-	}
-
+	tokens tokensPerWorkClass // protected by Controller.mu
 	// Waiting requests do so by waiting on signalCh without holding mutexes.
 	// Requests first check for available tokens, waiting if unavailable.
 	// - Whenever tokens are returned, signalCh is signaled, waking up a single
@@ -334,25 +326,14 @@ type bucket struct {
 	signalCh chan struct{}
 }
 
-func newBucket(t tokensPerWorkClass) *bucket {
-	b := bucket{
+func newBucket(t tokensPerWorkClass) bucket {
+	return bucket{
+		tokens: map[admissionpb.WorkClass]kvflowcontrol.Tokens{
+			regular: t[regular],
+			elastic: t[elastic],
+		},
 		signalCh: make(chan struct{}, 1),
 	}
-	b.mu.tokens = map[admissionpb.WorkClass]kvflowcontrol.Tokens{
-		regular: t[regular],
-		elastic: t[elastic],
-	}
-	return &b
-}
-
-func (b *bucket) tokens(wc admissionpb.WorkClass) kvflowcontrol.Tokens {
-	b.mu.Lock()
-	defer b.mu.Unlock()
-	return b.tokensLocked(wc)
-}
-
-func (b *bucket) tokensLocked(wc admissionpb.WorkClass) kvflowcontrol.Tokens {
-	return b.mu.tokens[wc]
 }
 
 func (b *bucket) signal() {
@@ -372,38 +353,35 @@ func (b *bucket) adjust(
 	delta kvflowcontrol.Tokens,
 	limit tokensPerWorkClass,
 ) (adjustment, unaccounted tokensPerWorkClass) {
-	b.mu.Lock()
-	defer b.mu.Unlock()
-
 	unaccounted = tokensPerWorkClass{
 		regular: 0,
 		elastic: 0,
 	}
 
 	before := tokensPerWorkClass{
-		regular: b.mu.tokens[regular],
-		elastic: b.mu.tokens[elastic],
+		regular: b.tokens[regular],
+		elastic: b.tokens[elastic],
 	}
 
 	switch class {
 	case elastic:
 		// Elastic {deductions,returns} only affect elastic flow tokens.
-		b.mu.tokens[class] += delta
-		if delta > 0 && b.mu.tokens[class] > limit[class] {
-			unaccounted[class] = b.mu.tokens[class] - limit[class]
-			b.mu.tokens[class] = limit[class] // enforce ceiling
+		b.tokens[class] += delta
+		if delta > 0 && b.tokens[class] > limit[class] {
+			unaccounted[class] = b.tokens[class] - limit[class]
+			b.tokens[class] = limit[class] // enforce ceiling
 		}
 	case regular:
-		b.mu.tokens[class] += delta
-		if delta > 0 && b.mu.tokens[class] > limit[class] {
-			unaccounted[class] = b.mu.tokens[class] - limit[class]
-			b.mu.tokens[class] = limit[class] // enforce ceiling
+		b.tokens[class] += delta
+		if delta > 0 && b.tokens[class] > limit[class] {
+			unaccounted[class] = b.tokens[class] - limit[class]
+			b.tokens[class] = limit[class] // enforce ceiling
 		}
 
-		b.mu.tokens[elastic] += delta
-		if delta > 0 && b.mu.tokens[elastic] > limit[elastic] {
-			unaccounted[elastic] = b.mu.tokens[elastic] - limit[elastic]
-			b.mu.tokens[elastic] = limit[elastic] // enforce ceiling
+		b.tokens[elastic] += delta
+		if delta > 0 && b.tokens[elastic] > limit[elastic] {
+			unaccounted[elastic] = b.tokens[elastic] - limit[elastic]
+			b.tokens[elastic] = limit[elastic] // enforce ceiling
 		}
 	}
 
@@ -413,8 +391,8 @@ func (b *bucket) adjust(
 	}
 
 	adjustment = tokensPerWorkClass{
-		regular: b.mu.tokens[regular] - before[regular],
-		elastic: b.mu.tokens[elastic] - before[elastic],
+		regular: b.tokens[regular] - before[regular],
+		elastic: b.tokens[elastic] - before[elastic],
 	}
 	return adjustment, unaccounted
 }
@@ -438,16 +416,12 @@ var validateTokenRange = settings.WithValidateInt(func(b int64) error {
 })
 
 func (c *Controller) getTokensForStream(stream kvflowcontrol.Stream) tokensPerWorkClass {
-	ret := make(map[admissionpb.WorkClass]kvflowcontrol.Tokens)
 	c.mu.Lock()
-	b := c.getBucketLocked(stream)
-	c.mu.Unlock()
-
-	b.mu.Lock()
-	for _, wc := range []admissionpb.WorkClass{regular, elastic} {
-		ret[wc] = b.tokensLocked(wc)
+	defer c.mu.Unlock()
+	ret := make(map[admissionpb.WorkClass]kvflowcontrol.Tokens)
+	for wc, c := range c.getBucketLocked(stream).tokens {
+		ret[wc] = c
 	}
-	b.mu.Unlock()
 	return ret
 }
 
@@ -488,12 +462,9 @@ func (c *Controller) TestingNonBlockingAdmit(
 
 		c.mu.Lock()
 		b := c.getBucketLocked(connection.Stream())
+		tokens := b.tokens[class]
 		c.mu.Unlock()
 
-		b.mu.Lock()
-		tokens := b.mu.tokens[class]
-		b.mu.Unlock()
-
 		if tokens <= 0 {
 			return false
 		}
@@ -526,7 +497,7 @@ func (c *Controller) TestingMetrics() interface{} {
 	return c.metrics
 }
 
-func (c *Controller) testingGetBucket(stream kvflowcontrol.Stream) *bucket {
+func (c *Controller) testingGetBucket(stream kvflowcontrol.Stream) bucket {
 	c.mu.Lock()
 	defer c.mu.Unlock()
 	return c.getBucketLocked(stream)
diff --git a/pkg/kv/kvserver/kvflowcontrol/kvflowcontroller/kvflowcontroller_metrics.go b/pkg/kv/kvserver/kvflowcontrol/kvflowcontroller/kvflowcontroller_metrics.go
index 5325e8a83f3..7b35f7a7341 100644
--- a/pkg/kv/kvserver/kvflowcontrol/kvflowcontroller/kvflowcontroller_metrics.go
+++ b/pkg/kv/kvserver/kvflowcontrol/kvflowcontroller/kvflowcontroller_metrics.go
@@ -157,8 +157,8 @@ func newMetrics(c *Controller) *metrics {
 				sum := int64(0)
 				c.mu.Lock()
 				defer c.mu.Unlock()
-				for _, b := range c.mu.buckets {
-					sum += int64(b.tokens(wc))
+				for _, wbc := range c.mu.buckets {
+					sum += int64(wbc.tokens[wc])
 				}
 				return sum
 			},
@@ -213,7 +213,7 @@ func newMetrics(c *Controller) *metrics {
 				defer c.mu.Unlock()
 
 				for s, wbc := range c.mu.buckets {
-					if wbc.tokens(wc) <= 0 {
+					if wbc.tokens[wc] <= 0 {
 						count += 1
 
 						if shouldLog {
diff --git a/pkg/kv/kvserver/rangefeed/catchup_scan.go b/pkg/kv/kvserver/rangefeed/catchup_scan.go
index a946f1fe971..990ea25ba35 100644
--- a/pkg/kv/kvserver/rangefeed/catchup_scan.go
+++ b/pkg/kv/kvserver/rangefeed/catchup_scan.go
@@ -82,30 +82,26 @@ func NewCatchUpIterator(
 	startTime hlc.Timestamp,
 	closer func(),
 	pacer *admission.Pacer,
-) (*CatchUpIterator, error) {
-	iter, err := storage.NewMVCCIncrementalIterator(reader,
-		storage.MVCCIncrementalIterOptions{
-			KeyTypes:  storage.IterKeyTypePointsAndRanges,
-			StartKey:  span.Key,
-			EndKey:    span.EndKey,
-			StartTime: startTime,
-			EndTime:   hlc.MaxTimestamp,
-			// We want to emit intents rather than error
-			// (the default behavior) so that we can skip
-			// over the provisional values during
-			// iteration.
-			IntentPolicy: storage.MVCCIncrementalIterIntentPolicyEmit,
-		})
-	if err != nil {
-		return nil, err
-	}
+) *CatchUpIterator {
 	return &CatchUpIterator{
-		simpleCatchupIter: iter,
-		close:             closer,
-		span:              span,
-		startTime:         startTime,
-		pacer:             pacer,
-	}, nil
+		simpleCatchupIter: storage.NewMVCCIncrementalIterator(reader.(storage.ReaderWithMustIterators),
+			storage.MVCCIncrementalIterOptions{
+				KeyTypes:  storage.IterKeyTypePointsAndRanges,
+				StartKey:  span.Key,
+				EndKey:    span.EndKey,
+				StartTime: startTime,
+				EndTime:   hlc.MaxTimestamp,
+				// We want to emit intents rather than error
+				// (the default behavior) so that we can skip
+				// over the provisional values during
+				// iteration.
+				IntentPolicy: storage.MVCCIncrementalIterIntentPolicyEmit,
+			}),
+		close:     closer,
+		span:      span,
+		startTime: startTime,
+		pacer:     pacer,
+	}
 }
 
 // Close closes the iterator and calls the instantiator-supplied close
diff --git a/pkg/kv/kvserver/rangefeed/catchup_scan_bench_test.go b/pkg/kv/kvserver/rangefeed/catchup_scan_bench_test.go
index f075fc2f9d4..ec1b7abbf37 100644
--- a/pkg/kv/kvserver/rangefeed/catchup_scan_bench_test.go
+++ b/pkg/kv/kvserver/rangefeed/catchup_scan_bench_test.go
@@ -50,13 +50,10 @@ func runCatchUpBenchmark(b *testing.B, emk engineMaker, opts benchOptions) (numE
 	b.ResetTimer()
 	for i := 0; i < b.N; i++ {
 		func() {
-			iter, err := rangefeed.NewCatchUpIterator(eng, span, opts.ts, nil, nil)
-			if err != nil {
-				b.Fatal(err)
-			}
+			iter := rangefeed.NewCatchUpIterator(eng, span, opts.ts, nil, nil)
 			defer iter.Close()
 			counter := 0
-			err = iter.CatchUpScan(ctx, func(*kvpb.RangeFeedEvent) error {
+			err := iter.CatchUpScan(ctx, func(*kvpb.RangeFeedEvent) error {
 				counter++
 				return nil
 			}, opts.withDiff)
diff --git a/pkg/kv/kvserver/rangefeed/catchup_scan_test.go b/pkg/kv/kvserver/rangefeed/catchup_scan_test.go
index 66380c25d6d..624767be276 100644
--- a/pkg/kv/kvserver/rangefeed/catchup_scan_test.go
+++ b/pkg/kv/kvserver/rangefeed/catchup_scan_test.go
@@ -113,8 +113,7 @@ func TestCatchupScan(t *testing.T) {
 	}
 	testutils.RunTrueAndFalse(t, "withDiff", func(t *testing.T, withDiff bool) {
 		span := roachpb.Span{Key: testKey1, EndKey: roachpb.KeyMax}
-		iter, err := NewCatchUpIterator(eng, span, ts1, nil, nil)
-		require.NoError(t, err)
+		iter := NewCatchUpIterator(eng, span, ts1, nil, nil)
 		defer iter.Close()
 		var events []kvpb.RangeFeedValue
 		// ts1 here is exclusive, so we do not want the versions at ts1.
@@ -157,11 +156,10 @@ func TestCatchupScanInlineError(t *testing.T) {
 
 	// Run a catchup scan across the span and watch it error.
 	span := roachpb.Span{Key: keys.LocalMax, EndKey: keys.MaxKey}
-	iter, err := NewCatchUpIterator(eng, span, hlc.Timestamp{}, nil, nil)
-	require.NoError(t, err)
+	iter := NewCatchUpIterator(eng, span, hlc.Timestamp{}, nil, nil)
 	defer iter.Close()
 
-	err = iter.CatchUpScan(ctx, nil, false)
+	err := iter.CatchUpScan(ctx, nil, false)
 	require.Error(t, err)
 	require.Contains(t, err.Error(), "unexpected inline value")
 }
@@ -198,8 +196,7 @@ func TestCatchupScanSeesOldIntent(t *testing.T) {
 
 	// Run a catchup scan across the span and watch it succeed.
 	span := roachpb.Span{Key: keys.LocalMax, EndKey: keys.MaxKey}
-	iter, err := NewCatchUpIterator(eng, span, tsCutoff, nil, nil)
-	require.NoError(t, err)
+	iter := NewCatchUpIterator(eng, span, tsCutoff, nil, nil)
 	defer iter.Close()
 
 	keys := map[string]struct{}{}
diff --git a/pkg/kv/kvserver/rangefeed/processor.go b/pkg/kv/kvserver/rangefeed/processor.go
index eb187be58c0..bbbc61012ae 100644
--- a/pkg/kv/kvserver/rangefeed/processor.go
+++ b/pkg/kv/kvserver/rangefeed/processor.go
@@ -304,7 +304,7 @@ type IntentScannerConstructor func() IntentScanner
 // for catchup-scans. Takes the key span and exclusive start time to run the
 // catchup scan for. It should be called from underneath a stopper task to
 // ensure that the engine has not been closed.
-type CatchUpIteratorConstructor func(roachpb.Span, hlc.Timestamp) (*CatchUpIterator, error)
+type CatchUpIteratorConstructor func(roachpb.Span, hlc.Timestamp) *CatchUpIterator
 
 // Start implements Processor interface.
 //
@@ -379,10 +379,7 @@ func (p *LegacyProcessor) run(
 			// Construct the catchUpIter before notifying the registration that it
 			// has been registered. Note that if the catchUpScan is never run, then
 			// the iterator constructed here will be closed in disconnect.
-			if err := r.maybeConstructCatchUpIter(); err != nil {
-				r.disconnect(kvpb.NewError(err))
-				return
-			}
+			r.maybeConstructCatchUpIter()
 
 			// Add the new registration to the registry.
 			p.reg.Register(&r)
diff --git a/pkg/kv/kvserver/rangefeed/registry.go b/pkg/kv/kvserver/rangefeed/registry.go
index 866fa852549..d6c41778900 100644
--- a/pkg/kv/kvserver/rangefeed/registry.go
+++ b/pkg/kv/kvserver/rangefeed/registry.go
@@ -590,21 +590,17 @@ func (r *registration) waitForCaughtUp() error {
 
 // maybeConstructCatchUpIter calls the catchUpIterConstructor and attaches
 // the catchUpIter to be detached in the catchUpScan or closed on disconnect.
-func (r *registration) maybeConstructCatchUpIter() error {
+func (r *registration) maybeConstructCatchUpIter() {
 	if r.catchUpIterConstructor == nil {
-		return nil
+		return
 	}
 
-	catchUpIter, err := r.catchUpIterConstructor(r.span, r.catchUpTimestamp)
-	if err != nil {
-		return err
-	}
+	catchUpIter := r.catchUpIterConstructor(r.span, r.catchUpTimestamp)
 	r.catchUpIterConstructor = nil
 
 	r.mu.Lock()
 	defer r.mu.Unlock()
 	r.mu.catchUpIter = catchUpIter
-	return nil
 }
 
 // detachCatchUpIter detaches the catchUpIter that was previously attached.
diff --git a/pkg/kv/kvserver/rangefeed/registry_test.go b/pkg/kv/kvserver/rangefeed/registry_test.go
index cf618d16ef5..7425974e29e 100644
--- a/pkg/kv/kvserver/rangefeed/registry_test.go
+++ b/pkg/kv/kvserver/rangefeed/registry_test.go
@@ -104,12 +104,12 @@ func makeCatchUpIteratorConstructor(iter storage.SimpleMVCCIterator) CatchUpIter
 	if iter == nil {
 		return nil
 	}
-	return func(span roachpb.Span, startTime hlc.Timestamp) (*CatchUpIterator, error) {
+	return func(span roachpb.Span, startTime hlc.Timestamp) *CatchUpIterator {
 		return &CatchUpIterator{
 			simpleCatchupIter: simpleCatchupIterAdapter{iter},
 			span:              span,
 			startTime:         startTime,
-		}, nil
+		}
 	}
 }
 
@@ -129,9 +129,7 @@ func newTestRegistration(
 		func() {},
 		&future.ErrorFuture{},
 	)
-	if err := r.maybeConstructCatchUpIter(); err != nil {
-		panic(err)
-	}
+	r.maybeConstructCatchUpIter()
 	return &testRegistration{
 		registration: r,
 		stream:       s,
diff --git a/pkg/kv/kvserver/replica_rangefeed.go b/pkg/kv/kvserver/replica_rangefeed.go
index 7def586f7dc..60028ad5da2 100644
--- a/pkg/kv/kvserver/replica_rangefeed.go
+++ b/pkg/kv/kvserver/replica_rangefeed.go
@@ -245,18 +245,15 @@ func (r *Replica) RangeFeed(
 	// Register the stream with a catch-up iterator.
 	var catchUpIterFunc rangefeed.CatchUpIteratorConstructor
 	if usingCatchUpIter {
-		catchUpIterFunc = func(span roachpb.Span, startTime hlc.Timestamp) (*rangefeed.CatchUpIterator, error) {
+		catchUpIterFunc = func(span roachpb.Span, startTime hlc.Timestamp) *rangefeed.CatchUpIterator {
 			// Assert that we still hold the raftMu when this is called to ensure
 			// that the catchUpIter reads from the current snapshot.
 			r.raftMu.AssertHeld()
-			i, err := rangefeed.NewCatchUpIterator(r.store.TODOEngine(), span, startTime, iterSemRelease, pacer)
-			if err != nil {
-				return nil, err
-			}
+			i := rangefeed.NewCatchUpIterator(r.store.TODOEngine(), span, startTime, iterSemRelease, pacer)
 			if f := r.store.TestingKnobs().RangefeedValueHeaderFilter; f != nil {
 				i.OnEmit = f
 			}
-			return i, nil
+			return i
 		}
 	}
 	var done future.ErrorFuture
diff --git a/pkg/kv/kvserver/spanset/batch.go b/pkg/kv/kvserver/spanset/batch.go
index fb6b1d4d933..3324d02979f 100644
--- a/pkg/kv/kvserver/spanset/batch.go
+++ b/pkg/kv/kvserver/spanset/batch.go
@@ -496,6 +496,16 @@ func (s spanSetReader) NewMVCCIterator(
 	return NewIteratorAt(mvccIter, s.spans, s.ts), nil
 }
 
+func (s spanSetReader) MustMVCCIterator(
+	iterKind storage.MVCCIterKind, opts storage.IterOptions,
+) storage.MVCCIterator {
+	iter, err := s.NewMVCCIterator(iterKind, opts)
+	if err != nil {
+		panic(err)
+	}
+	return iter
+}
+
 func (s spanSetReader) NewEngineIterator(opts storage.IterOptions) (storage.EngineIterator, error) {
 	engineIter, err := s.r.NewEngineIterator(opts)
 	if err != nil {
@@ -509,6 +519,14 @@ func (s spanSetReader) NewEngineIterator(opts storage.IterOptions) (storage.Engi
 	}, nil
 }
 
+func (s spanSetReader) MustEngineIterator(opts storage.IterOptions) storage.EngineIterator {
+	iter, err := s.NewEngineIterator(opts)
+	if err != nil {
+		panic(err)
+	}
+	return iter
+}
+
 // ConsistentIterators implements the storage.Reader interface.
 func (s spanSetReader) ConsistentIterators() bool {
 	return s.r.ConsistentIterators()
diff --git a/pkg/roachprod/install/cluster_synced.go b/pkg/roachprod/install/cluster_synced.go
index 975fd25b2f7..db66b2851f2 100644
--- a/pkg/roachprod/install/cluster_synced.go
+++ b/pkg/roachprod/install/cluster_synced.go
@@ -1454,9 +1454,9 @@ func (c *SyncedCluster) DistributeCerts(ctx context.Context, l *logger.Logger) e
 rm -fr certs
 mkdir -p certs
 VERSION=$(%[1]s version --build-tag)
-VERSION=${VERSION::5}
+VERSION=${VERSION::3}
 TENANT_SCOPE_OPT=""
-if [[ $VERSION = v22.2 ]]; then
+if [[ $VERSION = v22 ]]; then
        TENANT_SCOPE_OPT="--tenant-scope 1,2,3,4,11,12,13,14"
 fi
 %[1]s cert create-ca --certs-dir=certs --ca-key=certs/ca.key
diff --git a/pkg/sql/logictest/testdata/logic_test/udf_plpgsql b/pkg/sql/logictest/testdata/logic_test/udf_plpgsql
index 2f6c5536cfd..7a19e37e8d6 100644
--- a/pkg/sql/logictest/testdata/logic_test/udf_plpgsql
+++ b/pkg/sql/logictest/testdata/logic_test/udf_plpgsql
@@ -282,7 +282,7 @@ CREATE OR REPLACE FUNCTION f(a INT, b INT) RETURNS INT AS $$
   BEGIN
     LOOP
       IF i >= b THEN EXIT; END IF;
-      IF i = 2 THEN
+      IF i = 2 THEN 
         i := i + 1;
         CONTINUE;
       END IF;
@@ -332,78 +332,6 @@ SELECT f(1, 5), f(-5, 5), f(0, 1)
 # TODO(drewk): add back the dijkstra test once UDFs calling other UDFs is
 # allowed.
 
-# --------------------------------------------------
-# Tests for WHILE LOOP
-# --------------------------------------------------
-
-subtest while
-
-statement ok
-CREATE OR REPLACE FUNCTION f(n INT) RETURNS INT AS $$
-  DECLARE
-    i INT := 0;
-    sum INT := 0;
-  BEGIN
-    WHILE i <= n LOOP
-      sum := sum + i;
-      i := i + 1;
-    END LOOP;
-    RETURN sum;
-  END
-$$ LANGUAGE PLpgSQL;
-
-query IIII
-SELECT f(0), f(1), f(2), f(10);
-----
-0  1  3  55
-
-statement ok
-CREATE OR REPLACE FUNCTION f(n INT) RETURNS INT AS $$
-  DECLARE
-    i INT := 0;
-    sum INT := 0;
-  BEGIN
-    WHILE i <= n LOOP
-      IF i > 10 THEN
-        EXIT;
-      END IF;
-      sum := sum + i;
-      i := i + 1;
-    END LOOP;
-    RETURN sum;
-  END
-$$ LANGUAGE PLpgSQL;
-
-query II
-SELECT f(5), f(20)
-----
-15  55
-
-statement ok
-CREATE OR REPLACE FUNCTION f(n INT) RETURNS INT AS $$
-  DECLARE
-    i INT := 0;
-    sum INT := 0;
-  BEGIN
-    WHILE i <= n LOOP
-      IF i % 2 = 0 THEN
-        i := i + 1;
-        CONTINUE;
-      END IF;
-      sum := sum + i;
-      i := i + 1;
-    END LOOP;
-    RETURN sum;
-  END
-$$ LANGUAGE PLpgSQL;
-
-query IIII
-SELECT f(0), f(1), f(2), f(5)
-----
-0  1  1  9
-
-subtest end
-
 statement ok
 CREATE OR REPLACE FUNCTION f(a INT, b INT) RETURNS INT AS $$
   BEGIN
diff --git a/pkg/sql/opt/optbuilder/plpgsql.go b/pkg/sql/opt/optbuilder/plpgsql.go
index 692867b9af9..10f82f2a241 100644
--- a/pkg/sql/opt/optbuilder/plpgsql.go
+++ b/pkg/sql/opt/optbuilder/plpgsql.go
@@ -19,7 +19,7 @@ import (
 	"github.com/cockroachdb/cockroach/pkg/sql/pgwire/pgcode"
 	"github.com/cockroachdb/cockroach/pkg/sql/pgwire/pgerror"
 	"github.com/cockroachdb/cockroach/pkg/sql/sem/builtins/builtinsregistry"
-	ast "github.com/cockroachdb/cockroach/pkg/sql/sem/plpgsqltree"
+	"github.com/cockroachdb/cockroach/pkg/sql/sem/plpgsqltree"
 	"github.com/cockroachdb/cockroach/pkg/sql/sem/tree"
 	"github.com/cockroachdb/cockroach/pkg/sql/sem/volatility"
 	"github.com/cockroachdb/cockroach/pkg/sql/types"
@@ -115,7 +115,7 @@ type plpgsqlBuilder struct {
 	params []tree.ParamType
 
 	// decls is the set of variable declarations for a PL/pgSQL function.
-	decls []ast.Declaration
+	decls []plpgsqltree.PLpgSQLDecl
 
 	// varTypes maps from the name of each variable to its type.
 	varTypes map[tree.Name]*types.T
@@ -147,7 +147,11 @@ type plpgsqlBuilder struct {
 }
 
 func (b *plpgsqlBuilder) init(
-	ob *Builder, colRefs *opt.ColSet, params []tree.ParamType, block *ast.Block, returnType *types.T,
+	ob *Builder,
+	colRefs *opt.ColSet,
+	params []tree.ParamType,
+	block *plpgsqltree.PLpgSQLStmtBlock,
+	returnType *types.T,
 ) {
 	b.ob = ob
 	b.colRefs = colRefs
@@ -178,7 +182,7 @@ func (b *plpgsqlBuilder) init(
 
 // build constructs an expression that returns the result of executing a
 // PL/pgSQL function. See buildPLpgSQLStatements for more details.
-func (b *plpgsqlBuilder) build(block *ast.Block, s *scope) *scope {
+func (b *plpgsqlBuilder) build(block *plpgsqltree.PLpgSQLStmtBlock, s *scope) *scope {
 	s = s.push()
 	b.ensureScopeHasExpr(s)
 
@@ -220,11 +224,13 @@ func (b *plpgsqlBuilder) build(block *ast.Block, s *scope) *scope {
 //
 // buildPLpgSQLStatements returns nil if one or more branches in the given
 // statements do not eventually terminate with a RETURN statement.
-func (b *plpgsqlBuilder) buildPLpgSQLStatements(stmts []ast.Statement, s *scope) *scope {
+func (b *plpgsqlBuilder) buildPLpgSQLStatements(
+	stmts []plpgsqltree.PLpgSQLStatement, s *scope,
+) *scope {
 	b.ensureScopeHasExpr(s)
 	for i, stmt := range stmts {
 		switch t := stmt.(type) {
-		case *ast.Return:
+		case *plpgsqltree.PLpgSQLStmtReturn:
 			// RETURN is handled by projecting a single column with the expression
 			// that is being returned.
 			returnScalar := b.buildPLpgSQLExpr(t.Expr, b.returnType, s)
@@ -234,8 +240,7 @@ func (b *plpgsqlBuilder) buildPLpgSQLStatements(stmts []ast.Statement, s *scope)
 			b.ob.synthesizeColumn(returnScope, returnColName, b.returnType, nil /* expr */, returnScalar)
 			b.ob.constructProjectForScope(s, returnScope)
 			return returnScope
-
-		case *ast.Assignment:
+		case *plpgsqltree.PLpgSQLStmtAssign:
 			// Assignment (:=) is handled by projecting a new column with the same
 			// name as the variable being assigned.
 			s = b.addPLpgSQLAssign(s, t.Var, t.Value)
@@ -249,8 +254,7 @@ func (b *plpgsqlBuilder) buildPLpgSQLStatements(stmts []ast.Statement, s *scope)
 				b.appendPlpgSQLStmts(&catchCon, stmts[i+1:])
 				return b.callContinuation(&catchCon, s)
 			}
-
-		case *ast.If:
+		case *plpgsqltree.PLpgSQLStmtIf:
 			// IF statement control flow is handled by calling a "continuation"
 			// function in each branch that executes all the statements that logically
 			// follow the IF statement block.
@@ -310,8 +314,7 @@ func (b *plpgsqlBuilder) buildPLpgSQLStatements(stmts []ast.Statement, s *scope)
 			b.ob.synthesizeColumn(returnScope, returnColName, b.returnType, nil /* expr */, scalar)
 			b.ob.constructProjectForScope(s, returnScope)
 			return returnScope
-
-		case *ast.Loop:
+		case *plpgsqltree.PLpgSQLStmtSimpleLoop:
 			if t.Label != "" {
 				panic(unimplemented.New(
 					"LOOP label",
@@ -337,49 +340,13 @@ func (b *plpgsqlBuilder) buildPLpgSQLStatements(stmts []ast.Statement, s *scope)
 			b.popContinuation()
 			b.popExitContinuation()
 			return b.callContinuation(&loopContinuation, s)
-
-		case *ast.While:
-			// A WHILE LOOP is syntactic sugar for a LOOP with a conditional
-			// EXIT, so it is handled by a simple rewrite:
-			//
-			//   WHILE [cond] LOOP
-			//     [body];
-			//   END LOOP;
-			//   =>
-			//   LOOP
-			//     IF [cond] THEN
-			//       [body];
-			//     ELSE
-			//       EXIT;
-			//     END IF;
-			//   END LOOP;
-			//
-			loop := &ast.Loop{
-				Label: t.Label,
-				Body: []ast.Statement{&ast.If{
-					Condition: t.Condition,
-					ThenBody:  t.Body,
-					ElseBody:  []ast.Statement{&ast.Exit{}},
-				}},
-			}
-			newStmts := make([]ast.Statement, 0, len(stmts))
-			newStmts = append(newStmts, loop)
-			newStmts = append(newStmts, stmts[i+1:]...)
-			return b.buildPLpgSQLStatements(newStmts, s)
-
-		case *ast.Exit:
+		case *plpgsqltree.PLpgSQLStmtExit:
 			if t.Label != "" {
 				panic(unimplemented.New(
 					"EXIT label",
 					"EXIT statement labels are not yet supported",
 				))
 			}
-			if t.Condition != nil {
-				panic(unimplemented.New(
-					"EXIT WHEN",
-					"conditional EXIT statements are not yet supported",
-				))
-			}
 			// EXIT statements are handled by calling the function that executes the
 			// statements after a loop. Errors if used outside a loop.
 			if con := b.getExitContinuation(); con != nil {
@@ -390,20 +357,13 @@ func (b *plpgsqlBuilder) buildPLpgSQLStatements(stmts []ast.Statement, s *scope)
 					"EXIT cannot be used outside a loop, unless it has a label",
 				))
 			}
-
-		case *ast.Continue:
+		case *plpgsqltree.PLpgSQLStmtContinue:
 			if t.Label != "" {
 				panic(unimplemented.New(
 					"CONTINUE label",
 					"CONTINUE statement labels are not yet supported",
 				))
 			}
-			if t.Condition != nil {
-				panic(unimplemented.New(
-					"CONTINUE WHEN",
-					"conditional CONTINUE statements are not yet supported",
-				))
-			}
 			// CONTINUE statements are handled by calling the function that executes
 			// the loop body. Errors if used outside a loop.
 			if con := b.getLoopContinuation(); con != nil {
@@ -411,8 +371,7 @@ func (b *plpgsqlBuilder) buildPLpgSQLStatements(stmts []ast.Statement, s *scope)
 			} else {
 				panic(pgerror.New(pgcode.Syntax, "CONTINUE cannot be used outside a loop"))
 			}
-
-		case *ast.Raise:
+		case *plpgsqltree.PLpgSQLStmtRaise:
 			// RAISE statements allow the PLpgSQL function to send an error or a
 			// notice to the client. We handle these side effects by building them
 			// into a separate body statement that is only executed for its side
@@ -425,8 +384,7 @@ func (b *plpgsqlBuilder) buildPLpgSQLStatements(stmts []ast.Statement, s *scope)
 			b.appendBodyStmt(&con, b.buildPLpgSQLRaise(con.s, b.getRaiseArgs(con.s, t)))
 			b.appendPlpgSQLStmts(&con, stmts[i+1:])
 			return b.callContinuation(&con, s)
-
-		case *ast.Execute:
+		case *plpgsqltree.PLpgSQLStmtExecSql:
 			if t.Strict {
 				panic(unimplemented.NewWithIssuef(107854,
 					"INTO STRICT statements are not yet implemented",
@@ -498,7 +456,6 @@ func (b *plpgsqlBuilder) buildPLpgSQLStatements(stmts []ast.Statement, s *scope)
 			// Step 3: call the INTO continuation from the parent scope.
 			b.appendBodyStmt(&execCon, intoScope)
 			return b.callContinuation(&execCon, s)
-
 		default:
 			panic(unimplemented.New(
 				"unimplemented PL/pgSQL statement",
@@ -514,7 +471,9 @@ func (b *plpgsqlBuilder) buildPLpgSQLStatements(stmts []ast.Statement, s *scope)
 // new column with the variable name that projects the assigned expression.
 // If there is a column with the same name in the previous scope, it will be
 // replaced. This allows the plpgsqlBuilder to model variable mutations.
-func (b *plpgsqlBuilder) addPLpgSQLAssign(inScope *scope, ident ast.Variable, val ast.Expr) *scope {
+func (b *plpgsqlBuilder) addPLpgSQLAssign(
+	inScope *scope, ident plpgsqltree.PLpgSQLVariable, val plpgsqltree.PLpgSQLExpr,
+) *scope {
 	typ := b.resolveVariableForAssign(ident)
 	assignScope := inScope.push()
 	b.ensureScopeHasExpr(assignScope)
@@ -563,7 +522,9 @@ func (b *plpgsqlBuilder) buildPLpgSQLRaise(inScope *scope, args memo.ScalarListE
 // getRaiseArgs validates the options attached to the given PLpgSQL RAISE
 // statement and returns the arguments to be used for a call to the
 // crdb_internal.plpgsql_raise builtin function.
-func (b *plpgsqlBuilder) getRaiseArgs(s *scope, raise *ast.Raise) memo.ScalarListExpr {
+func (b *plpgsqlBuilder) getRaiseArgs(
+	s *scope, raise *plpgsqltree.PLpgSQLStmtRaise,
+) memo.ScalarListExpr {
 	var severity, message, detail, hint, code opt.ScalarExpr
 	makeConstStr := func(str string) opt.ScalarExpr {
 		return b.ob.factory.ConstructConstVal(tree.NewDString(str), types.String)
@@ -604,7 +565,7 @@ func (b *plpgsqlBuilder) getRaiseArgs(s *scope, raise *ast.Raise) memo.ScalarLis
 		code = makeConstStr(raise.CodeName)
 	}
 	// Retrieve the RAISE options, if any.
-	buildOptionExpr := func(name string, expr ast.Expr, isDup bool) opt.ScalarExpr {
+	buildOptionExpr := func(name string, expr plpgsqltree.PLpgSQLExpr, isDup bool) opt.ScalarExpr {
 		if isDup {
 			panic(pgerror.Newf(pgcode.Syntax, "RAISE option already specified: %s", name))
 		}
@@ -653,7 +614,7 @@ func (b *plpgsqlBuilder) getRaiseArgs(s *scope, raise *ast.Raise) memo.ScalarLis
 // is specified by doubling it: '%%'. The formatting arguments can be arbitrary
 // SQL expressions.
 func (b *plpgsqlBuilder) makeRaiseFormatMessage(
-	s *scope, format string, args []ast.Expr,
+	s *scope, format string, args []plpgsqltree.PLpgSQLExpr,
 ) (result opt.ScalarExpr) {
 	makeConstStr := func(str string) opt.ScalarExpr {
 		return b.ob.factory.ConstructConstVal(tree.NewDString(str), types.String)
@@ -769,7 +730,7 @@ func (b *plpgsqlBuilder) makeRaiseFormatMessage(
 // TODO(drewk): We could make a special case for exception handling, since the
 // exception handler is the same across all PLpgSQL routines. This would allow
 // us to only set the exception handler for the two cases above.
-func (b *plpgsqlBuilder) buildExceptions(block *ast.Block) {
+func (b *plpgsqlBuilder) buildExceptions(block *plpgsqltree.PLpgSQLStmtBlock) {
 	if len(block.Exceptions) == 0 {
 		return
 	}
@@ -893,7 +854,9 @@ func (b *plpgsqlBuilder) appendBodyStmt(con *continuation, bodyScope *scope) {
 // appendPlpgSQLStmts builds the given PLpgSQL statements into a relational
 // expression and appends it to the given continuation routine's body statements
 // list.
-func (b *plpgsqlBuilder) appendPlpgSQLStmts(con *continuation, stmts []ast.Statement) {
+func (b *plpgsqlBuilder) appendPlpgSQLStmts(
+	con *continuation, stmts []plpgsqltree.PLpgSQLStatement,
+) {
 	// Make sure to push s before constructing the continuation scope to ensure
 	// that the parameter columns are not projected.
 	continuationScope := b.buildPLpgSQLStatements(stmts, con.s.push())
@@ -939,7 +902,9 @@ func (b *plpgsqlBuilder) callContinuation(con *continuation, s *scope) *scope {
 
 // buildPLpgSQLExpr parses and builds the given SQL expression into a ScalarExpr
 // within the given scope.
-func (b *plpgsqlBuilder) buildPLpgSQLExpr(expr ast.Expr, typ *types.T, s *scope) opt.ScalarExpr {
+func (b *plpgsqlBuilder) buildPLpgSQLExpr(
+	expr plpgsqltree.PLpgSQLExpr, typ *types.T, s *scope,
+) opt.ScalarExpr {
 	expr, _ = tree.WalkExpr(s, expr)
 	typedExpr, err := expr.TypeCheck(b.ob.ctx, b.ob.semaCtx, typ)
 	if err != nil {
diff --git a/pkg/sql/opt/optbuilder/testdata/udf_plpgsql b/pkg/sql/opt/optbuilder/testdata/udf_plpgsql
index eacc4350cae..79c8e9629eb 100644
--- a/pkg/sql/opt/optbuilder/testdata/udf_plpgsql
+++ b/pkg/sql/opt/optbuilder/testdata/udf_plpgsql
@@ -1166,7 +1166,7 @@ CREATE OR REPLACE FUNCTION f(a INT, b INT) RETURNS INT AS $$
   BEGIN
     LOOP
       IF i >= b THEN EXIT; END IF;
-      IF i = 2 THEN
+      IF i = 2 THEN 
         i := i + 1;
         CONTINUE;
       END IF;
@@ -1547,166 +1547,6 @@ project
                      │                                                                                                                                      └── recursive-call
                      └── const: 1
 
-exec-ddl
-CREATE OR REPLACE FUNCTION f(n INT) RETURNS INT AS $$
-  DECLARE
-    sum INT := 0;
-    i INT := 0;
-  BEGIN
-    WHILE i < n LOOP
-      sum := sum + i;
-      i := i + 1;
-    END LOOP;
-    RETURN sum;
-  END
-$$ LANGUAGE PLpgSQL;
-----
-
-build format=show-scalars
-SELECT f(5)
-----
-project
- ├── columns: f:21
- ├── values
- │    └── tuple
- └── projections
-      └── udf: f [as=f:21]
-           ├── args
-           │    └── const: 5
-           ├── params: n:1
-           └── body
-                └── limit
-                     ├── columns: stmt_loop_3:20
-                     ├── project
-                     │    ├── columns: stmt_loop_3:20
-                     │    ├── project
-                     │    │    ├── columns: i:3!null sum:2!null
-                     │    │    ├── project
-                     │    │    │    ├── columns: sum:2!null
-                     │    │    │    ├── values
-                     │    │    │    │    └── tuple
-                     │    │    │    └── projections
-                     │    │    │         └── const: 0 [as=sum:2]
-                     │    │    └── projections
-                     │    │         └── const: 0 [as=i:3]
-                     │    └── projections
-                     │         └── udf: stmt_loop_3 [as=stmt_loop_3:20]
-                     │              ├── args
-                     │              │    ├── variable: sum:2
-                     │              │    ├── variable: i:3
-                     │              │    └── variable: n:1
-                     │              ├── params: sum:8 i:9 n:10
-                     │              └── body
-                     │                   └── project
-                     │                        ├── columns: stmt_if_5:19
-                     │                        ├── values
-                     │                        │    └── tuple
-                     │                        └── projections
-                     │                             └── case [as=stmt_if_5:19]
-                     │                                  ├── true
-                     │                                  ├── when
-                     │                                  │    ├── lt
-                     │                                  │    │    ├── variable: i:9
-                     │                                  │    │    └── variable: n:10
-                     │                                  │    └── subquery
-                     │                                  │         └── project
-                     │                                  │              ├── columns: stmt_if_4:17
-                     │                                  │              ├── project
-                     │                                  │              │    ├── columns: i:16 sum:15
-                     │                                  │              │    ├── project
-                     │                                  │              │    │    ├── columns: sum:15
-                     │                                  │              │    │    ├── values
-                     │                                  │              │    │    │    └── tuple
-                     │                                  │              │    │    └── projections
-                     │                                  │              │    │         └── plus [as=sum:15]
-                     │                                  │              │    │              ├── variable: sum:8
-                     │                                  │              │    │              └── variable: i:9
-                     │                                  │              │    └── projections
-                     │                                  │              │         └── plus [as=i:16]
-                     │                                  │              │              ├── variable: i:9
-                     │                                  │              │              └── const: 1
-                     │                                  │              └── projections
-                     │                                  │                   └── udf: stmt_if_4 [as=stmt_if_4:17]
-                     │                                  │                        ├── args
-                     │                                  │                        │    ├── variable: sum:15
-                     │                                  │                        │    ├── variable: i:16
-                     │                                  │                        │    └── variable: n:10
-                     │                                  │                        ├── params: sum:11 i:12 n:13
-                     │                                  │                        └── body
-                     │                                  │                             └── project
-                     │                                  │                                  ├── columns: stmt_loop_3:14
-                     │                                  │                                  ├── values
-                     │                                  │                                  │    └── tuple
-                     │                                  │                                  └── projections
-                     │                                  │                                       └── udf: stmt_loop_3 [as=stmt_loop_3:14]
-                     │                                  │                                            ├── args
-                     │                                  │                                            │    ├── variable: sum:11
-                     │                                  │                                            │    ├── variable: i:12
-                     │                                  │                                            │    └── variable: n:13
-                     │                                  │                                            └── recursive-call
-                     │                                  └── subquery
-                     │                                       └── project
-                     │                                            ├── columns: loop_exit_1:18
-                     │                                            ├── values
-                     │                                            │    └── tuple
-                     │                                            └── projections
-                     │                                                 └── udf: loop_exit_1 [as=loop_exit_1:18]
-                     │                                                      ├── args
-                     │                                                      │    ├── variable: sum:8
-                     │                                                      │    ├── variable: i:9
-                     │                                                      │    └── variable: n:10
-                     │                                                      ├── params: sum:4 i:5 n:6
-                     │                                                      └── body
-                     │                                                           └── project
-                     │                                                                ├── columns: stmt_return_2:7
-                     │                                                                ├── values
-                     │                                                                │    └── tuple
-                     │                                                                └── projections
-                     │                                                                     └── variable: sum:4 [as=stmt_return_2:7]
-                     └── const: 1
-
-exec-ddl
-CREATE OR REPLACE FUNCTION f(n INT) RETURNS INT AS $$
-  DECLARE
-    sum INT := 0;
-    i INT := 0;
-  BEGIN
-    LOOP
-      EXIT WHEN i > n;
-      sum := sum + i;
-      i := i + 1;
-    END LOOP;
-    RETURN sum;
-  END
-$$ LANGUAGE PLpgSQL;
-----
-
-build format=show-scalars
-SELECT f(5)
-----
-error (0A000): unimplemented: conditional EXIT statements are not yet supported
-
-exec-ddl
-CREATE OR REPLACE FUNCTION f(n INT) RETURNS INT AS $$
-  DECLARE
-    sum INT := 0;
-    i INT := 0;
-  BEGIN
-    LOOP
-      CONTINUE WHEN i % 2 = 0;
-      sum := sum + i;
-      i := i + 1;
-    END LOOP;
-    RETURN sum;
-  END
-$$ LANGUAGE PLpgSQL;
-----
-
-build format=show-scalars
-SELECT f(5)
-----
-error (0A000): unimplemented: conditional CONTINUE statements are not yet supported
-
 # TODO(drewk): consider adding a norm rules to fold nested UDFs.
 # Testing RAISE statements.
 exec-ddl
diff --git a/pkg/sql/parser/statements/statement.go b/pkg/sql/parser/statements/statement.go
index c4b4111bd0d..ebccfd9dd2f 100644
--- a/pkg/sql/parser/statements/statement.go
+++ b/pkg/sql/parser/statements/statement.go
@@ -66,7 +66,7 @@ func IsANSIDML(stmt tree.Statement) bool {
 // Statements is a list of parsed statements.
 type Statements []Statement[tree.Statement]
 
-type PLpgStatement Statement[*plpgsqltree.Block]
+type PLpgStatement Statement[*plpgsqltree.PLpgSQLStmtBlock]
 
 // String returns the AST formatted as a string.
 func (stmts Statements) String() string {
diff --git a/pkg/sql/plpgsql/parser/lexer.go b/pkg/sql/plpgsql/parser/lexer.go
index 442f85d5e0e..edfa9e0a949 100644
--- a/pkg/sql/plpgsql/parser/lexer.go
+++ b/pkg/sql/plpgsql/parser/lexer.go
@@ -35,7 +35,7 @@ type lexer struct {
 	// token returned by Lex().
 	lastPos int
 
-	stmt *plpgsqltree.Block
+	stmt *plpgsqltree.PLpgSQLStmtBlock
 
 	// numPlaceholders is 1 + the highest placeholder index encountered.
 	numPlaceholders int
@@ -120,8 +120,8 @@ func (l *lexer) Lex(lval *plpgsqlSymType) int {
 	return int(lval.id)
 }
 
-// MakeExecSqlStmt makes an Execute node.
-func (l *lexer) MakeExecSqlStmt() (*plpgsqltree.Execute, error) {
+// MakeExecSqlStmt makes a PLpgSQLStmtExecSql from current token position.
+func (l *lexer) MakeExecSqlStmt() (*plpgsqltree.PLpgSQLStmtExecSql, error) {
 	if l.parser.Lookahead() != -1 {
 		// Push back the lookahead token so that it can be included.
 		l.PushBack(1)
@@ -137,7 +137,7 @@ func (l *lexer) MakeExecSqlStmt() (*plpgsqltree.Execute, error) {
 
 	var haveInto, haveStrict bool
 	var intoStartPos, intoEndPos int
-	var target []plpgsqltree.Variable
+	var target []plpgsqltree.PLpgSQLVariable
 	firstTok := l.tokens[startPos]
 	tok := firstTok
 	for pos := startPos; pos < endPos; pos++ {
@@ -164,7 +164,7 @@ func (l *lexer) MakeExecSqlStmt() (*plpgsqltree.Execute, error) {
 				if tok.id != IDENT {
 					return nil, errors.Newf("\"%s\" is not a scalar variable", tok.str)
 				}
-				variable := plpgsqltree.Variable(strings.TrimSpace(l.getStr(pos, pos+1)))
+				variable := plpgsqltree.PLpgSQLVariable(strings.TrimSpace(l.getStr(pos, pos+1)))
 				target = append(target, variable)
 				if pos+1 == endPos || l.tokens[pos+1].id != ',' {
 					// This is the end of the target list.
@@ -193,16 +193,16 @@ func (l *lexer) MakeExecSqlStmt() (*plpgsqltree.Execute, error) {
 	if target != nil && sqlStmt.AST.StatementReturnType() != tree.Rows {
 		return nil, pgerror.New(pgcode.Syntax, "INTO used with a command that cannot return data")
 	}
-	return &plpgsqltree.Execute{
+	return &plpgsqltree.PLpgSQLStmtExecSql{
 		SqlStmt: sqlStmt.AST,
 		Strict:  haveStrict,
 		Target:  target,
 	}, nil
 }
 
-func (l *lexer) MakeDynamicExecuteStmt() *plpgsqltree.DynamicExecute {
+func (l *lexer) MakeDynamicExecuteStmt() *plpgsqltree.PLpgSQLStmtDynamicExecute {
 	cmdStr, _ := l.ReadSqlConstruct(INTO, USING, ';')
-	ret := &plpgsqltree.DynamicExecute{
+	ret := &plpgsqltree.PLpgSQLStmtDynamicExecute{
 		Query: cmdStr,
 	}
 
@@ -227,7 +227,7 @@ func (l *lexer) MakeDynamicExecuteStmt() *plpgsqltree.DynamicExecute {
 			if ret.Params != nil {
 				l.setErr(errors.AssertionFailedf("seen multiple USINGs"))
 			}
-			ret.Params = make([]plpgsqltree.Expr, 0)
+			ret.Params = make([]plpgsqltree.PLpgSQLExpr, 0)
 			for {
 				l.ReadSqlConstruct(',', ';', INTO)
 				ret.Params = append(ret.Params, nil)
@@ -246,19 +246,19 @@ func (l *lexer) MakeDynamicExecuteStmt() *plpgsqltree.DynamicExecute {
 	return ret
 }
 
-func (l *lexer) ProcessForOpenCursor(nullCursorExplicitExpr bool) *plpgsqltree.Open {
-	openStmt := &plpgsqltree.Open{}
-	openStmt.CursorOptions = plpgsqltree.CursorOptionFastPlan.Mask()
+func (l *lexer) ProcessForOpenCursor(nullCursorExplicitExpr bool) *plpgsqltree.PLpgSQLStmtOpen {
+	openStmt := &plpgsqltree.PLpgSQLStmtOpen{}
+	openStmt.CursorOptions = plpgsqltree.PLpgSQLCursorOptFastPlan.Mask()
 
 	if nullCursorExplicitExpr {
 		if l.Peek().id == NO {
 			l.lastPos++
 			if l.Peek().id == SCROLL {
-				openStmt.CursorOptions |= plpgsqltree.CursorOptionNoScroll.Mask()
+				openStmt.CursorOptions |= plpgsqltree.PLpgSQLCursorOptNoScroll.Mask()
 				l.lastPos++
 			}
 		} else if l.Peek().id == SCROLL {
-			openStmt.CursorOptions |= plpgsqltree.CursorOptionScroll.Mask()
+			openStmt.CursorOptions |= plpgsqltree.PLpgSQLCursorOptScroll.Mask()
 			l.lastPos++
 		}
 
@@ -380,7 +380,7 @@ func (l *lexer) getStr(startPos, endPos int) string {
 	return l.in[start:end]
 }
 
-func (l *lexer) ProcessQueryForCursorWithoutExplicitExpr(openStmt *plpgsqltree.Open) {
+func (l *lexer) ProcessQueryForCursorWithoutExplicitExpr(openStmt *plpgsqltree.PLpgSQLStmtOpen) {
 	l.lastPos++
 	if int(l.Peek().id) == EXECUTE {
 		dynamicQuery, endToken := l.ReadSqlExpressionStr2(USING, ';')
@@ -440,8 +440,8 @@ func (l *lexer) lastToken() plpgsqlSymType {
 }
 
 // SetStmt is called from the parser when the statement is constructed.
-func (l *lexer) SetStmt(stmt plpgsqltree.Statement) {
-	l.stmt = stmt.(*plpgsqltree.Block)
+func (l *lexer) SetStmt(stmt plpgsqltree.PLpgSQLStatement) {
+	l.stmt = stmt.(*plpgsqltree.PLpgSQLStmtBlock)
 }
 
 // setErr is called from parsing action rules to register an error observed
@@ -476,6 +476,6 @@ func (l *lexer) GetTypeFromValidSQLSyntax(sqlStr string) (tree.ResolvableTypeRef
 	return parser.GetTypeFromValidSQLSyntax(sqlStr)
 }
 
-func (l *lexer) ParseExpr(sqlStr string) (plpgsqltree.Expr, error) {
+func (l *lexer) ParseExpr(sqlStr string) (plpgsqltree.PLpgSQLExpr, error) {
 	return parser.ParseExpr(sqlStr)
 }
diff --git a/pkg/sql/plpgsql/parser/parser_test.go b/pkg/sql/plpgsql/parser/parser_test.go
index 191c8b7d907..3a42f69a9f3 100644
--- a/pkg/sql/plpgsql/parser/parser_test.go
+++ b/pkg/sql/plpgsql/parser/parser_test.go
@@ -21,7 +21,7 @@ import (
 
 // TODO: Define(if possible) a data driven test framework so that sql and
 // plpgsql share a parse test
-func TestParseDataDriven(t *testing.T) {
+func TestParseDataDriver(t *testing.T) {
 	datadriven.Walk(t, datapathutils.TestDataPath(t), func(t *testing.T, path string) {
 		datadriven.RunTest(t, path, func(t *testing.T, d *datadriven.TestData) string {
 			switch d.Cmd {
diff --git a/pkg/sql/plpgsql/parser/plpgsql.y b/pkg/sql/plpgsql/parser/plpgsql.y
index 9cfc3144330..e4dca0b3ec6 100644
--- a/pkg/sql/plpgsql/parser/plpgsql.y
+++ b/pkg/sql/plpgsql/parser/plpgsql.y
@@ -62,24 +62,24 @@ type plpgsqlSymUnion struct {
     val interface{}
 }
 
-func (u *plpgsqlSymUnion) block() *plpgsqltree.Block {
-    return u.val.(*plpgsqltree.Block)
+func (u *plpgsqlSymUnion) plpgsqlStmtBlock() *plpgsqltree.PLpgSQLStmtBlock {
+    return u.val.(*plpgsqltree.PLpgSQLStmtBlock)
 }
 
-func (u *plpgsqlSymUnion) caseWhen() *plpgsqltree.CaseWhen {
-    return u.val.(*plpgsqltree.CaseWhen)
+func (u *plpgsqlSymUnion) plpgsqlStmtCaseWhenArm() *plpgsqltree.PLpgSQLStmtCaseWhenArm {
+    return u.val.(*plpgsqltree.PLpgSQLStmtCaseWhenArm)
 }
 
-func (u *plpgsqlSymUnion) caseWhens() []*plpgsqltree.CaseWhen {
-    return u.val.([]*plpgsqltree.CaseWhen)
+func (u *plpgsqlSymUnion) plpgsqlStmtCaseWhenArms() []*plpgsqltree.PLpgSQLStmtCaseWhenArm {
+    return u.val.([]*plpgsqltree.PLpgSQLStmtCaseWhenArm)
 }
 
-func (u *plpgsqlSymUnion) statement() plpgsqltree.Statement {
-    return u.val.(plpgsqltree.Statement)
+func (u *plpgsqlSymUnion) plpgsqlStatement() plpgsqltree.PLpgSQLStatement {
+    return u.val.(plpgsqltree.PLpgSQLStatement)
 }
 
-func (u *plpgsqlSymUnion) statements() []plpgsqltree.Statement {
-    return u.val.([]plpgsqltree.Statement)
+func (u *plpgsqlSymUnion) plpgsqlStatements() []plpgsqltree.PLpgSQLStatement {
+    return u.val.([]plpgsqltree.PLpgSQLStatement)
 }
 
 func (u *plpgsqlSymUnion) int32() int32 {
@@ -102,68 +102,68 @@ func (u *plpgsqlSymUnion) typ() tree.ResolvableTypeReference {
     return u.val.(tree.ResolvableTypeReference)
 }
 
-func (u *plpgsqlSymUnion) getDiagnosticsKind() plpgsqltree.GetDiagnosticsKind {
-    return u.val.(plpgsqltree.GetDiagnosticsKind)
+func (u *plpgsqlSymUnion) pLpgSQLGetDiagKind() plpgsqltree.PLpgSQLGetDiagKind {
+    return u.val.(plpgsqltree.PLpgSQLGetDiagKind)
 }
 
-func (u *plpgsqlSymUnion) getDiagnosticsItem() *plpgsqltree.GetDiagnosticsItem {
-    return u.val.(*plpgsqltree.GetDiagnosticsItem)
+func (u *plpgsqlSymUnion) pLpgSQLStmtGetDiagItem() *plpgsqltree.PLpgSQLStmtGetDiagItem {
+    return u.val.(*plpgsqltree.PLpgSQLStmtGetDiagItem)
 }
 
-func (u *plpgsqlSymUnion) getDiagnosticsItemList() plpgsqltree.GetDiagnosticsItemList {
-    return u.val.(plpgsqltree.GetDiagnosticsItemList)
+func (u *plpgsqlSymUnion) pLpgSQLStmtGetDiagItemList() plpgsqltree.PLpgSQLStmtGetDiagItemList {
+    return u.val.(plpgsqltree.PLpgSQLStmtGetDiagItemList)
 }
 
-func (u *plpgsqlSymUnion) elseIf() []plpgsqltree.ElseIf {
-    return u.val.([]plpgsqltree.ElseIf)
+func (u *plpgsqlSymUnion) pLpgSQLStmtIfElseIfArmList() []plpgsqltree.PLpgSQLStmtIfElseIfArm {
+    return u.val.([]plpgsqltree.PLpgSQLStmtIfElseIfArm)
 }
 
-func (u *plpgsqlSymUnion) open() *plpgsqltree.Open {
-    return u.val.(*plpgsqltree.Open)
+func (u *plpgsqlSymUnion) pLpgSQLStmtOpen() *plpgsqltree.PLpgSQLStmtOpen {
+    return u.val.(*plpgsqltree.PLpgSQLStmtOpen)
 }
 
-func (u *plpgsqlSymUnion) expr() plpgsqltree.Expr {
+func (u *plpgsqlSymUnion) plpgsqlExpr() plpgsqltree.PLpgSQLExpr {
     if u.val == nil {
       return nil
     }
-    return u.val.(plpgsqltree.Expr)
+    return u.val.(plpgsqltree.PLpgSQLExpr)
 }
 
-func (u *plpgsqlSymUnion) exprs() []plpgsqltree.Expr {
-    return u.val.([]plpgsqltree.Expr)
+func (u *plpgsqlSymUnion) plpgsqlExprs() []plpgsqltree.PLpgSQLExpr {
+    return u.val.([]plpgsqltree.PLpgSQLExpr)
 }
 
-func (u *plpgsqlSymUnion) declaration() *plpgsqltree.Declaration {
-    return u.val.(*plpgsqltree.Declaration)
+func (u *plpgsqlSymUnion) plpgsqlDecl() *plpgsqltree.PLpgSQLDecl {
+    return u.val.(*plpgsqltree.PLpgSQLDecl)
 }
 
-func (u *plpgsqlSymUnion) declarations() []plpgsqltree.Declaration {
-    return u.val.([]plpgsqltree.Declaration)
+func (u *plpgsqlSymUnion) plpgsqlDecls() []plpgsqltree.PLpgSQLDecl {
+    return u.val.([]plpgsqltree.PLpgSQLDecl)
 }
 
-func (u *plpgsqlSymUnion) raiseOption() *plpgsqltree.RaiseOption {
-    return u.val.(*plpgsqltree.RaiseOption)
+func (u *plpgsqlSymUnion) plpgsqlOptionExpr() *plpgsqltree.PLpgSQLStmtRaiseOption {
+    return u.val.(*plpgsqltree.PLpgSQLStmtRaiseOption)
 }
 
-func (u *plpgsqlSymUnion) raiseOptions() []plpgsqltree.RaiseOption {
-    return u.val.([]plpgsqltree.RaiseOption)
+func (u *plpgsqlSymUnion) plpgsqlOptionExprs() []plpgsqltree.PLpgSQLStmtRaiseOption {
+    return u.val.([]plpgsqltree.PLpgSQLStmtRaiseOption)
 }
 
 
-func (u *plpgsqlSymUnion) exception() *plpgsqltree.Exception {
-    return u.val.(*plpgsqltree.Exception)
+func (u *plpgsqlSymUnion) plpgsqlException() *plpgsqltree.PLpgSQLException {
+    return u.val.(*plpgsqltree.PLpgSQLException)
 }
 
-func (u *plpgsqlSymUnion) exceptions() []plpgsqltree.Exception {
-    return u.val.([]plpgsqltree.Exception)
+func (u *plpgsqlSymUnion) plpgsqlExceptions() []plpgsqltree.PLpgSQLException {
+    return u.val.([]plpgsqltree.PLpgSQLException)
 }
 
-func (u *plpgsqlSymUnion) condition() *plpgsqltree.Condition {
-    return u.val.(*plpgsqltree.Condition)
+func (u *plpgsqlSymUnion) plpgsqlCondition() *plpgsqltree.PLpgSQLCondition {
+    return u.val.(*plpgsqltree.PLpgSQLCondition)
 }
 
-func (u *plpgsqlSymUnion) conditions() []plpgsqltree.Condition {
-    return u.val.([]plpgsqltree.Condition)
+func (u *plpgsqlSymUnion) plpgsqlConditions() []plpgsqltree.PLpgSQLCondition {
+    return u.val.([]plpgsqltree.PLpgSQLCondition)
 }
 
 %}
@@ -311,60 +311,63 @@ func (u *plpgsqlSymUnion) conditions() []plpgsqltree.Condition {
 
 %type <str> decl_varname decl_defkey
 %type <bool>	decl_const decl_notnull
-%type <plpgsqltree.Expr>	decl_defval decl_cursor_query
+%type <plpgsqltree.PLpgSQLExpr>	decl_defval decl_cursor_query
 %type <tree.ResolvableTypeReference>	decl_datatype
 %type <str>		decl_collate
+%type <plpgsqltree.PLpgSQLDatum>	decl_cursor_args
 
-%type <*plpgsqltree.Open> open_stmt_processor
+%type <*plpgsqltree.PLpgSQLStmtOpen> open_stmt_processor
 %type <str>	expr_until_semi expr_until_paren
 %type <str>	expr_until_then expr_until_loop opt_expr_until_when
-%type <plpgsqltree.Expr>	opt_exitcond
+%type <plpgsqltree.PLpgSQLExpr>	opt_exitcond
 
+%type <plpgsqltree.PLpgSQLScalarVar>		cursor_variable
+%type <plpgsqltree.PLpgSQLDatum>	decl_cursor_arg
 %type <forvariable>	for_variable
-%type <plpgsqltree.Expr>	return_variable
+%type <plpgsqltree.PLpgSQLExpr>	return_variable
 %type <*tree.NumVal>	foreach_slice
-%type <plpgsqltree.Statement>	for_control
+%type <plpgsqltree.PLpgSQLStatement>	for_control
 
 %type <str> any_identifier opt_block_label opt_loop_label opt_label query_options
 %type <str> opt_error_level option_type
 
-%type <[]plpgsqltree.Statement> proc_sect
-%type <[]plpgsqltree.ElseIf> stmt_elsifs
-%type <[]plpgsqltree.Statement> stmt_else loop_body // TODO is this a list of statement?
-%type <plpgsqltree.Statement>  pl_block
-%type <plpgsqltree.Statement>	proc_stmt
-%type <plpgsqltree.Statement>	stmt_assign stmt_if stmt_loop stmt_while stmt_exit stmt_continue
-%type <plpgsqltree.Statement>	stmt_return stmt_raise stmt_assert stmt_execsql
-%type <plpgsqltree.Statement>	stmt_dynexecute stmt_for stmt_perform stmt_call stmt_getdiag
-%type <plpgsqltree.Statement>	stmt_open stmt_fetch stmt_move stmt_close stmt_null
-%type <plpgsqltree.Statement>	stmt_commit stmt_rollback
-%type <plpgsqltree.Statement>	stmt_case stmt_foreach_a
-
-%type <*plpgsqltree.Declaration> decl_stmt decl_statement
-%type <[]plpgsqltree.Declaration> decl_sect opt_decl_stmts decl_stmts
-
-%type <[]plpgsqltree.Exception> exception_sect proc_exceptions
-%type <*plpgsqltree.Exception>	proc_exception
-%type <[]plpgsqltree.Condition> proc_conditions
-%type <*plpgsqltree.Condition> proc_condition
-
-%type <*plpgsqltree.CaseWhen>	case_when
-%type <[]*plpgsqltree.CaseWhen>	case_when_list
-%type <[]plpgsqltree.Statement> opt_case_else
+%type <[]plpgsqltree.PLpgSQLStatement> proc_sect
+%type <[]plpgsqltree.PLpgSQLStmtIfElseIfArm> stmt_elsifs
+%type <[]plpgsqltree.PLpgSQLStatement> stmt_else loop_body // TODO is this a list of statement?
+%type <plpgsqltree.PLpgSQLStatement>  pl_block
+%type <plpgsqltree.PLpgSQLStatement>	proc_stmt
+%type <plpgsqltree.PLpgSQLStatement>	stmt_assign stmt_if stmt_loop stmt_while stmt_exit stmt_continue
+%type <plpgsqltree.PLpgSQLStatement>	stmt_return stmt_raise stmt_assert stmt_execsql
+%type <plpgsqltree.PLpgSQLStatement>	stmt_dynexecute stmt_for stmt_perform stmt_call stmt_getdiag
+%type <plpgsqltree.PLpgSQLStatement>	stmt_open stmt_fetch stmt_move stmt_close stmt_null
+%type <plpgsqltree.PLpgSQLStatement>	stmt_commit stmt_rollback
+%type <plpgsqltree.PLpgSQLStatement>	stmt_case stmt_foreach_a
+
+%type <*plpgsqltree.PLpgSQLDecl> decl_stmt decl_statement
+%type <[]plpgsqltree.PLpgSQLDecl> decl_sect opt_decl_stmts decl_stmts
+
+%type <[]plpgsqltree.PLpgSQLException> exception_sect proc_exceptions
+%type <*plpgsqltree.PLpgSQLException>	proc_exception
+%type <[]plpgsqltree.PLpgSQLCondition> proc_conditions
+%type <*plpgsqltree.PLpgSQLCondition> proc_condition
+
+%type <*plpgsqltree.PLpgSQLStmtCaseWhenArm>	case_when
+%type <[]*plpgsqltree.PLpgSQLStmtCaseWhenArm>	case_when_list
+%type <[]plpgsqltree.PLpgSQLStatement> opt_case_else
 
 %type <bool>	getdiag_area_opt
-%type <plpgsqltree.GetDiagnosticsItemList>	getdiag_list // TODO don't know what this is
-%type <*plpgsqltree.GetDiagnosticsItem> getdiag_list_item // TODO don't know what this is
+%type <plpgsqltree.PLpgSQLStmtGetDiagItemList>	getdiag_list // TODO don't know what this is
+%type <*plpgsqltree.PLpgSQLStmtGetDiagItem> getdiag_list_item // TODO don't know what this is
 %type <int32> getdiag_item
 
-%type <*plpgsqltree.RaiseOption> option_expr
-%type <[]plpgsqltree.RaiseOption> option_exprs opt_option_exprs
-%type <plpgsqltree.Expr> format_expr
-%type <[]plpgsqltree.Expr> opt_format_exprs format_exprs
+%type <*plpgsqltree.PLpgSQLStmtRaiseOption> option_expr
+%type <[]plpgsqltree.PLpgSQLStmtRaiseOption> option_exprs opt_option_exprs
+%type <plpgsqltree.PLpgSQLExpr> format_expr
+%type <[]plpgsqltree.PLpgSQLExpr> opt_format_exprs format_exprs
 
 %type <uint32>	opt_scrollable
 
-%type <*plpgsqltree.Fetch>	opt_fetch_direction
+%type <*plpgsqltree.PLpgSQLStmtFetch>	opt_fetch_direction
 
 %type <*tree.NumVal>	opt_transaction_chain
 
@@ -374,7 +377,7 @@ func (u *plpgsqlSymUnion) conditions() []plpgsqltree.Condition {
 pl_function:
   pl_block opt_semi
   {
-    plpgsqllex.(*lexer).SetStmt($1.statement())
+    plpgsqllex.(*lexer).SetStmt($1.plpgsqlStatement())
   }
 
 opt_semi:
@@ -383,40 +386,40 @@ opt_semi:
 
 pl_block: opt_block_label decl_sect BEGIN proc_sect exception_sect END opt_label
   {
-    $$.val = &plpgsqltree.Block{
+    $$.val = &plpgsqltree.PLpgSQLStmtBlock{
       Label: $1,
-      Decls: $2.declarations(),
-      Body: $4.statements(),
-      Exceptions: $5.exceptions(),
+      Decls: $2.plpgsqlDecls(),
+      Body: $4.plpgsqlStatements(),
+      Exceptions: $5.plpgsqlExceptions(),
     }
   }
 ;
 
 decl_sect: DECLARE opt_decl_stmts
   {
-    $$.val = $2.declarations()
+    $$.val = $2.plpgsqlDecls()
   }
 | /* EMPTY */
   {
     // Use a nil slice to indicate DECLARE was not used.
-    $$.val = []plpgsqltree.Declaration(nil)
+    $$.val = []plpgsqltree.PLpgSQLDecl(nil)
   }
 ;
 
 opt_decl_stmts: decl_stmts
   {
-    $$.val = $1.declarations()
+    $$.val = $1.plpgsqlDecls()
   }
 | /* EMPTY */
   {
-    $$.val = []plpgsqltree.Declaration{}
+    $$.val = []plpgsqltree.PLpgSQLDecl{}
   }
 ;
 
 decl_stmts: decl_stmts decl_stmt
   {
-    decs := $1.declarations()
-    dec := $2.declaration()
+    decs := $1.plpgsqlDecls()
+    dec := $2.plpgsqlDecl()
     if dec == nil {
       $$.val = decs
     } else {
@@ -425,23 +428,23 @@ decl_stmts: decl_stmts decl_stmt
   }
 | decl_stmt
   {
-    dec := $1.declaration()
+    dec := $1.plpgsqlDecl()
     if dec == nil {
-      $$.val = []plpgsqltree.Declaration{}
+      $$.val = []plpgsqltree.PLpgSQLDecl{}
     } else {
-      $$.val = []plpgsqltree.Declaration{*dec}
+      $$.val = []plpgsqltree.PLpgSQLDecl{*dec}
     }
 	}
 ;
 
 decl_stmt	: decl_statement
   {
-    $$.val = $1.declaration()
+    $$.val = $1.plpgsqlDecl()
   }
 | DECLARE
   {
     // This is to allow useless extra "DECLARE" keywords in the declare section.
-    $$.val = (*plpgsqltree.Declaration)(nil)
+    $$.val = (*plpgsqltree.PLpgSQLDecl)(nil)
   }
 // TODO(chengxiong): turn this block on and throw useful error if user
 // tries to put the block label just before BEGIN instead of before
@@ -453,13 +456,13 @@ decl_stmt	: decl_statement
 
 decl_statement: decl_varname decl_const decl_datatype decl_collate decl_notnull decl_defval
   {
-    $$.val = &plpgsqltree.Declaration{
-      Var: plpgsqltree.Variable($1),
+    $$.val = &plpgsqltree.PLpgSQLDecl{
+      Var: plpgsqltree.PLpgSQLVariable($1),
       Constant: $2.bool(),
       Typ: $3.typ(),
       Collate: $4,
       NotNull: $5.bool(),
-      Expr: $6.expr(),
+      Expr: $6.plpgsqlExpr(),
     }
   }
 | decl_varname ALIAS FOR decl_aliasitem ';'
@@ -581,7 +584,7 @@ decl_notnull:
 
 decl_defval: ';'
   {
-    $$.val = (plpgsqltree.Expr)(nil)
+    $$.val = (plpgsqltree.PLpgSQLExpr)(nil)
   }
 | decl_defkey ';'
   {
@@ -614,35 +617,35 @@ assign_operator: '='
 
 proc_sect:
   {
-    $$.val = []plpgsqltree.Statement{}
+    $$.val = []plpgsqltree.PLpgSQLStatement{}
   }
 | proc_sect proc_stmt
   {
-    stmts := $1.statements()
-    stmts = append(stmts, $2.statement())
+    stmts := $1.plpgsqlStatements()
+    stmts = append(stmts, $2.plpgsqlStatement())
     $$.val = stmts
   }
 ;
 
 proc_stmt:pl_block ';'
   {
-    $$.val = $1.block()
+    $$.val = $1.plpgsqlStmtBlock()
   }
 | stmt_assign
   {
-    $$.val = $1.statement()
+    $$.val = $1.plpgsqlStatement()
   }
 | stmt_if
   {
-    $$.val = $1.statement()
+    $$.val = $1.plpgsqlStatement()
   }
 | stmt_case
   {
-    $$.val = $1.statement()
+    $$.val = $1.plpgsqlStatement()
   }
 | stmt_loop
   {
-    $$.val = $1.statement()
+    $$.val = $1.plpgsqlStatement()
   }
 | stmt_while
   { }
@@ -652,37 +655,37 @@ proc_stmt:pl_block ';'
   { }
 | stmt_exit
   {
-    $$.val = $1.statement()
+    $$.val = $1.plpgsqlStatement()
   }
 | stmt_continue
   {
-    $$.val = $1.statement()
+    $$.val = $1.plpgsqlStatement()
   }
 | stmt_return
   {
-    $$.val = $1.statement()
+    $$.val = $1.plpgsqlStatement()
   }
 | stmt_raise
   {
-    $$.val = $1.statement()
+    $$.val = $1.plpgsqlStatement()
   }
 | stmt_assert
   {
-    $$.val = $1.statement()
+    $$.val = $1.plpgsqlStatement()
   }
 | stmt_execsql
   {
-    $$.val = $1.statement()
+    $$.val = $1.plpgsqlStatement()
   }
 | stmt_dynexecute
   {
-    $$.val = $1.statement()
+    $$.val = $1.plpgsqlStatement()
   }
 | stmt_perform
   { }
 | stmt_call
   {
-    $$.val = $1.statement()
+    $$.val = $1.plpgsqlStatement()
   }
 | stmt_getdiag
   { }
@@ -694,7 +697,7 @@ proc_stmt:pl_block ';'
   { }
 | stmt_close
   {
-    $$.val = $1.statement()
+    $$.val = $1.plpgsqlStatement()
   }
 | stmt_null
   { }
@@ -712,11 +715,11 @@ stmt_perform: PERFORM expr_until_semi ';'
 
 stmt_call: CALL call_cmd ';'
   {
-    $$.val = &plpgsqltree.Call{IsCall: true}
+    $$.val = &plpgsqltree.PLpgSQLStmtCall{IsCall: true}
   }
 | DO call_cmd ';'
   {
-    $$.val = &plpgsqltree.Call{IsCall: false}
+    $$.val = &plpgsqltree.PLpgSQLStmtCall{IsCall: false}
   }
 ;
 
@@ -732,8 +735,8 @@ stmt_assign: IDENT assign_operator expr_until_semi ';'
     if err != nil {
       return setErr(plpgsqllex, err)
     }
-    $$.val = &plpgsqltree.Assignment{
-      Var: plpgsqltree.Variable($1),
+    $$.val = &plpgsqltree.PLpgSQLStmtAssign{
+      Var: plpgsqltree.PLpgSQLVariable($1),
       Value: expr,
     }
   }
@@ -741,9 +744,9 @@ stmt_assign: IDENT assign_operator expr_until_semi ';'
 
 stmt_getdiag: GET getdiag_area_opt DIAGNOSTICS getdiag_list ';'
   {
-  $$.val = &plpgsqltree.GetDiagnostics{
+  $$.val = &plpgsqltree.PLpgSQLStmtGetDiag{
     IsStacked: $2.bool(),
-    DiagItems: $4.getDiagnosticsItemList(),
+    DiagItems: $4.pLpgSQLStmtGetDiagItemList(),
   }
   // TODO(jane): Check information items are valid for area option.
   }
@@ -765,18 +768,18 @@ getdiag_area_opt:
 
 getdiag_list: getdiag_list ',' getdiag_list_item
   {
-    $$.val = append($1.getDiagnosticsItemList(), $3.getDiagnosticsItem())
+    $$.val = append($1.pLpgSQLStmtGetDiagItemList(), $3.pLpgSQLStmtGetDiagItem())
   }
 | getdiag_list_item
   {
-    $$.val = plpgsqltree.GetDiagnosticsItemList{$1.getDiagnosticsItem()}
+    $$.val = plpgsqltree.PLpgSQLStmtGetDiagItemList{$1.pLpgSQLStmtGetDiagItem()}
   }
 ;
 
 getdiag_list_item: IDENT assign_operator getdiag_item
   {
-    $$.val = &plpgsqltree.GetDiagnosticsItem{
-      Kind : $3.getDiagnosticsKind(),
+    $$.val = &plpgsqltree.PLpgSQLStmtGetDiagItem{
+      Kind : $3.pLpgSQLGetDiagKind(),
       TargetName: $1,
       // TODO(jane): set the target from $1.
     }
@@ -786,29 +789,29 @@ getdiag_list_item: IDENT assign_operator getdiag_item
 getdiag_item: unreserved_keyword {
   switch $1 {
     case "row_count":
-      $$.val = plpgsqltree.GetDiagnosticsRowCount;
+      $$.val = plpgsqltree.PlpgsqlGetdiagRowCount;
     case "pg_context":
-      $$.val = plpgsqltree.GetDiagnosticsContext;
+      $$.val = plpgsqltree.PlpgsqlGetdiagContext;
     case "pg_exception_detail":
-      $$.val = plpgsqltree.GetDiagnosticsErrorDetail;
+      $$.val = plpgsqltree.PlpgsqlGetdiagErrorDetail;
     case "pg_exception_hint":
-      $$.val = plpgsqltree.GetDiagnosticsErrorHint;
+      $$.val = plpgsqltree.PlpgsqlGetdiagErrorHint;
     case "pg_exception_context":
-      $$.val = plpgsqltree.GetDiagnosticsErrorContext;
+      $$.val = plpgsqltree.PlpgsqlGetdiagErrorContext;
     case "column_name":
-      $$.val = plpgsqltree.GetDiagnosticsColumnName;
+      $$.val = plpgsqltree.PlpgsqlGetdiagColumnName;
     case "constraint_name":
-      $$.val = plpgsqltree.GetDiagnosticsConstraintName;
+      $$.val = plpgsqltree.PlpgsqlGetdiagConstraintName;
     case "pg_datatype_name":
-      $$.val = plpgsqltree.GetDiagnosticsDatatypeName;
+      $$.val = plpgsqltree.PlpgsqlGetdiagDatatypeName;
     case "message_text":
-      $$.val = plpgsqltree.GetDiagnosticsMessageText;
+      $$.val = plpgsqltree.PlpgsqlGetdiagMessageText;
     case "table_name":
-      $$.val = plpgsqltree.GetDiagnosticsTableName;
+      $$.val = plpgsqltree.PlpgsqlGetdiagTableName;
     case "schema_name":
-      $$.val = plpgsqltree.GetDiagnosticsSchemaName;
+      $$.val = plpgsqltree.PlpgsqlGetdiagSchemaName;
     case "returned_sqlstate":
-      $$.val = plpgsqltree.GetDiagnosticsReturnedSQLState;
+      $$.val = plpgsqltree.PlpgsqlGetdiagReturnedSqlstate;
     default:
       // TODO(jane): Should this use an unimplemented error instead?
       setErr(plpgsqllex, errors.Newf("unrecognized GET DIAGNOSTICS item: %s", redact.Safe($1)))
@@ -829,18 +832,18 @@ stmt_if: IF expr_until_then THEN proc_sect stmt_elsifs stmt_else END_IF IF ';'
     if err != nil {
       return setErr(plpgsqllex, err)
     }
-    $$.val = &plpgsqltree.If{
+    $$.val = &plpgsqltree.PLpgSQLStmtIf{
       Condition: cond,
-      ThenBody: $4.statements(),
-      ElseIfList: $5.elseIf(),
-      ElseBody: $6.statements(),
+      ThenBody: $4.plpgsqlStatements(),
+      ElseIfList: $5.pLpgSQLStmtIfElseIfArmList(),
+      ElseBody: $6.plpgsqlStatements(),
     }
   }
 ;
 
 stmt_elsifs:
   {
-    $$.val = []plpgsqltree.ElseIf{};
+    $$.val = []plpgsqltree.PLpgSQLStmtIfElseIfArm{};
   }
 | stmt_elsifs ELSIF expr_until_then THEN proc_sect
   {
@@ -848,33 +851,33 @@ stmt_elsifs:
     if err != nil {
       return setErr(plpgsqllex, err)
     }
-    newStmt := plpgsqltree.ElseIf{
+    newStmt := plpgsqltree.PLpgSQLStmtIfElseIfArm{
       Condition: cond,
-      Stmts: $5.statements(),
+      Stmts: $5.plpgsqlStatements(),
     }
-    $$.val = append($1.elseIf() , newStmt)
+    $$.val = append($1.pLpgSQLStmtIfElseIfArmList() , newStmt)
   }
 ;
 
 stmt_else:
   {
-    $$.val = []plpgsqltree.Statement{};
+    $$.val = []plpgsqltree.PLpgSQLStatement{};
   }
 | ELSE proc_sect
   {
-    $$.val = $2.statements();
+    $$.val = $2.plpgsqlStatements();
   }
 ;
 
 stmt_case: CASE opt_expr_until_when case_when_list opt_case_else END_CASE CASE ';'
   {
-    expr := &plpgsqltree.Case {
+    expr := &plpgsqltree.PLpgSQLStmtCase {
       TestExpr: $2,
-      CaseWhenList: $3.caseWhens(),
+      CaseWhenList: $3.plpgsqlStmtCaseWhenArms(),
     }
     if $4.val != nil {
        expr.HaveElse = true
-       expr.ElseStmts = $4.statements()
+       expr.ElseStmts = $4.plpgsqlStatements()
     }
     $$.val = expr
   }
@@ -893,23 +896,23 @@ opt_expr_until_when:
 
 case_when_list: case_when_list case_when
   {
-    stmts := $1.caseWhens()
-    stmts = append(stmts, $2.caseWhen())
+    stmts := $1.plpgsqlStmtCaseWhenArms()
+    stmts = append(stmts, $2.plpgsqlStmtCaseWhenArm())
     $$.val = stmts
   }
 | case_when
   {
-    stmts := []*plpgsqltree.CaseWhen{}
-    stmts = append(stmts, $1.caseWhen())
+    stmts := []*plpgsqltree.PLpgSQLStmtCaseWhenArm{}
+    stmts = append(stmts, $1.plpgsqlStmtCaseWhenArm())
     $$.val = stmts
   }
 ;
 
 case_when: WHEN expr_until_then THEN proc_sect
   {
-     expr := &plpgsqltree.CaseWhen{
+     expr := &plpgsqltree.PLpgSQLStmtCaseWhenArm{
        Expr: $2,
-       Stmts: $4.statements(),
+       Stmts: $4.plpgsqlStatements(),
      }
      $$.val = expr
   }
@@ -921,7 +924,7 @@ opt_case_else:
   }
 | ELSE proc_sect
   {
-    $$.val = $2.statements()
+    $$.val = $2.plpgsqlStatements()
   }
 ;
 
@@ -929,26 +932,16 @@ stmt_loop: opt_loop_label LOOP loop_body opt_label ';'
   {
     // TODO(drewk): does the second usage of the label actually
     // do anything?
-    $$.val = &plpgsqltree.Loop{
+    $$.val = &plpgsqltree.PLpgSQLStmtSimpleLoop{
       Label: $1,
-      Body: $3.statements(),
+      Body: $3.plpgsqlStatements(),
     }
   }
 ;
 
-stmt_while: opt_loop_label WHILE expr_until_loop LOOP loop_body opt_label ';'
+stmt_while: opt_loop_label WHILE expr_until_loop loop_body
   {
-    // TODO(drewk): does the second usage of the label actually
-    // do anything?
-    cond, err := plpgsqllex.(*lexer).ParseExpr($3)
-    if err != nil {
-      return setErr(plpgsqllex, err)
-    }
-    $$.val = &plpgsqltree.While{
-      Label: $1,
-      Condition: cond,
-      Body: $5.statements(),
-    }
+    return unimplemented(plpgsqllex, "while loop")
   }
 ;
 
@@ -1005,18 +998,18 @@ foreach_slice:
 
 stmt_exit: EXIT opt_label opt_exitcond
   {
-    $$.val = &plpgsqltree.Exit{
+    $$.val = &plpgsqltree.PLpgSQLStmtExit{
       Label: $2,
-      Condition: $3.expr(),
+      Condition: $3.plpgsqlExpr(),
     }
   }
 ;
 
 stmt_continue: CONTINUE opt_label opt_exitcond
   {
-    $$.val = &plpgsqltree.Continue{
+    $$.val = &plpgsqltree.PLpgSQLStmtContinue{
       Label: $2,
-      Condition: $3.expr(),
+      Condition: $3.plpgsqlExpr(),
     }
   }
 ;
@@ -1029,8 +1022,8 @@ stmt_continue: CONTINUE opt_label opt_exitcond
 
 stmt_return: RETURN return_variable ';'
   {
-    $$.val = &plpgsqltree.Return{
-      Expr: $2.expr(),
+    $$.val = &plpgsqltree.PLpgSQLStmtReturn{
+      Expr: $2.plpgsqlExpr(),
     }
   }
 | RETURN_NEXT NEXT return_variable ';'
@@ -1072,34 +1065,34 @@ stmt_raise:
   }
 | RAISE opt_error_level SCONST opt_format_exprs opt_option_exprs ';'
   {
-    $$.val = &plpgsqltree.Raise{
+    $$.val = &plpgsqltree.PLpgSQLStmtRaise{
       LogLevel: $2,
       Message: $3,
-      Params: $4.exprs(),
-      Options: $5.raiseOptions(),
+      Params: $4.plpgsqlExprs(),
+      Options: $5.plpgsqlOptionExprs(),
     }
   }
 | RAISE opt_error_level IDENT opt_option_exprs ';'
   {
-    $$.val = &plpgsqltree.Raise{
+    $$.val = &plpgsqltree.PLpgSQLStmtRaise{
       LogLevel: $2,
       CodeName: $3,
-      Options: $4.raiseOptions(),
+      Options: $4.plpgsqlOptionExprs(),
     }
   }
 | RAISE opt_error_level SQLSTATE SCONST opt_option_exprs ';'
   {
-    $$.val = &plpgsqltree.Raise{
+    $$.val = &plpgsqltree.PLpgSQLStmtRaise{
       LogLevel: $2,
       Code: $4,
-      Options: $5.raiseOptions(),
+      Options: $5.plpgsqlOptionExprs(),
     }
   }
 | RAISE opt_error_level USING option_exprs ';'
   {
-    $$.val = &plpgsqltree.Raise{
+    $$.val = &plpgsqltree.PLpgSQLStmtRaise{
       LogLevel: $2,
-      Options: $4.raiseOptions(),
+      Options: $4.plpgsqlOptionExprs(),
     }
   }
 ;
@@ -1120,24 +1113,24 @@ opt_error_level:
 opt_option_exprs:
   USING option_exprs
   {
-    $$.val = $2.raiseOptions()
+    $$.val = $2.plpgsqlOptionExprs()
   }
 | /* EMPTY */
   {
-    $$.val = []plpgsqltree.RaiseOption{}
+    $$.val = []plpgsqltree.PLpgSQLStmtRaiseOption{}
   }
 ;
 
 option_exprs:
   option_exprs ',' option_expr
   {
-    option := $3.raiseOption()
-    $$.val = append($1.raiseOptions(), *option)
+    option := $3.plpgsqlOptionExpr()
+    $$.val = append($1.plpgsqlOptionExprs(), *option)
   }
 | option_expr
   {
-    option := $1.raiseOption()
-    $$.val = []plpgsqltree.RaiseOption{*option}
+    option := $1.plpgsqlOptionExpr()
+    $$.val = []plpgsqltree.PLpgSQLStmtRaiseOption{*option}
   }
 ;
 
@@ -1150,7 +1143,7 @@ option_expr:
     if err != nil {
       return setErr(plpgsqllex, err)
     }
-    $$.val = &plpgsqltree.RaiseOption{
+    $$.val = &plpgsqltree.PLpgSQLStmtRaiseOption{
       OptType: $1,
       Expr: optionExpr,
     }
@@ -1172,22 +1165,22 @@ option_type:
 opt_format_exprs:
   format_exprs
   {
-    $$.val = $1.exprs()
+    $$.val = $1.plpgsqlExprs()
   }
  | /* EMPTY */
   {
-    $$.val = []plpgsqltree.Expr{}
+    $$.val = []plpgsqltree.PLpgSQLExpr{}
   }
 ;
 
 format_exprs:
   format_expr
   {
-    $$.val = []plpgsqltree.Expr{$1.expr()}
+    $$.val = []plpgsqltree.PLpgSQLExpr{$1.plpgsqlExpr()}
   }
 | format_exprs format_expr
   {
-    $$.val = append($1.exprs(), $2.expr())
+    $$.val = append($1.plpgsqlExprs(), $2.plpgsqlExpr())
   }
 ;
 
@@ -1205,7 +1198,7 @@ format_expr: ','
 
 stmt_assert: ASSERT assert_cond ';'
   {
-    $$.val = &plpgsqltree.Assert{}
+    $$.val = &plpgsqltree.PLpgSQLStmtAssert{}
   }
 ;
 
@@ -1220,7 +1213,7 @@ assert_cond:
 
 loop_body: proc_sect END LOOP
   {
-    $$.val = $1.statements()
+    $$.val = $1.plpgsqlStatements()
   }
 ;
 
@@ -1250,7 +1243,7 @@ stmt_dynexecute: EXECUTE
 // TODO: change expr_until_semi to process_cursor_before_semi
 stmt_open: OPEN IDENT open_stmt_processor ';'
   {
-    openCursorStmt := $3.open()
+    openCursorStmt := $3.pLpgSQLStmtOpen()
     openCursorStmt.CursorName = $2
     $$.val = openCursorStmt
   }
@@ -1275,13 +1268,13 @@ opt_fetch_direction:
 
 stmt_close: CLOSE cursor_variable ';'
   {
-    $$.val = &plpgsqltree.Close{}
+    $$.val = &plpgsqltree.PLpgSQLStmtClose{}
   }
 ;
 
 stmt_null: NULL ';'
   {
-  $$.val = &plpgsqltree.Null{};
+  $$.val = &plpgsqltree.PLpgSQLStmtNull{};
   }
 ;
 
@@ -1313,54 +1306,54 @@ cursor_variable: IDENT
 
 exception_sect: /* EMPTY */
   {
-    $$.val = []plpgsqltree.Exception(nil)
+    $$.val = []plpgsqltree.PLpgSQLException(nil)
   }
 | EXCEPTION proc_exceptions
   {
-    $$.val = $2.exceptions()
+    $$.val = $2.plpgsqlExceptions()
   }
 ;
 
 proc_exceptions: proc_exceptions proc_exception
   {
-    e := $2.exception()
-    $$.val = append($1.exceptions(), *e)
+    e := $2.plpgsqlException()
+    $$.val = append($1.plpgsqlExceptions(), *e)
   }
 | proc_exception
   {
-    e := $1.exception()
-    $$.val = []plpgsqltree.Exception{*e}
+    e := $1.plpgsqlException()
+    $$.val = []plpgsqltree.PLpgSQLException{*e}
   }
 ;
 
 proc_exception: WHEN proc_conditions THEN proc_sect
   {
-    $$.val = &plpgsqltree.Exception{
-      Conditions: $2.conditions(),
-      Action: $4.statements(),
+    $$.val = &plpgsqltree.PLpgSQLException{
+      Conditions: $2.plpgsqlConditions(),
+      Action: $4.plpgsqlStatements(),
     }
   }
 ;
 
 proc_conditions: proc_conditions OR proc_condition
   {
-    c := $3.condition()
-    $$.val = append($1.conditions(), *c)
+    c := $3.plpgsqlCondition()
+    $$.val = append($1.plpgsqlConditions(), *c)
   }
 | proc_condition
   {
-    c := $1.condition()
-    $$.val = []plpgsqltree.Condition{*c}
+    c := $1.plpgsqlCondition()
+    $$.val = []plpgsqltree.PLpgSQLCondition{*c}
   }
 ;
 
 proc_condition: any_identifier
   {
-    $$.val = &plpgsqltree.Condition{SqlErrName: $1}
+    $$.val = &plpgsqltree.PLpgSQLCondition{SqlErrName: $1}
   }
 | SQLSTATE SCONST
   {
-    $$.val = &plpgsqltree.Condition{SqlErrState: $2}
+    $$.val = &plpgsqltree.PLpgSQLCondition{SqlErrState: $2}
   }
 ;
 
@@ -1383,7 +1376,7 @@ expr_until_then:
 
 expr_until_loop:
   {
-    $$ = plpgsqllex.(*lexer).ReadSqlExpressionStr(LOOP)
+    return unimplemented(plpgsqllex, "loop expr")
   }
 ;
 
diff --git a/pkg/sql/plpgsql/parser/testdata/stmt_while b/pkg/sql/plpgsql/parser/testdata/stmt_while
index 691241e3226..0214a201ee8 100644
--- a/pkg/sql/plpgsql/parser/testdata/stmt_while
+++ b/pkg/sql/plpgsql/parser/testdata/stmt_while
@@ -7,32 +7,9 @@ WHILE x > 0 LOOP
 END LOOP;
 END
 ----
-DECLARE
-BEGIN
-x := 10;
-WHILE x > 0 LOOP
-x := x - 1;
-END LOOP;
-END
+at or near "while": syntax error: unimplemented: this syntax
+
 
-parse
-DECLARE
-BEGIN
-x := 10;
-WHILE x > 0 AND x < 100 LOOP
-  x := x - 1;
-  x := x - 2;
-END LOOP;
-END
-----
-DECLARE
-BEGIN
-x := 10;
-WHILE (x > 0) AND (x < 100) LOOP
-x := x - 1;
-x := x - 2;
-END LOOP;
-END
 
 parse
 DECLARE
@@ -44,10 +21,4 @@ WHILE x > 0 LOOP
 END LOOP labeled;
 END
 ----
-DECLARE
-BEGIN
-x := 10;
-WHILE x > 0 LOOP
-x := x - 1;
-END LOOP labeled;
-END
+at or near "while": syntax error: unimplemented: this syntax
diff --git a/pkg/sql/sem/plpgsqltree/constants.go b/pkg/sql/sem/plpgsqltree/constants.go
index 8c8c389406c..7b7dd2e9a9b 100644
--- a/pkg/sql/sem/plpgsqltree/constants.go
+++ b/pkg/sql/sem/plpgsqltree/constants.go
@@ -12,122 +12,124 @@ package plpgsqltree
 
 import "github.com/cockroachdb/errors"
 
-// GetDiagnosticsKind represents the type of error diagnostic
+// PLpgSQLGetDiagKind represents the type of error diagnostic
 // item in stmt_getdiag.
-type GetDiagnosticsKind int
+type PLpgSQLGetDiagKind int
 
+// PlpgsqlGetdiagRowCount is an option for diagnostic items that can be
+// present in stmt_getdiag.
 const (
-	// GetDiagnosticsRowCount returns the number of rows processed by the recent
+	// PlpgsqlGetdiagRowCount returns the number of rows processed by the recent
 	// SQL command.
-	GetDiagnosticsRowCount GetDiagnosticsKind = iota
-	// GetDiagnosticsContext returns text describing the current call stack.
-	GetDiagnosticsContext
-	// GetDiagnosticsErrorContext returns text describing the exception's
-	// callstack.
-	GetDiagnosticsErrorContext
-	// GetDiagnosticsErrorDetail returns the exceptions detail message.
-	GetDiagnosticsErrorDetail
-	// GetDiagnosticsErrorHint returns the exceptions hint message.
-	GetDiagnosticsErrorHint
-	// GetDiagnosticsReturnedSQLState returns the SQLSTATE error code related to
+	PlpgsqlGetdiagRowCount PLpgSQLGetDiagKind = iota
+	// PlpgsqlGetdiagContext returns text describing the current call stack.
+	PlpgsqlGetdiagContext
+	// PlpgsqlGetdiagErrorContext returns text describing the exception's callstack.
+	PlpgsqlGetdiagErrorContext
+	// PlpgsqlGetdiagErrorDetail returns the exceptions detail message.
+	PlpgsqlGetdiagErrorDetail
+	// PlpgsqlGetdiagErrorHint returns the exceptions hint message.
+	PlpgsqlGetdiagErrorHint
+	// PlpgsqlGetdiagReturnedSqlstate returns the SQLSTATE error code related to
 	// the exception.
-	GetDiagnosticsReturnedSQLState
-	// GetDiagnosticsColumnName returns the column name related to the exception.
-	GetDiagnosticsColumnName
-	// GetDiagnosticsConstraintName returns the constraint name related to
+	PlpgsqlGetdiagReturnedSqlstate
+	// PlpgsqlGetdiagColumnName returns the column name related to the exception.
+	PlpgsqlGetdiagColumnName
+	// PlpgsqlGetdiagConstraintName returns the constraint name related to
 	// the exception.
-	GetDiagnosticsConstraintName
-	// GetDiagnosticsDatatypeName returns the data type name related to the
-	// exception.
-	GetDiagnosticsDatatypeName
-	// GetDiagnosticsMessageText returns the exceptions primary message.
-	GetDiagnosticsMessageText
-	// GetDiagnosticsTableName returns the name of the table related to the
-	// exception.
-	GetDiagnosticsTableName
-	// GetDiagnosticsSchemaName returns the name of the schema related to the
-	// exception.
-	GetDiagnosticsSchemaName
+	PlpgsqlGetdiagConstraintName
+	// PlpgsqlGetdiagDatatypeName returns the data type name related to
+	// the exception.
+	PlpgsqlGetdiagDatatypeName
+	// PlpgsqlGetdiagMessageText returns the exceptions primary message.
+	PlpgsqlGetdiagMessageText
+	// PlpgsqlGetdiagTableName returns the name of the table related to
+	// the exception.
+	PlpgsqlGetdiagTableName
+	// PlpgsqlGetdiagSchemaName returns the name of the schema related to
+	//	// the exception.
+	PlpgsqlGetdiagSchemaName
 )
 
 // String implements the fmt.Stringer interface.
-func (k GetDiagnosticsKind) String() string {
+func (k PLpgSQLGetDiagKind) String() string {
 	switch k {
-	case GetDiagnosticsRowCount:
+	case PlpgsqlGetdiagRowCount:
 		return "ROW_COUNT"
-	case GetDiagnosticsContext:
+	case PlpgsqlGetdiagContext:
 		return "PG_CONTEXT"
-	case GetDiagnosticsErrorContext:
+	case PlpgsqlGetdiagErrorContext:
 		return "PG_EXCEPTION_CONTEXT"
-	case GetDiagnosticsErrorDetail:
+	case PlpgsqlGetdiagErrorDetail:
 		return "PG_EXCEPTION_DETAIL"
-	case GetDiagnosticsErrorHint:
+	case PlpgsqlGetdiagErrorHint:
 		return "PG_EXCEPTION_HINT"
-	case GetDiagnosticsReturnedSQLState:
+	case PlpgsqlGetdiagReturnedSqlstate:
 		return "RETURNED_SQLSTATE"
-	case GetDiagnosticsColumnName:
+	case PlpgsqlGetdiagColumnName:
 		return "COLUMN_NAME"
-	case GetDiagnosticsConstraintName:
+	case PlpgsqlGetdiagConstraintName:
 		return "CONSTRAINT_NAME"
-	case GetDiagnosticsDatatypeName:
+	case PlpgsqlGetdiagDatatypeName:
 		return "PG_DATATYPE_NAME"
-	case GetDiagnosticsMessageText:
+	case PlpgsqlGetdiagMessageText:
 		return "MESSAGE_TEXT"
-	case GetDiagnosticsTableName:
+	case PlpgsqlGetdiagTableName:
 		return "TABLE_NAME"
-	case GetDiagnosticsSchemaName:
+	case PlpgsqlGetdiagSchemaName:
 		return "SCHEMA_NAME"
 	}
-	panic(errors.AssertionFailedf("unknown diagnostics kind"))
+	panic(errors.AssertionFailedf("no Annotations unknown getDiagnistics kind"))
 
 }
 
-// FetchDirection represents the direction clause passed into a fetch statement.
-type FetchDirection int
+// PLpgSQLFetchDirection represents the direction clause passed into a
+// fetch statement.
+type PLpgSQLFetchDirection int
 
-// CursorOption represents a cursor option, which describes how a cursor will
-// behave.
-type CursorOption uint32
+// PLpgSQLCursorOpt represents a cursor option, which describes
+// how a cursor will behave.
+type PLpgSQLCursorOpt uint32
 
 const (
-	// CursorOptionNone
-	CursorOptionNone CursorOption = iota
-	// CursorOptionBinary describes cursors that return data in binary form.
-	CursorOptionBinary
-	// CursorOptionScroll describes cursors that can retrieve rows in
+	// PLpgSQLCursorOptNone
+	PLpgSQLCursorOptNone PLpgSQLCursorOpt = iota
+	// PLpgSQLCursorOptBinary describes cursors that return data in binary form.
+	PLpgSQLCursorOptBinary
+	// PLpgSQLCursorOptScroll describes cursors that can retrieve rows in
 	// non-sequential fashion.
-	CursorOptionScroll
-	// CursorOptionNoScroll describes cursors that can not retrieve rows in
+	PLpgSQLCursorOptScroll
+	// PLpgSQLCursorOptNoScroll describes cursors that can not retrieve rows in
 	// non-sequential fashion.
-	CursorOptionNoScroll
-	// CursorOptionInsensitive describes cursors that can't see changes to
+	PLpgSQLCursorOptNoScroll
+	// PLpgSQLCursorOptInsensitive describes cursors that can't see changes to
 	// done to data in same txn.
-	CursorOptionInsensitive
-	// CursorOPtionAsensitive describes cursors that may be able to see
+	PLpgSQLCursorOptInsensitive
+	// PLpgSQLCursorOptAsensitive describes cursors that may be able to see
 	// changes to done to data in same txn.
-	CursorOPtionAsensitive
-	// CursorOptionHold describes cursors that can be used after a txn that it
+	PLpgSQLCursorOptAsensitive
+	// PLpgSQLCursorOptHold describes cursors that can be used after a txn that it
 	// was created in commits.
-	CursorOptionHold
-	// CursorOptionFastPlan describes cursors that can not be used after a txn
+	PLpgSQLCursorOptHold
+	// PLpgSQLCursorOptFastPlan describes cursors that can not be used after a txn
 	// that it was created in commits.
-	CursorOptionFastPlan
-	// CursorOptionGenericPlan describes cursors that uses a generic plan.
-	CursorOptionGenericPlan
-	// CursorOptionCustomPlan describes cursors that uses a custom plan.
-	CursorOptionCustomPlan
-	// CursorOptionParallelOK describes cursors that allows parallel queries.
-	CursorOptionParallelOK
+	PLpgSQLCursorOptFastPlan
+	// PLpgSQLCursorOptGenericPlan describes cursors that uses a generic plan.
+	PLpgSQLCursorOptGenericPlan
+	// PLpgSQLCursorOptCustomPlan describes cursors that uses a custom plan.
+	PLpgSQLCursorOptCustomPlan
+	// PLpgSQLCursorOptParallelOK describes cursors that allows parallel queries.
+	PLpgSQLCursorOptParallelOK
 )
 
 // String implements the fmt.Stringer interface.
-func (o CursorOption) String() string {
+func (o PLpgSQLCursorOpt) String() string {
 	switch o {
-	case CursorOptionNoScroll:
+	case PLpgSQLCursorOptNoScroll:
 		return "NO SCROLL"
-	case CursorOptionScroll:
+	case PLpgSQLCursorOptScroll:
 		return "SCROLL"
-	case CursorOptionFastPlan:
+	case PLpgSQLCursorOptFastPlan:
 		return ""
 	// TODO(jane): implement string representation for other opts.
 	default:
@@ -136,19 +138,19 @@ func (o CursorOption) String() string {
 }
 
 // Mask returns the bitmask for a given cursor option.
-func (o CursorOption) Mask() uint32 {
+func (o PLpgSQLCursorOpt) Mask() uint32 {
 	return 1 << o
 }
 
 // IsSetIn returns true if this cursor option is set in the supplied bitfield.
-func (o CursorOption) IsSetIn(bits uint32) bool {
+func (o PLpgSQLCursorOpt) IsSetIn(bits uint32) bool {
 	return bits&o.Mask() != 0
 }
 
-type cursorOptionList []CursorOption
+type plpgSQLCursorOptList []PLpgSQLCursorOpt
 
 // ToBitField returns the bitfield representation of a list of cursor options.
-func (ol cursorOptionList) ToBitField() uint32 {
+func (ol plpgSQLCursorOptList) ToBitField() uint32 {
 	var ret uint32
 	for _, o := range ol {
 		ret |= o.Mask()
@@ -157,19 +159,19 @@ func (ol cursorOptionList) ToBitField() uint32 {
 }
 
 // OptListFromBitField returns a list of cursor option to be printed.
-func OptListFromBitField(m uint32) cursorOptionList {
-	ret := cursorOptionList{}
-	opts := []CursorOption{
-		CursorOptionBinary,
-		CursorOptionScroll,
-		CursorOptionNoScroll,
-		CursorOptionInsensitive,
-		CursorOPtionAsensitive,
-		CursorOptionHold,
-		CursorOptionFastPlan,
-		CursorOptionGenericPlan,
-		CursorOptionCustomPlan,
-		CursorOptionParallelOK,
+func OptListFromBitField(m uint32) plpgSQLCursorOptList {
+	ret := plpgSQLCursorOptList{}
+	opts := []PLpgSQLCursorOpt{
+		PLpgSQLCursorOptBinary,
+		PLpgSQLCursorOptScroll,
+		PLpgSQLCursorOptNoScroll,
+		PLpgSQLCursorOptInsensitive,
+		PLpgSQLCursorOptAsensitive,
+		PLpgSQLCursorOptHold,
+		PLpgSQLCursorOptFastPlan,
+		PLpgSQLCursorOptGenericPlan,
+		PLpgSQLCursorOptCustomPlan,
+		PLpgSQLCursorOptParallelOK,
 	}
 	for _, opt := range opts {
 		if opt.IsSetIn(m) {
diff --git a/pkg/sql/sem/plpgsqltree/exception.go b/pkg/sql/sem/plpgsqltree/exception.go
index 7b77216598c..85766b0cb42 100644
--- a/pkg/sql/sem/plpgsqltree/exception.go
+++ b/pkg/sql/sem/plpgsqltree/exception.go
@@ -16,13 +16,13 @@ import (
 	"github.com/cockroachdb/cockroach/pkg/sql/sem/tree"
 )
 
-type Exception struct {
-	StatementImpl
-	Conditions []Condition
-	Action     []Statement
+type PLpgSQLException struct {
+	PLpgSQLStatementImpl
+	Conditions []PLpgSQLCondition
+	Action     []PLpgSQLStatement
 }
 
-func (s *Exception) Format(ctx *tree.FmtCtx) {
+func (s *PLpgSQLException) Format(ctx *tree.FmtCtx) {
 	ctx.WriteString("WHEN ")
 	for i, cond := range s.Conditions {
 		if i > 0 {
@@ -40,18 +40,18 @@ func (s *Exception) Format(ctx *tree.FmtCtx) {
 	}
 }
 
-func (s *Exception) PlpgSQLStatementTag() string {
+func (s *PLpgSQLException) PlpgSQLStatementTag() string {
 	return "proc_exception"
 }
 
-func (s *Exception) WalkStmt(visitor StatementVisitor) {
+func (s *PLpgSQLException) WalkStmt(visitor PLpgSQLStmtVisitor) {
 	visitor.Visit(s)
 	for _, stmt := range s.Action {
 		stmt.WalkStmt(visitor)
 	}
 }
 
-type Condition struct {
+type PLpgSQLCondition struct {
 	SqlErrState string
 	SqlErrName  string
 }
diff --git a/pkg/sql/sem/plpgsqltree/statements.go b/pkg/sql/sem/plpgsqltree/statements.go
index 68da4883dce..3924f9450b2 100644
--- a/pkg/sql/sem/plpgsqltree/statements.go
+++ b/pkg/sql/sem/plpgsqltree/statements.go
@@ -17,21 +17,21 @@ import (
 	"github.com/cockroachdb/cockroach/pkg/sql/sem/tree"
 )
 
-type Expr = tree.Expr
+type PLpgSQLExpr = tree.Expr
 
-type Statement interface {
+type PLpgSQLStatement interface {
 	tree.NodeFormatter
 	GetLineNo() int
 	GetStmtID() uint
 	plpgsqlStmt()
-	WalkStmt(StatementVisitor)
+	WalkStmt(PLpgSQLStmtVisitor)
 }
 
-type TaggedStatement interface {
+type TaggedPLpgSQLStatement interface {
 	PlpgSQLStatementTag() string
 }
 
-type StatementImpl struct {
+type PLpgSQLStatementImpl struct {
 	// TODO(Chengxiong): figure out how to get line number from scanner.
 	LineNo int
 	/*
@@ -43,27 +43,27 @@ type StatementImpl struct {
 	StmtID uint
 }
 
-func (s *StatementImpl) GetLineNo() int {
+func (s *PLpgSQLStatementImpl) GetLineNo() int {
 	return s.LineNo
 }
 
-func (s *StatementImpl) GetStmtID() uint {
+func (s *PLpgSQLStatementImpl) GetStmtID() uint {
 	return s.StmtID
 }
 
-func (s *StatementImpl) plpgsqlStmt() {}
+func (s *PLpgSQLStatementImpl) plpgsqlStmt() {}
 
 // pl_block
-type Block struct {
-	StatementImpl
+type PLpgSQLStmtBlock struct {
+	PLpgSQLStatementImpl
 	Label      string
-	Decls      []Declaration
-	Body       []Statement
-	Exceptions []Exception
+	Decls      []PLpgSQLDecl
+	Body       []PLpgSQLStatement
+	Exceptions []PLpgSQLException
 }
 
 // TODO(drewk): format Label and Exceptions fields.
-func (s *Block) Format(ctx *tree.FmtCtx) {
+func (s *PLpgSQLStmtBlock) Format(ctx *tree.FmtCtx) {
 	if s.Decls != nil {
 		ctx.WriteString("DECLARE\n")
 		for _, dec := range s.Decls {
@@ -85,11 +85,11 @@ func (s *Block) Format(ctx *tree.FmtCtx) {
 	ctx.WriteString("END\n")
 }
 
-func (s *Block) PlpgSQLStatementTag() string {
+func (s *PLpgSQLStmtBlock) PlpgSQLStatementTag() string {
 	return "stmt_block"
 }
 
-func (s *Block) WalkStmt(visitor StatementVisitor) {
+func (s *PLpgSQLStmtBlock) WalkStmt(visitor PLpgSQLStmtVisitor) {
 	visitor.Visit(s)
 	for _, stmt := range s.Body {
 		stmt.WalkStmt(visitor)
@@ -97,17 +97,17 @@ func (s *Block) WalkStmt(visitor StatementVisitor) {
 }
 
 // decl_stmt
-type Declaration struct {
-	StatementImpl
-	Var      Variable
+type PLpgSQLDecl struct {
+	PLpgSQLStatementImpl
+	Var      PLpgSQLVariable
 	Constant bool
 	Typ      tree.ResolvableTypeReference
 	Collate  string
 	NotNull  bool
-	Expr     Expr
+	Expr     PLpgSQLExpr
 }
 
-func (s *Declaration) Format(ctx *tree.FmtCtx) {
+func (s *PLpgSQLDecl) Format(ctx *tree.FmtCtx) {
 	ctx.WriteString(string(s.Var))
 	if s.Constant {
 		ctx.WriteString(" CONSTANT")
@@ -126,43 +126,43 @@ func (s *Declaration) Format(ctx *tree.FmtCtx) {
 	ctx.WriteString(";\n")
 }
 
-func (s *Declaration) PlpgSQLStatementTag() string {
+func (s *PLpgSQLDecl) PlpgSQLStatementTag() string {
 	return "decl_stmt"
 }
 
-func (s *Declaration) WalkStmt(visitor StatementVisitor) {
+func (s *PLpgSQLDecl) WalkStmt(visitor PLpgSQLStmtVisitor) {
 	visitor.Visit(s)
 }
 
 // stmt_assign
-type Assignment struct {
-	Statement
-	Var   Variable
-	Value Expr
+type PLpgSQLStmtAssign struct {
+	PLpgSQLStatement
+	Var   PLpgSQLVariable
+	Value PLpgSQLExpr
 }
 
-func (s *Assignment) PlpgSQLStatementTag() string {
+func (s *PLpgSQLStmtAssign) PlpgSQLStatementTag() string {
 	return "stmt_assign"
 }
 
-func (s *Assignment) Format(ctx *tree.FmtCtx) {
+func (s *PLpgSQLStmtAssign) Format(ctx *tree.FmtCtx) {
 	ctx.WriteString(fmt.Sprintf("%s := %s;\n", s.Var, s.Value))
 }
 
-func (s *Assignment) WalkStmt(visitor StatementVisitor) {
+func (s *PLpgSQLStmtAssign) WalkStmt(visitor PLpgSQLStmtVisitor) {
 	visitor.Visit(s)
 }
 
 // stmt_if
-type If struct {
-	StatementImpl
-	Condition  Expr
-	ThenBody   []Statement
-	ElseIfList []ElseIf
-	ElseBody   []Statement
+type PLpgSQLStmtIf struct {
+	PLpgSQLStatementImpl
+	Condition  PLpgSQLExpr
+	ThenBody   []PLpgSQLStatement
+	ElseIfList []PLpgSQLStmtIfElseIfArm
+	ElseBody   []PLpgSQLStatement
 }
 
-func (s *If) Format(ctx *tree.FmtCtx) {
+func (s *PLpgSQLStmtIf) Format(ctx *tree.FmtCtx) {
 	ctx.WriteString("IF ")
 	s.Condition.Format(ctx)
 	ctx.WriteString(" THEN\n")
@@ -184,11 +184,11 @@ func (s *If) Format(ctx *tree.FmtCtx) {
 	ctx.WriteString("END IF;\n")
 }
 
-func (s *If) PlpgSQLStatementTag() string {
+func (s *PLpgSQLStmtIf) PlpgSQLStatementTag() string {
 	return "stmt_if"
 }
 
-func (s *If) WalkStmt(visitor StatementVisitor) {
+func (s *PLpgSQLStmtIf) WalkStmt(visitor PLpgSQLStmtVisitor) {
 	visitor.Visit(s)
 
 	for _, thenStmt := range s.ThenBody {
@@ -205,13 +205,13 @@ func (s *If) WalkStmt(visitor StatementVisitor) {
 
 }
 
-type ElseIf struct {
-	StatementImpl
-	Condition Expr
-	Stmts     []Statement
+type PLpgSQLStmtIfElseIfArm struct {
+	PLpgSQLStatementImpl
+	Condition PLpgSQLExpr
+	Stmts     []PLpgSQLStatement
 }
 
-func (s *ElseIf) Format(ctx *tree.FmtCtx) {
+func (s *PLpgSQLStmtIfElseIfArm) Format(ctx *tree.FmtCtx) {
 	ctx.WriteString("ELSIF ")
 	s.Condition.Format(ctx)
 	ctx.WriteString(" THEN\n")
@@ -221,11 +221,11 @@ func (s *ElseIf) Format(ctx *tree.FmtCtx) {
 	}
 }
 
-func (s *ElseIf) PlpgSQLStatementTag() string {
+func (s *PLpgSQLStmtIfElseIfArm) PlpgSQLStatementTag() string {
 	return "stmt_if_else_if"
 }
 
-func (s *ElseIf) WalkStmt(visitor StatementVisitor) {
+func (s *PLpgSQLStmtIfElseIfArm) WalkStmt(visitor PLpgSQLStmtVisitor) {
 	visitor.Visit(s)
 
 	for _, stmt := range s.Stmts {
@@ -234,19 +234,19 @@ func (s *ElseIf) WalkStmt(visitor StatementVisitor) {
 }
 
 // stmt_case
-type Case struct {
-	StatementImpl
-	// TODO(drewk): Change to Expr
+type PLpgSQLStmtCase struct {
+	PLpgSQLStatementImpl
+	// TODO(drewk): Change to PLpgSQLExpr
 	TestExpr     string
-	Var          Variable
-	CaseWhenList []*CaseWhen
+	Var          PLpgSQLVariable
+	CaseWhenList []*PLpgSQLStmtCaseWhenArm
 	HaveElse     bool
-	ElseStmts    []Statement
+	ElseStmts    []PLpgSQLStatement
 }
 
 // TODO(drewk): fix the whitespace/newline formatting for CASE (see the
 // stmt_case test file).
-func (s *Case) Format(ctx *tree.FmtCtx) {
+func (s *PLpgSQLStmtCase) Format(ctx *tree.FmtCtx) {
 	ctx.WriteString("CASE")
 	if len(s.TestExpr) > 0 {
 		ctx.WriteString(fmt.Sprintf(" %s", s.TestExpr))
@@ -265,11 +265,11 @@ func (s *Case) Format(ctx *tree.FmtCtx) {
 	ctx.WriteString("END CASE\n")
 }
 
-func (s *Case) PlpgSQLStatementTag() string {
+func (s *PLpgSQLStmtCase) PlpgSQLStatementTag() string {
 	return "stmt_case"
 }
 
-func (s *Case) WalkStmt(visitor StatementVisitor) {
+func (s *PLpgSQLStmtCase) WalkStmt(visitor PLpgSQLStmtVisitor) {
 	visitor.Visit(s)
 
 	for _, when := range s.CaseWhenList {
@@ -283,14 +283,14 @@ func (s *Case) WalkStmt(visitor StatementVisitor) {
 	}
 }
 
-type CaseWhen struct {
-	StatementImpl
-	// TODO(drewk): Change to Expr
+type PLpgSQLStmtCaseWhenArm struct {
+	PLpgSQLStatementImpl
+	// TODO(drewk): Change to PLpgSQLExpr
 	Expr  string
-	Stmts []Statement
+	Stmts []PLpgSQLStatement
 }
 
-func (s *CaseWhen) Format(ctx *tree.FmtCtx) {
+func (s *PLpgSQLStmtCaseWhenArm) Format(ctx *tree.FmtCtx) {
 	ctx.WriteString(fmt.Sprintf("WHEN %s THEN\n", s.Expr))
 	for i, stmt := range s.Stmts {
 		ctx.WriteString("  ")
@@ -301,11 +301,11 @@ func (s *CaseWhen) Format(ctx *tree.FmtCtx) {
 	}
 }
 
-func (s *CaseWhen) PlpgSQLStatementTag() string {
+func (s *PLpgSQLStmtCaseWhenArm) PlpgSQLStatementTag() string {
 	return "stmt_when"
 }
 
-func (s *CaseWhen) WalkStmt(visitor StatementVisitor) {
+func (s *PLpgSQLStmtCaseWhenArm) WalkStmt(visitor PLpgSQLStmtVisitor) {
 	visitor.Visit(s)
 
 	for _, stmt := range s.Stmts {
@@ -314,17 +314,17 @@ func (s *CaseWhen) WalkStmt(visitor StatementVisitor) {
 }
 
 // stmt_loop
-type Loop struct {
-	StatementImpl
+type PLpgSQLStmtSimpleLoop struct {
+	PLpgSQLStatementImpl
 	Label string
-	Body  []Statement
+	Body  []PLpgSQLStatement
 }
 
-func (s *Loop) PlpgSQLStatementTag() string {
+func (s *PLpgSQLStmtSimpleLoop) PlpgSQLStatementTag() string {
 	return "stmt_simple_loop"
 }
 
-func (s *Loop) Format(ctx *tree.FmtCtx) {
+func (s *PLpgSQLStmtSimpleLoop) Format(ctx *tree.FmtCtx) {
 	ctx.WriteString("LOOP\n")
 	for _, stmt := range s.Body {
 		stmt.Format(ctx)
@@ -336,7 +336,7 @@ func (s *Loop) Format(ctx *tree.FmtCtx) {
 	ctx.WriteString(";\n")
 }
 
-func (s *Loop) WalkStmt(visitor StatementVisitor) {
+func (s *PLpgSQLStmtSimpleLoop) WalkStmt(visitor PLpgSQLStmtVisitor) {
 	visitor.Visit(s)
 	for _, stmt := range s.Body {
 		stmt.WalkStmt(visitor)
@@ -344,32 +344,21 @@ func (s *Loop) WalkStmt(visitor StatementVisitor) {
 }
 
 // stmt_while
-type While struct {
-	StatementImpl
+type PLpgSQLStmtWhileLoop struct {
+	PLpgSQLStatementImpl
 	Label     string
-	Condition Expr
-	Body      []Statement
+	Condition PLpgSQLExpr
+	Body      []PLpgSQLStatement
 }
 
-func (s *While) Format(ctx *tree.FmtCtx) {
-	ctx.WriteString("WHILE ")
-	s.Condition.Format(ctx)
-	ctx.WriteString(" LOOP\n")
-	for _, stmt := range s.Body {
-		stmt.Format(ctx)
-	}
-	ctx.WriteString("END LOOP")
-	if s.Label != "" {
-		ctx.WriteString(fmt.Sprintf(" %s", s.Label))
-	}
-	ctx.WriteString(";\n")
+func (s *PLpgSQLStmtWhileLoop) Format(ctx *tree.FmtCtx) {
 }
 
-func (s *While) PlpgSQLStatementTag() string {
+func (s *PLpgSQLStmtWhileLoop) PlpgSQLStatementTag() string {
 	return "stmt_while"
 }
 
-func (s *While) WalkStmt(visitor StatementVisitor) {
+func (s *PLpgSQLStmtWhileLoop) WalkStmt(visitor PLpgSQLStmtVisitor) {
 	visitor.Visit(s)
 	for _, stmt := range s.Body {
 		stmt.WalkStmt(visitor)
@@ -377,123 +366,123 @@ func (s *While) WalkStmt(visitor StatementVisitor) {
 }
 
 // stmt_for
-type ForInt struct {
-	StatementImpl
+type PLpgSQLStmtForIntLoop struct {
+	PLpgSQLStatementImpl
 	Label   string
-	Var     Variable
-	Lower   Expr
-	Upper   Expr
-	Step    Expr
+	Var     PLpgSQLVariable
+	Lower   PLpgSQLExpr
+	Upper   PLpgSQLExpr
+	Step    PLpgSQLExpr
 	Reverse int
-	Body    []Statement
+	Body    []PLpgSQLStatement
 }
 
-func (s *ForInt) Format(ctx *tree.FmtCtx) {
+func (s *PLpgSQLStmtForIntLoop) Format(ctx *tree.FmtCtx) {
 }
 
-func (s *ForInt) PlpgSQLStatementTag() string {
+func (s *PLpgSQLStmtForIntLoop) PlpgSQLStatementTag() string {
 	return "stmt_for_int_loop"
 }
 
-func (s *ForInt) WalkStmt(visitor StatementVisitor) {
+func (s *PLpgSQLStmtForIntLoop) WalkStmt(visitor PLpgSQLStmtVisitor) {
 	visitor.Visit(s)
 	for _, stmt := range s.Body {
 		stmt.WalkStmt(visitor)
 	}
 }
 
-type ForQuery struct {
-	StatementImpl
+type PLpgSQLStmtForQueryLoop struct {
+	PLpgSQLStatementImpl
 	Label string
-	Var   Variable
-	Body  []Statement
+	Var   PLpgSQLVariable
+	Body  []PLpgSQLStatement
 }
 
-func (s *ForQuery) Format(ctx *tree.FmtCtx) {
+func (s *PLpgSQLStmtForQueryLoop) Format(ctx *tree.FmtCtx) {
 }
 
-func (s *ForQuery) PlpgSQLStatementTag() string {
+func (s *PLpgSQLStmtForQueryLoop) PlpgSQLStatementTag() string {
 	return "stmt_for_query_loop"
 }
 
-func (s *ForQuery) WalkStmt(visitor StatementVisitor) {
+func (s *PLpgSQLStmtForQueryLoop) WalkStmt(visitor PLpgSQLStmtVisitor) {
 	visitor.Visit(s)
 	for _, stmt := range s.Body {
 		stmt.WalkStmt(visitor)
 	}
 }
 
-type ForSelect struct {
-	ForQuery
-	Query Expr
+type PLpgSQLStmtForQuerySelectLoop struct {
+	PLpgSQLStmtForQueryLoop
+	Query PLpgSQLExpr
 }
 
-func (s *ForSelect) Format(ctx *tree.FmtCtx) {
+func (s *PLpgSQLStmtForQuerySelectLoop) Format(ctx *tree.FmtCtx) {
 }
 
-func (s *ForSelect) PlpgSQLStatementTag() string {
+func (s *PLpgSQLStmtForQuerySelectLoop) PlpgSQLStatementTag() string {
 	return "stmt_query_select_loop"
 }
 
-func (s *ForSelect) WalkStmt(visitor StatementVisitor) {
+func (s *PLpgSQLStmtForQuerySelectLoop) WalkStmt(visitor PLpgSQLStmtVisitor) {
 	visitor.Visit(s)
-	s.ForQuery.WalkStmt(visitor)
+	s.PLpgSQLStmtForQueryLoop.WalkStmt(visitor)
 }
 
-type ForCursor struct {
-	ForQuery
+type PLpgSQLStmtForQueryCursorLoop struct {
+	PLpgSQLStmtForQueryLoop
 	CurVar   int // TODO(drewk): is this CursorVariable?
-	ArgQuery Expr
+	ArgQuery PLpgSQLExpr
 }
 
-func (s *ForCursor) Format(ctx *tree.FmtCtx) {
+func (s *PLpgSQLStmtForQueryCursorLoop) Format(ctx *tree.FmtCtx) {
 }
 
-func (s *ForCursor) PlpgSQLStatementTag() string {
+func (s *PLpgSQLStmtForQueryCursorLoop) PlpgSQLStatementTag() string {
 	return "stmt_for_query_cursor_loop"
 }
 
-func (s *ForCursor) WalkStmt(visitor StatementVisitor) {
+func (s *PLpgSQLStmtForQueryCursorLoop) WalkStmt(visitor PLpgSQLStmtVisitor) {
 	visitor.Visit(s)
-	s.ForQuery.WalkStmt(visitor)
+	s.PLpgSQLStmtForQueryLoop.WalkStmt(visitor)
 }
 
-type ForDynamic struct {
-	ForQuery
-	Query  Expr
-	Params []Expr
+type PLpgSQLStmtForDynamicLoop struct {
+	PLpgSQLStmtForQueryLoop
+	Query  PLpgSQLExpr
+	Params []PLpgSQLExpr
 }
 
-func (s *ForDynamic) Format(ctx *tree.FmtCtx) {
+func (s *PLpgSQLStmtForDynamicLoop) Format(ctx *tree.FmtCtx) {
 }
 
-func (s *ForDynamic) PlpgSQLStatementTag() string {
+func (s *PLpgSQLStmtForDynamicLoop) PlpgSQLStatementTag() string {
 	return "stmt_for_dyn_loop"
 }
 
-func (s *ForDynamic) WalkStmt(visitor StatementVisitor) {
+func (s *PLpgSQLStmtForDynamicLoop) WalkStmt(visitor PLpgSQLStmtVisitor) {
 	visitor.Visit(s)
-	s.ForQuery.WalkStmt(visitor)
+	s.PLpgSQLStmtForQueryLoop.WalkStmt(visitor)
 }
 
 // stmt_foreach_a
-type ForEachArray struct {
-	StatementImpl
+type PLpgSQLStmtForEachALoop struct {
+	PLpgSQLStatementImpl
 	Label string
-	Var   *Variable
+	Var   *PLpgSQLVariable
 	Slice int // TODO(drewk): not sure what this is
-	Expr  Expr
-	Body  []Statement
+	Expr  PLpgSQLExpr
+	Body  []PLpgSQLStatement
 }
 
-func (s *ForEachArray) Format(ctx *tree.FmtCtx) {
+func (s *PLpgSQLStmtForEachALoop) Format(ctx *tree.FmtCtx) {
 }
 
-func (s *ForEachArray) PlpgSQLStatementTag() string {
+func (s *PLpgSQLStmtForEachALoop) PlpgSQLStatementTag() string {
 	return "stmt_for_each_a"
 }
 
-func (s *ForEachArray) WalkStmt(visitor StatementVisitor) {
+func (s *PLpgSQLStmtForEachALoop) WalkStmt(visitor PLpgSQLStmtVisitor) {
 	visitor.Visit(s)
 
 	for _, stmt := range s.Body {
@@ -502,13 +491,13 @@ func (s *ForEachArray) WalkStmt(visitor StatementVisitor) {
 }
 
 // stmt_exit
-type Exit struct {
-	StatementImpl
+type PLpgSQLStmtExit struct {
+	PLpgSQLStatementImpl
 	Label     string
-	Condition Expr
+	Condition PLpgSQLExpr
 }
 
-func (s *Exit) Format(ctx *tree.FmtCtx) {
+func (s *PLpgSQLStmtExit) Format(ctx *tree.FmtCtx) {
 	ctx.WriteString("EXIT")
 	if s.Label != "" {
 		ctx.WriteString(fmt.Sprintf(" %s", s.Label))
@@ -521,22 +510,22 @@ func (s *Exit) Format(ctx *tree.FmtCtx) {
 
 }
 
-func (s *Exit) PlpgSQLStatementTag() string {
+func (s *PLpgSQLStmtExit) PlpgSQLStatementTag() string {
 	return "stmt_exit"
 }
 
-func (s *Exit) WalkStmt(visitor StatementVisitor) {
+func (s *PLpgSQLStmtExit) WalkStmt(visitor PLpgSQLStmtVisitor) {
 	visitor.Visit(s)
 }
 
 // stmt_continue
-type Continue struct {
-	StatementImpl
+type PLpgSQLStmtContinue struct {
+	PLpgSQLStatementImpl
 	Label     string
-	Condition Expr
+	Condition PLpgSQLExpr
 }
 
-func (s *Continue) Format(ctx *tree.FmtCtx) {
+func (s *PLpgSQLStmtContinue) Format(ctx *tree.FmtCtx) {
 	ctx.WriteString("CONTINUE")
 	if s.Label != "" {
 		ctx.WriteString(fmt.Sprintf(" %s", s.Label))
@@ -548,22 +537,22 @@ func (s *Continue) Format(ctx *tree.FmtCtx) {
 	ctx.WriteString(";\n")
 }
 
-func (s *Continue) PlpgSQLStatementTag() string {
+func (s *PLpgSQLStmtContinue) PlpgSQLStatementTag() string {
 	return "stmt_continue"
 }
 
-func (s *Continue) WalkStmt(visitor StatementVisitor) {
+func (s *PLpgSQLStmtContinue) WalkStmt(visitor PLpgSQLStmtVisitor) {
 	visitor.Visit(s)
 }
 
 // stmt_return
-type Return struct {
-	StatementImpl
-	Expr   Expr
-	RetVar Variable
+type PLpgSQLStmtReturn struct {
+	PLpgSQLStatementImpl
+	Expr   PLpgSQLExpr
+	RetVar PLpgSQLVariable
 }
 
-func (s *Return) Format(ctx *tree.FmtCtx) {
+func (s *PLpgSQLStmtReturn) Format(ctx *tree.FmtCtx) {
 	ctx.WriteString("RETURN ")
 	if s.Expr == nil {
 		s.RetVar.Format(ctx)
@@ -573,61 +562,61 @@ func (s *Return) Format(ctx *tree.FmtCtx) {
 	ctx.WriteString(";\n")
 }
 
-func (s *Return) PlpgSQLStatementTag() string {
+func (s *PLpgSQLStmtReturn) PlpgSQLStatementTag() string {
 	return "stmt_return"
 }
 
-func (s *Return) WalkStmt(visitor StatementVisitor) {
+func (s *PLpgSQLStmtReturn) WalkStmt(visitor PLpgSQLStmtVisitor) {
 	visitor.Visit(s)
 }
 
-type ReturnNext struct {
-	StatementImpl
-	Expr   Expr
-	RetVar Variable
+type PLpgSQLStmtReturnNext struct {
+	PLpgSQLStatementImpl
+	Expr   PLpgSQLExpr
+	RetVar PLpgSQLVariable
 }
 
-func (s *ReturnNext) Format(ctx *tree.FmtCtx) {
+func (s *PLpgSQLStmtReturnNext) Format(ctx *tree.FmtCtx) {
 }
 
-func (s *ReturnNext) PlpgSQLStatementTag() string {
+func (s *PLpgSQLStmtReturnNext) PlpgSQLStatementTag() string {
 	return "stmt_return_next"
 }
 
-func (s *ReturnNext) WalkStmt(visitor StatementVisitor) {
+func (s *PLpgSQLStmtReturnNext) WalkStmt(visitor PLpgSQLStmtVisitor) {
 	visitor.Visit(s)
 }
 
-type ReturnQuery struct {
-	StatementImpl
-	Query        Expr
-	DynamicQuery Expr
-	Params       []Expr
+type PLpgSQLStmtReturnQuery struct {
+	PLpgSQLStatementImpl
+	Query        PLpgSQLExpr
+	DynamicQuery PLpgSQLExpr
+	Params       []PLpgSQLExpr
 }
 
-func (s *ReturnQuery) Format(ctx *tree.FmtCtx) {
+func (s *PLpgSQLStmtReturnQuery) Format(ctx *tree.FmtCtx) {
 }
 
-func (s *ReturnQuery) PlpgSQLStatementTag() string {
+func (s *PLpgSQLStmtReturnQuery) PlpgSQLStatementTag() string {
 	return "stmt_return_query"
 }
 
-func (s *ReturnQuery) WalkStmt(visitor StatementVisitor) {
+func (s *PLpgSQLStmtReturnQuery) WalkStmt(visitor PLpgSQLStmtVisitor) {
 	visitor.Visit(s)
 }
 
 // stmt_raise
-type Raise struct {
-	StatementImpl
+type PLpgSQLStmtRaise struct {
+	PLpgSQLStatementImpl
 	LogLevel string
 	Code     string
 	CodeName string
 	Message  string
-	Params   []Expr
-	Options  []RaiseOption
+	Params   []PLpgSQLExpr
+	Options  []PLpgSQLStmtRaiseOption
 }
 
-func (s *Raise) Format(ctx *tree.FmtCtx) {
+func (s *PLpgSQLStmtRaise) Format(ctx *tree.FmtCtx) {
 	ctx.WriteString("RAISE")
 	if s.LogLevel != "" {
 		ctx.WriteString(" ")
@@ -657,53 +646,53 @@ func (s *Raise) Format(ctx *tree.FmtCtx) {
 	ctx.WriteString(";\n")
 }
 
-type RaiseOption struct {
+type PLpgSQLStmtRaiseOption struct {
 	OptType string
-	Expr    Expr
+	Expr    PLpgSQLExpr
 }
 
-func (s *RaiseOption) Format(ctx *tree.FmtCtx) {
+func (s *PLpgSQLStmtRaiseOption) Format(ctx *tree.FmtCtx) {
 	ctx.WriteString(fmt.Sprintf("%s = ", strings.ToUpper(s.OptType)))
 	s.Expr.Format(ctx)
 }
 
-func (s *Raise) PlpgSQLStatementTag() string {
+func (s *PLpgSQLStmtRaise) PlpgSQLStatementTag() string {
 	return "stmt_raise"
 }
 
-func (s *Raise) WalkStmt(visitor StatementVisitor) {
+func (s *PLpgSQLStmtRaise) WalkStmt(visitor PLpgSQLStmtVisitor) {
 	visitor.Visit(s)
 }
 
 // stmt_assert
-type Assert struct {
-	StatementImpl
-	Condition Expr
-	Message   Expr
+type PLpgSQLStmtAssert struct {
+	PLpgSQLStatementImpl
+	Condition PLpgSQLExpr
+	Message   PLpgSQLExpr
 }
 
-func (s *Assert) Format(ctx *tree.FmtCtx) {
+func (s *PLpgSQLStmtAssert) Format(ctx *tree.FmtCtx) {
 	// TODO(drewk): Pretty print the assert condition and message
 	ctx.WriteString("ASSERT\n")
 }
 
-func (s *Assert) PlpgSQLStatementTag() string {
+func (s *PLpgSQLStmtAssert) PlpgSQLStatementTag() string {
 	return "stmt_assert"
 }
 
-func (s *Assert) WalkStmt(visitor StatementVisitor) {
+func (s *PLpgSQLStmtAssert) WalkStmt(visitor PLpgSQLStmtVisitor) {
 	visitor.Visit(s)
 }
 
 // stmt_execsql
-type Execute struct {
-	StatementImpl
+type PLpgSQLStmtExecSql struct {
+	PLpgSQLStatementImpl
 	SqlStmt tree.Statement
 	Strict  bool // INTO STRICT flag
-	Target  []Variable
+	Target  []PLpgSQLVariable
 }
 
-func (s *Execute) Format(ctx *tree.FmtCtx) {
+func (s *PLpgSQLStmtExecSql) Format(ctx *tree.FmtCtx) {
 	s.SqlStmt.Format(ctx)
 	if s.Target != nil {
 		ctx.WriteString(" INTO ")
@@ -720,26 +709,26 @@ func (s *Execute) Format(ctx *tree.FmtCtx) {
 	ctx.WriteString(";\n")
 }
 
-func (s *Execute) PlpgSQLStatementTag() string {
+func (s *PLpgSQLStmtExecSql) PlpgSQLStatementTag() string {
 	return "stmt_exec_sql"
 }
 
-func (s *Execute) WalkStmt(visitor StatementVisitor) {
+func (s *PLpgSQLStmtExecSql) WalkStmt(visitor PLpgSQLStmtVisitor) {
 	visitor.Visit(s)
 }
 
 // stmt_dynexecute
 // TODO(chengxiong): query should be a better expression type.
-type DynamicExecute struct {
-	StatementImpl
+type PLpgSQLStmtDynamicExecute struct {
+	PLpgSQLStatementImpl
 	Query  string
 	Into   bool
 	Strict bool
-	Target Variable
-	Params []Expr
+	Target PLpgSQLVariable
+	Params []PLpgSQLExpr
 }
 
-func (s *DynamicExecute) Format(ctx *tree.FmtCtx) {
+func (s *PLpgSQLStmtDynamicExecute) Format(ctx *tree.FmtCtx) {
 	// TODO(drewk): Pretty print the original command
 	ctx.WriteString("EXECUTE a dynamic command")
 	if s.Into {
@@ -754,40 +743,40 @@ func (s *DynamicExecute) Format(ctx *tree.FmtCtx) {
 	ctx.WriteString("\n")
 }
 
-func (s *DynamicExecute) PlpgSQLStatementTag() string {
+func (s *PLpgSQLStmtDynamicExecute) PlpgSQLStatementTag() string {
 	return "stmt_dyn_exec"
 }
 
-func (s *DynamicExecute) WalkStmt(visitor StatementVisitor) {
+func (s *PLpgSQLStmtDynamicExecute) WalkStmt(visitor PLpgSQLStmtVisitor) {
 	visitor.Visit(s)
 }
 
 // stmt_perform
-type Perform struct {
-	StatementImpl
-	Expr Expr
+type PLpgSQLStmtPerform struct {
+	PLpgSQLStatementImpl
+	Expr PLpgSQLExpr
 }
 
-func (s *Perform) Format(ctx *tree.FmtCtx) {
+func (s *PLpgSQLStmtPerform) Format(ctx *tree.FmtCtx) {
 }
 
-func (s *Perform) PlpgSQLStatementTag() string {
+func (s *PLpgSQLStmtPerform) PlpgSQLStatementTag() string {
 	return "stmt_perform"
 }
 
-func (s *Perform) WalkStmt(visitor StatementVisitor) {
+func (s *PLpgSQLStmtPerform) WalkStmt(visitor PLpgSQLStmtVisitor) {
 	visitor.Visit(s)
 }
 
 // stmt_call
-type Call struct {
-	StatementImpl
-	Expr   Expr
+type PLpgSQLStmtCall struct {
+	PLpgSQLStatementImpl
+	Expr   PLpgSQLExpr
 	IsCall bool
-	Target Variable
+	Target PLpgSQLVariable
 }
 
-func (s *Call) Format(ctx *tree.FmtCtx) {
+func (s *PLpgSQLStmtCall) Format(ctx *tree.FmtCtx) {
 	// TODO(drewk): Correct the Call field and print the Expr and Target.
 	if s.IsCall {
 		ctx.WriteString("CALL a function/procedure\n")
@@ -796,22 +785,22 @@ func (s *Call) Format(ctx *tree.FmtCtx) {
 	}
 }
 
-func (s *Call) PlpgSQLStatementTag() string {
+func (s *PLpgSQLStmtCall) PlpgSQLStatementTag() string {
 	return "stmt_call"
 }
 
-func (s *Call) WalkStmt(visitor StatementVisitor) {
+func (s *PLpgSQLStmtCall) WalkStmt(visitor PLpgSQLStmtVisitor) {
 	visitor.Visit(s)
 }
 
 // stmt_getdiag
-type GetDiagnostics struct {
-	StatementImpl
+type PLpgSQLStmtGetDiag struct {
+	PLpgSQLStatementImpl
 	IsStacked bool
-	DiagItems GetDiagnosticsItemList // TODO(drewk): what is this?
+	DiagItems PLpgSQLStmtGetDiagItemList // TODO(drewk): what is this?
 }
 
-func (s *GetDiagnostics) Format(ctx *tree.FmtCtx) {
+func (s *PLpgSQLStmtGetDiag) Format(ctx *tree.FmtCtx) {
 	if s.IsStacked {
 		ctx.WriteString("GET STACKED DIAGNOSTICS ")
 	} else {
@@ -826,46 +815,46 @@ func (s *GetDiagnostics) Format(ctx *tree.FmtCtx) {
 	ctx.WriteString("\n")
 }
 
-type GetDiagnosticsItem struct {
-	Kind GetDiagnosticsKind
+type PLpgSQLStmtGetDiagItem struct {
+	Kind PLpgSQLGetDiagKind
 	// TODO(jane): TargetName is temporary -- should be removed and use Target.
 	TargetName string
 	Target     int // where to assign it?
 }
 
-func (s *GetDiagnosticsItem) Format(ctx *tree.FmtCtx) {
+func (s *PLpgSQLStmtGetDiagItem) Format(ctx *tree.FmtCtx) {
 	ctx.WriteString(fmt.Sprintf("%s := %s", s.TargetName, s.Kind.String()))
 }
 
-type GetDiagnosticsItemList []*GetDiagnosticsItem
+type PLpgSQLStmtGetDiagItemList []*PLpgSQLStmtGetDiagItem
 
-func (s *GetDiagnostics) PlpgSQLStatementTag() string {
+func (s *PLpgSQLStmtGetDiag) PlpgSQLStatementTag() string {
 	return "stmt_get_diag"
 }
 
-func (s *GetDiagnostics) WalkStmt(visitor StatementVisitor) {
+func (s *PLpgSQLStmtGetDiag) WalkStmt(visitor PLpgSQLStmtVisitor) {
 	visitor.Visit(s)
 }
 
 // stmt_open
-type Open struct {
-	StatementImpl
-	CurVar        int // TODO(drewk): this could just a Variable
+type PLpgSQLStmtOpen struct {
+	PLpgSQLStatementImpl
+	CurVar        int // TODO(drewk): this could just a PLpgSQLVariable
 	CursorOptions uint32
 	// TODO(jane): This is temporary and we should remove it and use CurVar.
 	CursorName       string
 	WithExplicitExpr bool
-	// TODO(jane): Should be Expr
+	// TODO(jane): Should be PLpgSQLExpr
 	ArgQuery string
-	// TODO(jane): Should be Expr
+	// TODO(jane): Should be PLpgSQLExpr
 	Query string
-	// TODO(jane): Should be Expr
+	// TODO(jane): Should be PLpgSQLExpr
 	DynamicQuery string
-	// TODO(jane): Should be []Expr
+	// TODO(jane): Should be []PLpgSQLExpr
 	Params []string
 }
 
-func (s *Open) Format(ctx *tree.FmtCtx) {
+func (s *PLpgSQLStmtOpen) Format(ctx *tree.FmtCtx) {
 	ctx.WriteString(
 		fmt.Sprintf(
 			"OPEN %s ",
@@ -896,108 +885,108 @@ func (s *Open) Format(ctx *tree.FmtCtx) {
 	ctx.WriteString("\n")
 }
 
-func (s *Open) PlpgSQLStatementTag() string {
+func (s *PLpgSQLStmtOpen) PlpgSQLStatementTag() string {
 	return "stmt_open"
 }
 
-func (s *Open) WalkStmt(visitor StatementVisitor) {
+func (s *PLpgSQLStmtOpen) WalkStmt(visitor PLpgSQLStmtVisitor) {
 	visitor.Visit(s)
 }
 
 // stmt_fetch
 // stmt_move (where IsMove = true)
-type Fetch struct {
-	StatementImpl
-	Target           Variable
-	CurVar           int // TODO(drewk): this could just a Variable
-	Direction        FetchDirection
+type PLpgSQLStmtFetch struct {
+	PLpgSQLStatementImpl
+	Target           PLpgSQLVariable
+	CurVar           int // TODO(drewk): this could just a PLpgSQLVariable
+	Direction        PLpgSQLFetchDirection
 	HowMany          int64
-	Expr             Expr
+	Expr             PLpgSQLExpr
 	IsMove           bool
 	ReturnsMultiRows bool
 }
 
-func (s *Fetch) Format(ctx *tree.FmtCtx) {
+func (s *PLpgSQLStmtFetch) Format(ctx *tree.FmtCtx) {
 }
 
-func (s *Fetch) PlpgSQLStatementTag() string {
+func (s *PLpgSQLStmtFetch) PlpgSQLStatementTag() string {
 	if s.IsMove {
 		return "stmt_move"
 	}
 	return "stmt_fetch"
 }
 
-func (s *Fetch) WalkStmt(visitor StatementVisitor) {
+func (s *PLpgSQLStmtFetch) WalkStmt(visitor PLpgSQLStmtVisitor) {
 	visitor.Visit(s)
 }
 
 // stmt_close
-type Close struct {
-	StatementImpl
-	CurVar int // TODO(drewk): this could just a Variable
+type PLpgSQLStmtClose struct {
+	PLpgSQLStatementImpl
+	CurVar int // TODO(drewk): this could just a PLpgSQLVariable
 }
 
-func (s *Close) Format(ctx *tree.FmtCtx) {
+func (s *PLpgSQLStmtClose) Format(ctx *tree.FmtCtx) {
 	// TODO(drewk): Pretty- Print the cursor identifier
 	ctx.WriteString("CLOSE a cursor\n")
 
 }
 
-func (s *Close) PlpgSQLStatementTag() string {
+func (s *PLpgSQLStmtClose) PlpgSQLStatementTag() string {
 	return "stmt_close"
 }
 
-func (s *Close) WalkStmt(visitor StatementVisitor) {
+func (s *PLpgSQLStmtClose) WalkStmt(visitor PLpgSQLStmtVisitor) {
 	visitor.Visit(s)
 }
 
 // stmt_commit
-type Commit struct {
-	StatementImpl
+type PLpgSQLStmtCommit struct {
+	PLpgSQLStatementImpl
 	Chain bool
 }
 
-func (s *Commit) Format(ctx *tree.FmtCtx) {
+func (s *PLpgSQLStmtCommit) Format(ctx *tree.FmtCtx) {
 }
 
-func (s *Commit) PlpgSQLStatementTag() string {
+func (s *PLpgSQLStmtCommit) PlpgSQLStatementTag() string {
 	return "stmt_commit"
 }
 
-func (s *Commit) WalkStmt(visitor StatementVisitor) {
+func (s *PLpgSQLStmtCommit) WalkStmt(visitor PLpgSQLStmtVisitor) {
 	visitor.Visit(s)
 }
 
 // stmt_rollback
-type Rollback struct {
-	StatementImpl
+type PLpgSQLStmtRollback struct {
+	PLpgSQLStatementImpl
 	Chain bool
 }
 
-func (s *Rollback) Format(ctx *tree.FmtCtx) {
+func (s *PLpgSQLStmtRollback) Format(ctx *tree.FmtCtx) {
 }
 
-func (s *Rollback) PlpgSQLStatementTag() string {
+func (s *PLpgSQLStmtRollback) PlpgSQLStatementTag() string {
 	return "stmt_rollback"
 }
 
-func (s *Rollback) WalkStmt(visitor StatementVisitor) {
+func (s *PLpgSQLStmtRollback) WalkStmt(visitor PLpgSQLStmtVisitor) {
 	visitor.Visit(s)
 }
 
 // stmt_null
-type Null struct {
-	StatementImpl
+type PLpgSQLStmtNull struct {
+	PLpgSQLStatementImpl
 }
 
-func (s *Null) Format(ctx *tree.FmtCtx) {
+func (s *PLpgSQLStmtNull) Format(ctx *tree.FmtCtx) {
 	ctx.WriteString("NULL\n")
 }
 
-func (s *Null) PlpgSQLStatementTag() string {
+func (s *PLpgSQLStmtNull) PlpgSQLStatementTag() string {
 	return "stmt_null"
 }
 
-func (s *Null) WalkStmt(visitor StatementVisitor) {
+func (s *PLpgSQLStmtNull) WalkStmt(visitor PLpgSQLStmtVisitor) {
 	visitor.Visit(s)
 }
diff --git a/pkg/sql/sem/plpgsqltree/telemetry_visitor.go b/pkg/sql/sem/plpgsqltree/telemetry_visitor.go
index cbad8e13fe5..252c2027826 100644
--- a/pkg/sql/sem/plpgsqltree/telemetry_visitor.go
+++ b/pkg/sql/sem/plpgsqltree/telemetry_visitor.go
@@ -10,14 +10,14 @@
 
 package plpgsqltree
 
-// StatementVisitor defines methods that are called plpgsql statements during
+// PLpgSQLStmtVisitor defines methods that are called plpgsql statements during
 // a statement walk.
-type StatementVisitor interface {
+type PLpgSQLStmtVisitor interface {
 	// Visit is called during a statement walk.
-	Visit(stmt Statement)
+	Visit(stmt PLpgSQLStatement)
 }
 
 // Walk traverses the plpgsql statement.
-func Walk(v StatementVisitor, stmt Statement) {
+func Walk(v PLpgSQLStmtVisitor, stmt PLpgSQLStatement) {
 	stmt.WalkStmt(v)
 }
diff --git a/pkg/sql/sem/plpgsqltree/utils/plpg_visitor.go b/pkg/sql/sem/plpgsqltree/utils/plpg_visitor.go
index 0e775e864ef..e61037839a1 100644
--- a/pkg/sql/sem/plpgsqltree/utils/plpg_visitor.go
+++ b/pkg/sql/sem/plpgsqltree/utils/plpg_visitor.go
@@ -55,11 +55,11 @@ type telemetryVisitor struct {
 	Err     error
 }
 
-var _ plpgsqltree.StatementVisitor = &telemetryVisitor{}
+var _ plpgsqltree.PLpgSQLStmtVisitor = &telemetryVisitor{}
 
-// Visit implements the StatementVisitor interface
-func (v *telemetryVisitor) Visit(stmt plpgsqltree.Statement) {
-	taggedStmt, ok := stmt.(plpgsqltree.TaggedStatement)
+// Visit implements the PLpgSQLStmtVisitor interface
+func (v *telemetryVisitor) Visit(stmt plpgsqltree.PLpgSQLStatement) {
+	taggedStmt, ok := stmt.(plpgsqltree.TaggedPLpgSQLStatement)
 	if !ok {
 		v.Err = errors.AssertionFailedf("no tag found for stmt %q", stmt)
 	}
diff --git a/pkg/sql/sem/plpgsqltree/variable.go b/pkg/sql/sem/plpgsqltree/variable.go
index b4cdb315f5d..a19c74ca4fe 100644
--- a/pkg/sql/sem/plpgsqltree/variable.go
+++ b/pkg/sql/sem/plpgsqltree/variable.go
@@ -12,4 +12,4 @@ package plpgsqltree
 
 import "github.com/cockroachdb/cockroach/pkg/sql/sem/tree"
 
-type Variable = tree.Name
+type PLpgSQLVariable = tree.Name
diff --git a/pkg/storage/engine.go b/pkg/storage/engine.go
index af7bc28653f..869dd07f3d5 100644
--- a/pkg/storage/engine.go
+++ b/pkg/storage/engine.go
@@ -639,6 +639,25 @@ type Reader interface {
 	PinEngineStateForIterators() error
 }
 
+// ReaderWithMustIterators is a Reader that guarantees no errors during
+// iterator creation.
+//
+// TODO(bilal): The only user of this interface is NewMVCCIncrementalIterator.
+// Update that method to handle errors and remove this interface.
+type ReaderWithMustIterators interface {
+	Reader
+
+	// MustMVCCIterator is identical to NewMVCCIterator, except it is implemented
+	// only for those Reader implementations that do not return an error on
+	// iterator creation.
+	MustMVCCIterator(iterKind MVCCIterKind, opts IterOptions) MVCCIterator
+
+	// MustEngineIterator is identical to NewEngineIterator, except it is
+	// implemented only for those Reader implementations that do not return an
+	// error on iterator creation.
+	MustEngineIterator(opts IterOptions) EngineIterator
+}
+
 // Writer is the write interface to an engine's data.
 type Writer interface {
 	// ApplyBatchRepr atomically applies a set of batched updates. Created by
@@ -907,6 +926,15 @@ type ReadWriter interface {
 	Writer
 }
 
+// ReadWriterWithMustIterators is a version of ReadWriter that supports iterator
+// creation without errors.
+//
+// TODO(bilal): Move away from this interface and onto ReadWriter.
+type ReadWriterWithMustIterators interface {
+	ReaderWithMustIterators
+	Writer
+}
+
 // DurabilityRequirement is an advanced option. If in doubt, use
 // StandardDurability.
 //
@@ -925,7 +953,7 @@ const (
 
 // Engine is the interface that wraps the core operations of a key/value store.
 type Engine interface {
-	Reader
+	ReaderWithMustIterators
 	Writer
 	// Attrs returns the engine/store attributes.
 	Attrs() roachpb.Attributes
@@ -1076,7 +1104,7 @@ type Batch interface {
 	// iterator creation. To guarantee that they see all the mutations, the
 	// iterator has to be repositioned using a seek operation, after the
 	// mutations were done.
-	Reader
+	ReaderWithMustIterators
 	WriteBatch
 }
 
diff --git a/pkg/storage/mvcc.go b/pkg/storage/mvcc.go
index d9800ed37f2..260cb9b29fb 100644
--- a/pkg/storage/mvcc.go
+++ b/pkg/storage/mvcc.go
@@ -2871,16 +2871,13 @@ func MVCCClearTimeRange(
 	// time-range, as we do not want to clear any running transactions. We don't
 	// _expect_ to hit this since the RevertRange is only intended for non-live
 	// key spans, but there could be an intent leftover.
-	iter, err := NewMVCCIncrementalIterator(rw, MVCCIncrementalIterOptions{
+	iter := NewMVCCIncrementalIterator(rw.(ReaderWithMustIterators), MVCCIncrementalIterOptions{
 		KeyTypes:  IterKeyTypePointsAndRanges,
 		StartKey:  key,
 		EndKey:    endKey,
 		StartTime: startTime,
 		EndTime:   endTime,
 	})
-	if err != nil {
-		return nil, err
-	}
 	defer iter.Close()
 
 	// clearedMetaKey is the latest surfaced key that will get cleared.
@@ -3350,16 +3347,13 @@ func MVCCPredicateDeleteRange(
 	// endTime. We don't _expect_ to hit intents or newer keys in the client
 	// provided span since the MVCCPredicateDeleteRange is only intended for
 	// non-live key spans, but there could be an intent leftover.
-	iter, err := NewMVCCIncrementalIterator(rw, MVCCIncrementalIterOptions{
+	iter := NewMVCCIncrementalIterator(rw.(ReaderWithMustIterators), MVCCIncrementalIterOptions{
 		EndKey:               endKey,
 		StartTime:            predicates.StartTime,
 		EndTime:              hlc.MaxTimestamp,
 		RangeKeyMaskingBelow: endTime,
 		KeyTypes:             IterKeyTypePointsAndRanges,
 	})
-	if err != nil {
-		return nil, err
-	}
 	defer iter.Close()
 
 	iter.SeekGE(MVCCKey{Key: startKey})
@@ -3563,15 +3557,12 @@ func MVCCDeleteRangeUsingTombstone(
 	// do a separate time-bound scan for point key conflicts.
 	if msCovered != nil {
 		if err := func() error {
-			iter, err := NewMVCCIncrementalIterator(rw, MVCCIncrementalIterOptions{
+			iter := NewMVCCIncrementalIterator(rw.(ReaderWithMustIterators), MVCCIncrementalIterOptions{
 				KeyTypes:  IterKeyTypePointsOnly,
 				StartKey:  startKey,
 				EndKey:    endKey,
 				StartTime: timestamp.Prev(), // make inclusive
 			})
-			if err != nil {
-				return err
-			}
 			defer iter.Close()
 			iter.SeekGE(MVCCKey{Key: startKey})
 			if ok, err := iter.Valid(); err != nil {
@@ -5753,16 +5744,13 @@ func MVCCGarbageCollectRangeKeys(
 
 			// Verify that there are no remaining data under the deleted range using
 			// time bound iterator.
-			ptIter, err := NewMVCCIncrementalIterator(rw, MVCCIncrementalIterOptions{
+			ptIter := NewMVCCIncrementalIterator(rw.(ReaderWithMustIterators), MVCCIncrementalIterOptions{
 				KeyTypes:     IterKeyTypePointsOnly,
 				StartKey:     rangeKeys.Bounds.Key,
 				EndKey:       rangeKeys.Bounds.EndKey,
 				EndTime:      gcKey.Timestamp,
 				IntentPolicy: MVCCIncrementalIterIntentPolicyEmit,
 			})
-			if err != nil {
-				return err
-			}
 			defer ptIter.Close()
 
 			for ptIter.SeekGE(MVCCKey{Key: rangeKeys.Bounds.Key}); ; ptIter.Next() {
@@ -6535,8 +6523,7 @@ func MVCCIsSpanEmpty(
 			return false, err
 		}
 	} else {
-		var err error
-		iter, err = NewMVCCIncrementalIterator(reader, MVCCIncrementalIterOptions{
+		iter = NewMVCCIncrementalIterator(reader.(ReaderWithMustIterators), MVCCIncrementalIterOptions{
 			KeyTypes:     IterKeyTypePointsAndRanges,
 			StartKey:     opts.StartKey,
 			EndKey:       opts.EndKey,
@@ -6544,9 +6531,6 @@ func MVCCIsSpanEmpty(
 			EndTime:      opts.EndTS,
 			IntentPolicy: MVCCIncrementalIterIntentPolicyEmit,
 		})
-		if err != nil {
-			return false, err
-		}
 	}
 	defer iter.Close()
 	iter.SeekGE(MVCCKey{Key: opts.StartKey})
@@ -6711,7 +6695,7 @@ func mvccExportToWriter(
 	// actually doing the backup work.
 	elasticCPUHandle.StartTimer()
 
-	iter, err := NewMVCCIncrementalIterator(reader, MVCCIncrementalIterOptions{
+	iter := NewMVCCIncrementalIterator(reader.(ReaderWithMustIterators), MVCCIncrementalIterOptions{
 		KeyTypes:             IterKeyTypePointsAndRanges,
 		StartKey:             opts.StartKey.Key,
 		EndKey:               opts.EndKey,
@@ -6720,9 +6704,6 @@ func mvccExportToWriter(
 		RangeKeyMaskingBelow: rangeKeyMasking,
 		IntentPolicy:         MVCCIncrementalIterIntentPolicyAggregate,
 	})
-	if err != nil {
-		return kvpb.BulkOpSummary{}, ExportRequestResumeInfo{}, err
-	}
 	defer iter.Close()
 
 	paginated := opts.TargetSize > 0
diff --git a/pkg/storage/mvcc_history_test.go b/pkg/storage/mvcc_history_test.go
index 3b106a59f4c..a73935526d4 100644
--- a/pkg/storage/mvcc_history_test.go
+++ b/pkg/storage/mvcc_history_test.go
@@ -877,7 +877,7 @@ func cmdTxnUpdate(e *evalCtx) error {
 }
 
 type clearKeyPrintingReadWriter struct {
-	storage.ReadWriter
+	storage.ReadWriterWithMustIterators
 	buf *redact.StringBuilder
 }
 
@@ -885,19 +885,21 @@ func (rw clearKeyPrintingReadWriter) ClearEngineKey(
 	key storage.EngineKey, opts storage.ClearOptions,
 ) error {
 	rw.buf.Printf("called ClearEngineKey(%v)\n", key)
-	return rw.ReadWriter.ClearEngineKey(key, opts)
+	return rw.ReadWriterWithMustIterators.ClearEngineKey(key, opts)
 }
 
 func (rw clearKeyPrintingReadWriter) SingleClearEngineKey(key storage.EngineKey) error {
 	rw.buf.Printf("called SingleClearEngineKey(%v)\n", key)
-	return rw.ReadWriter.SingleClearEngineKey(key)
+	return rw.ReadWriterWithMustIterators.SingleClearEngineKey(key)
 }
 
-func (e *evalCtx) tryWrapForClearKeyPrinting(rw storage.ReadWriter) storage.ReadWriter {
+func (e *evalCtx) tryWrapForClearKeyPrinting(
+	rw storage.ReadWriterWithMustIterators,
+) storage.ReadWriterWithMustIterators {
 	if e.results.traceClearKey {
 		return clearKeyPrintingReadWriter{
-			ReadWriter: rw,
-			buf:        e.results.buf,
+			ReadWriterWithMustIterators: rw,
+			buf:                         e.results.buf,
 		}
 	}
 	return rw
@@ -1782,11 +1784,7 @@ func cmdIterNewIncremental(e *evalCtx) error {
 	}
 
 	r := e.newReader()
-	mvccIter, err := storage.NewMVCCIncrementalIterator(r, opts)
-	if err != nil {
-		return err
-	}
-	it := storage.SimpleMVCCIterator(mvccIter)
+	it := storage.SimpleMVCCIterator(storage.NewMVCCIncrementalIterator(r.(storage.ReaderWithMustIterators), opts))
 	// Can't metamorphically move the iterator around since when intents get aggregated
 	// or emitted we can't undo that later at the level of the metamorphic iterator.
 	if opts.IntentPolicy == storage.MVCCIncrementalIterIntentPolicyError {
@@ -2338,7 +2336,7 @@ func (e *evalCtx) withReader(fn func(storage.Reader) error) error {
 // metamorphically chosen to be a batch, which will be committed and closed when
 // done.
 func (e *evalCtx) withWriter(cmd string, fn func(_ storage.ReadWriter) error) error {
-	var rw storage.ReadWriter
+	var rw storage.ReadWriterWithMustIterators
 	rw = e.engine
 	var batch storage.Batch
 	if e.hasArg("batched") || mvccHistoriesUseBatch {
@@ -2686,7 +2684,7 @@ func (i *iterWithCloser) Close() {
 
 // noopCloseReader overrides Reader.Close() with a noop.
 type noopCloseReader struct {
-	storage.Reader
+	storage.ReaderWithMustIterators
 }
 
 func (noopCloseReader) Close() {}
diff --git a/pkg/storage/mvcc_incremental_iterator.go b/pkg/storage/mvcc_incremental_iterator.go
index 62f55d2b347..dcc41bdd6bb 100644
--- a/pkg/storage/mvcc_incremental_iterator.go
+++ b/pkg/storage/mvcc_incremental_iterator.go
@@ -174,9 +174,11 @@ type MVCCIncrementalIterOptions struct {
 // NewMVCCIncrementalIterator creates an MVCCIncrementalIterator with the
 // specified reader and options. The timestamp hint range should not be more
 // restrictive than the start and end time range.
+//
+// TODO(bilal): Update this method to take a storage.Reader and return an error
 func NewMVCCIncrementalIterator(
-	reader Reader, opts MVCCIncrementalIterOptions,
-) (*MVCCIncrementalIterator, error) {
+	reader ReaderWithMustIterators, opts MVCCIncrementalIterOptions,
+) *MVCCIncrementalIterator {
 	// Default to MaxTimestamp for EndTime, since the code assumes it is set.
 	if opts.EndTime.IsEmpty() {
 		opts.EndTime = hlc.MaxTimestamp
@@ -191,20 +193,16 @@ func NewMVCCIncrementalIterator(
 	}
 
 	var iter MVCCIterator
-	var err error
 	var timeBoundIter MVCCIterator
 	if useTBI {
 		// An iterator without the timestamp hints is created to ensure that the
 		// iterator visits every required version of every key that has changed.
-		iter, err = reader.NewMVCCIterator(MVCCKeyAndIntentsIterKind, IterOptions{
+		iter = reader.MustMVCCIterator(MVCCKeyAndIntentsIterKind, IterOptions{
 			KeyTypes:             opts.KeyTypes,
 			LowerBound:           opts.StartKey,
 			UpperBound:           opts.EndKey,
 			RangeKeyMaskingBelow: opts.RangeKeyMaskingBelow,
 		})
-		if err != nil {
-			return nil, err
-		}
 		// The timeBoundIter is only required to see versioned keys, since the
 		// intents will be found by iter. It can also always enable range key
 		// masking at the start time, since we never care about point keys below it
@@ -214,7 +212,7 @@ func NewMVCCIncrementalIterator(
 		if tbiRangeKeyMasking.LessEq(opts.StartTime) && opts.KeyTypes == IterKeyTypePointsAndRanges {
 			tbiRangeKeyMasking = opts.StartTime.Next()
 		}
-		timeBoundIter, err = reader.NewMVCCIterator(MVCCKeyIterKind, IterOptions{
+		timeBoundIter = reader.MustMVCCIterator(MVCCKeyIterKind, IterOptions{
 			KeyTypes:   opts.KeyTypes,
 			LowerBound: opts.StartKey,
 			UpperBound: opts.EndKey,
@@ -224,20 +222,13 @@ func NewMVCCIncrementalIterator(
 			MaxTimestampHint:     opts.EndTime,
 			RangeKeyMaskingBelow: tbiRangeKeyMasking,
 		})
-		if err != nil {
-			iter.Close()
-			return nil, err
-		}
 	} else {
-		iter, err = reader.NewMVCCIterator(MVCCKeyAndIntentsIterKind, IterOptions{
+		iter = reader.MustMVCCIterator(MVCCKeyAndIntentsIterKind, IterOptions{
 			KeyTypes:             opts.KeyTypes,
 			LowerBound:           opts.StartKey,
 			UpperBound:           opts.EndKey,
 			RangeKeyMaskingBelow: opts.RangeKeyMaskingBelow,
 		})
-		if err != nil {
-			return nil, err
-		}
 	}
 
 	return &MVCCIncrementalIterator{
@@ -246,7 +237,7 @@ func NewMVCCIncrementalIterator(
 		endTime:       opts.EndTime,
 		timeBoundIter: timeBoundIter,
 		intentPolicy:  opts.IntentPolicy,
-	}, nil
+	}
 }
 
 // SeekGE implements SimpleMVCCIterator.
diff --git a/pkg/storage/mvcc_incremental_iterator_test.go b/pkg/storage/mvcc_incremental_iterator_test.go
index 0f6e204309a..ff615f014a9 100644
--- a/pkg/storage/mvcc_incremental_iterator_test.go
+++ b/pkg/storage/mvcc_incremental_iterator_test.go
@@ -102,14 +102,11 @@ func assertExpectErr(
 	revisions bool,
 	expectedIntent roachpb.Intent,
 ) {
-	iter, err := NewMVCCIncrementalIterator(e, MVCCIncrementalIterOptions{
+	iter := NewMVCCIncrementalIterator(e, MVCCIncrementalIterOptions{
 		EndKey:    endKey,
 		StartTime: startTime,
 		EndTime:   endTime,
 	})
-	if err != nil {
-		t.Fatal(err)
-	}
 	defer iter.Close()
 	var iterFn func()
 	if revisions {
@@ -124,7 +121,7 @@ func assertExpectErr(
 		// pass
 	}
 
-	_, err = iter.Valid()
+	_, err := iter.Valid()
 	if lcErr := (*kvpb.LockConflictError)(nil); errors.As(err, &lcErr) {
 		if !expectedIntent.Key.Equal(lcErr.Locks[0].Key) {
 			t.Fatalf("Expected intent key %v, but got %v", expectedIntent.Key, lcErr.Locks[0].Key)
@@ -142,15 +139,12 @@ func assertExpectErrs(
 	revisions bool,
 	expectedIntents []roachpb.Intent,
 ) {
-	iter, err := NewMVCCIncrementalIterator(e, MVCCIncrementalIterOptions{
+	iter := NewMVCCIncrementalIterator(e, MVCCIncrementalIterOptions{
 		EndKey:       endKey,
 		StartTime:    startTime,
 		EndTime:      endTime,
 		IntentPolicy: MVCCIncrementalIterIntentPolicyAggregate,
 	})
-	if err != nil {
-		t.Fatal(err)
-	}
 	defer iter.Close()
 	var iterFn func()
 	if revisions {
@@ -168,7 +162,7 @@ func assertExpectErrs(
 	if iter.NumCollectedIntents() != len(expectedIntents) {
 		t.Fatalf("Expected %d intents but found %d", len(expectedIntents), iter.NumCollectedIntents())
 	}
-	err = iter.TryGetIntentError()
+	err := iter.TryGetIntentError()
 	if lcErr := (*kvpb.LockConflictError)(nil); errors.As(err, &lcErr) {
 		for i := range expectedIntents {
 			if !expectedIntents[i].Key.Equal(lcErr.Locks[i].Key) {
@@ -285,14 +279,11 @@ func ignoreTimeExpectErr(
 	errString string,
 	nextKey bool,
 ) {
-	iter, err := NewMVCCIncrementalIterator(e, MVCCIncrementalIterOptions{
+	iter := NewMVCCIncrementalIterator(e, MVCCIncrementalIterOptions{
 		EndKey:    endKey,
 		StartTime: startTime,
 		EndTime:   endTime,
 	})
-	if err != nil {
-		t.Fatal(err)
-	}
 	var next func()
 	if nextKey {
 		next = iter.NextKeyIgnoringTime
@@ -319,14 +310,11 @@ func assertIgnoreTimeIteratedKVs(
 	expected []MVCCKeyValue,
 	nextKey bool,
 ) {
-	iter, err := NewMVCCIncrementalIterator(e, MVCCIncrementalIterOptions{
+	iter := NewMVCCIncrementalIterator(e, MVCCIncrementalIterOptions{
 		EndKey:    endKey,
 		StartTime: startTime,
 		EndTime:   endTime,
 	})
-	if err != nil {
-		t.Fatal(err)
-	}
 	defer iter.Close()
 	next := func() {
 		if nextKey {
@@ -371,15 +359,12 @@ func assertIteratedKVs(
 	revisions bool,
 	expected []MVCCKeyValue,
 ) {
-	iter, err := NewMVCCIncrementalIterator(e, MVCCIncrementalIterOptions{
+	iter := NewMVCCIncrementalIterator(e, MVCCIncrementalIterOptions{
 		EndKey:       endKey,
 		StartTime:    startTime,
 		EndTime:      endTime,
 		IntentPolicy: MVCCIncrementalIterIntentPolicyAggregate,
 	})
-	if err != nil {
-		t.Fatal(err)
-	}
 	defer iter.Close()
 	var iterFn func()
 	if revisions {
@@ -743,32 +728,30 @@ func TestMVCCIncrementalIteratorInlinePolicy(t *testing.T) {
 		}
 	}
 	t.Run("returns error if inline value is found", func(t *testing.T) {
-		iter, err := NewMVCCIncrementalIterator(e, MVCCIncrementalIterOptions{
+		iter := NewMVCCIncrementalIterator(e, MVCCIncrementalIterOptions{
 			EndKey:    keyMax,
 			StartTime: tsMin,
 			EndTime:   tsMax,
 		})
-		assert.NoError(t, err)
 		defer iter.Close()
 		iter.SeekGE(MakeMVCCMetadataKey(testKey1))
-		_, err = iter.Valid()
+		_, err := iter.Valid()
 		assert.EqualError(t, err, "unexpected inline value found: \"/db1\"")
 	})
 	t.Run("returns error on NextIgnoringTime if inline value is found",
 		func(t *testing.T) {
-			iter, err := NewMVCCIncrementalIterator(e, MVCCIncrementalIterOptions{
+			iter := NewMVCCIncrementalIterator(e, MVCCIncrementalIterOptions{
 				EndKey:    keyMax,
 				StartTime: tsMin,
 				EndTime:   tsMax,
 			})
-			assert.NoError(t, err)
 			defer iter.Close()
 			iter.SeekGE(MakeMVCCMetadataKey(testKey2))
 			expectKeyValue(t, iter, kv2_2_2)
 			iter.NextIgnoringTime()
 			expectKeyValue(t, iter, kv2_1_1)
 			iter.NextIgnoringTime()
-			_, err = iter.Valid()
+			_, err := iter.Valid()
 			assert.EqualError(t, err, "unexpected inline value found: \"/db3\"")
 		})
 }
@@ -818,13 +801,12 @@ func TestMVCCIncrementalIteratorIntentPolicy(t *testing.T) {
 		t.Fatal(err)
 	}
 	t.Run("PolicyError returns error if an intent is in the time range", func(t *testing.T) {
-		iter, err := NewMVCCIncrementalIterator(e, MVCCIncrementalIterOptions{
+		iter := NewMVCCIncrementalIterator(e, MVCCIncrementalIterOptions{
 			EndKey:       keyMax,
 			StartTime:    tsMin,
 			EndTime:      tsMax,
 			IntentPolicy: MVCCIncrementalIterIntentPolicyError,
 		})
-		assert.NoError(t, err)
 		defer iter.Close()
 		iter.SeekGE(MakeMVCCMetadataKey(testKey1))
 		for ; ; iter.Next() {
@@ -832,7 +814,7 @@ func TestMVCCIncrementalIteratorIntentPolicy(t *testing.T) {
 				break
 			}
 		}
-		_, err = iter.Valid()
+		_, err := iter.Valid()
 		assert.EqualError(t, err, lcErr.Error())
 
 		iter.SeekGE(MakeMVCCMetadataKey(testKey1))
@@ -846,13 +828,12 @@ func TestMVCCIncrementalIteratorIntentPolicy(t *testing.T) {
 		}
 	})
 	t.Run("PolicyError ignores intents outside of time range", func(t *testing.T) {
-		iter, err := NewMVCCIncrementalIterator(e, MVCCIncrementalIterOptions{
+		iter := NewMVCCIncrementalIterator(e, MVCCIncrementalIterOptions{
 			EndKey:       keyMax,
 			StartTime:    ts2,
 			EndTime:      tsMax,
 			IntentPolicy: MVCCIncrementalIterIntentPolicyError,
 		})
-		assert.NoError(t, err)
 		defer iter.Close()
 		iter.SeekGE(MakeMVCCMetadataKey(testKey1))
 		expectKeyValue(t, iter, kv1_3_3)
@@ -862,13 +843,12 @@ func TestMVCCIncrementalIteratorIntentPolicy(t *testing.T) {
 		assert.False(t, valid)
 	})
 	t.Run("PolicyEmit returns inline values to caller", func(t *testing.T) {
-		iter, err := NewMVCCIncrementalIterator(e, MVCCIncrementalIterOptions{
+		iter := NewMVCCIncrementalIterator(e, MVCCIncrementalIterOptions{
 			EndKey:       keyMax,
 			StartTime:    tsMin,
 			EndTime:      tsMax,
 			IntentPolicy: MVCCIncrementalIterIntentPolicyEmit,
 		})
-		assert.NoError(t, err)
 		defer iter.Close()
 		testIterWithNextFunc := func(nextFunc func()) {
 			iter.SeekGE(MakeMVCCMetadataKey(testKey1))
@@ -886,13 +866,12 @@ func TestMVCCIncrementalIteratorIntentPolicy(t *testing.T) {
 		testIterWithNextFunc(iter.NextIgnoringTime)
 	})
 	t.Run("PolicyEmit ignores intents outside of time range", func(t *testing.T) {
-		iter, err := NewMVCCIncrementalIterator(e, MVCCIncrementalIterOptions{
+		iter := NewMVCCIncrementalIterator(e, MVCCIncrementalIterOptions{
 			EndKey:       keyMax,
 			StartTime:    ts2,
 			EndTime:      tsMax,
 			IntentPolicy: MVCCIncrementalIterIntentPolicyEmit,
 		})
-		assert.NoError(t, err)
 		defer iter.Close()
 		iter.SeekGE(MakeMVCCMetadataKey(testKey1))
 		expectKeyValue(t, iter, kv1_3_3)
@@ -1109,14 +1088,11 @@ func slurpKVsInTimeRange(
 	reader Reader, prefix roachpb.Key, startTime, endTime hlc.Timestamp,
 ) ([]MVCCKeyValue, error) {
 	endKey := prefix.PrefixEnd()
-	iter, err := NewMVCCIncrementalIterator(reader, MVCCIncrementalIterOptions{
+	iter := NewMVCCIncrementalIterator(reader.(ReaderWithMustIterators), MVCCIncrementalIterOptions{
 		EndKey:    endKey,
 		StartTime: startTime,
 		EndTime:   endTime,
 	})
-	if err != nil {
-		return nil, err
-	}
 	defer iter.Close()
 	var kvs []MVCCKeyValue
 	for iter.SeekGE(MakeMVCCMetadataKey(prefix)); ; iter.Next() {
@@ -1417,14 +1393,11 @@ func TestMVCCIncrementalIteratorIntentStraddlesSStables(t *testing.T) {
 		// 2]. Note that incremental iterators are exclusive on the start time and
 		// inclusive on the end time. The expectation is that we'll see a write
 		// intent error.
-		it, err := NewMVCCIncrementalIterator(db2, MVCCIncrementalIterOptions{
+		it := NewMVCCIncrementalIterator(db2, MVCCIncrementalIterOptions{
 			EndKey:    keys.MaxKey,
 			StartTime: hlc.Timestamp{WallTime: 1},
 			EndTime:   hlc.Timestamp{WallTime: 2},
 		})
-		if err != nil {
-			t.Fatal(err)
-		}
 		defer it.Close()
 		for it.SeekGE(MVCCKey{Key: keys.LocalMax}); ; it.Next() {
 			ok, err := it.Valid()
@@ -1544,14 +1517,11 @@ func runIncrementalBenchmark(b *testing.B, ts hlc.Timestamp, opts mvccBenchData)
 	endKey := roachpb.Key(encoding.EncodeUvarintAscending([]byte("key-"), uint64(opts.numKeys)))
 	b.ResetTimer()
 	for i := 0; i < b.N; i++ {
-		it, err := NewMVCCIncrementalIterator(eng, MVCCIncrementalIterOptions{
+		it := NewMVCCIncrementalIterator(eng, MVCCIncrementalIterOptions{
 			EndKey:    endKey,
 			StartTime: ts,
 			EndTime:   hlc.MaxTimestamp,
 		})
-		if err != nil {
-			b.Fatal(err)
-		}
 		defer it.Close()
 		it.SeekGE(MVCCKey{Key: startKey})
 		for {
@@ -1654,14 +1624,11 @@ func BenchmarkMVCCIncrementalIteratorForOldData(b *testing.B) {
 			b.ResetTimer()
 			for i := 0; i < b.N; i++ {
 				func() {
-					it, err := NewMVCCIncrementalIterator(eng, MVCCIncrementalIterOptions{
+					it := NewMVCCIncrementalIterator(eng, MVCCIncrementalIterOptions{
 						EndKey:    endKey,
 						StartTime: hlc.Timestamp{},
 						EndTime:   hlc.Timestamp{WallTime: baseTimestamp},
 					})
-					if err != nil {
-						b.Fatal(err)
-					}
 					defer it.Close()
 					it.SeekGE(MVCCKey{Key: startKey})
 					for {
diff --git a/pkg/storage/pebble.go b/pkg/storage/pebble.go
index 93498b612e2..f5afc12b018 100644
--- a/pkg/storage/pebble.go
+++ b/pkg/storage/pebble.go
@@ -1500,11 +1500,35 @@ func (p *Pebble) NewMVCCIterator(iterKind MVCCIterKind, opts IterOptions) (MVCCI
 	return maybeWrapInUnsafeIter(iter), nil
 }
 
+// MustMVCCIterator implements the ReaderWithMustIterators interface.
+//
+// If the underlying DB struct in Pebble ever starts returning errors in
+// NewIter(), this method must be removed.
+func (p *Pebble) MustMVCCIterator(iterKind MVCCIterKind, opts IterOptions) MVCCIterator {
+	iter, err := p.NewMVCCIterator(iterKind, opts)
+	if err != nil {
+		panic(err)
+	}
+	return iter
+}
+
 // NewEngineIterator implements the Engine interface.
 func (p *Pebble) NewEngineIterator(opts IterOptions) (EngineIterator, error) {
 	return newPebbleIterator(p.db, opts, StandardDurability, p)
 }
 
+// MustEngineIterator implements the ReaderWithMustIterators interface.
+//
+// If the underlying DB struct in Pebble ever starts returning errors in
+// NewIter(), this method must be removed.
+func (p *Pebble) MustEngineIterator(opts IterOptions) EngineIterator {
+	iter, err := p.NewEngineIterator(opts)
+	if err != nil {
+		panic(err)
+	}
+	return iter
+}
+
 // ScanInternal implements the Engine interface.
 func (p *Pebble) ScanInternal(
 	ctx context.Context,
@@ -2489,6 +2513,18 @@ func (p *pebbleReadOnly) NewMVCCIterator(
 	return maybeWrapInUnsafeIter(iter), nil
 }
 
+// MustMVCCIterator implements the ReaderWithMustIterators interface.
+//
+// If the underlying DB struct in Pebble ever starts returning errors in
+// NewIter(), this method must be removed.
+func (p *pebbleReadOnly) MustMVCCIterator(iterKind MVCCIterKind, opts IterOptions) MVCCIterator {
+	iter, err := p.NewMVCCIterator(iterKind, opts)
+	if err != nil {
+		panic(err)
+	}
+	return iter
+}
+
 // NewEngineIterator implements the Engine interface.
 func (p *pebbleReadOnly) NewEngineIterator(opts IterOptions) (EngineIterator, error) {
 	if p.closed {
@@ -2525,6 +2561,18 @@ func (p *pebbleReadOnly) NewEngineIterator(opts IterOptions) (EngineIterator, er
 	return iter, nil
 }
 
+// MustEngineIterator implements the ReaderWithMustIterators interface.
+//
+// If the underlying DB struct in Pebble ever starts returning errors in
+// NewIter(), this method must be removed.
+func (p *pebbleReadOnly) MustEngineIterator(opts IterOptions) EngineIterator {
+	iter, err := p.NewEngineIterator(opts)
+	if err != nil {
+		panic(err)
+	}
+	return iter
+}
+
 // ConsistentIterators implements the Engine interface.
 func (p *pebbleReadOnly) ConsistentIterators() bool {
 	return true
@@ -2718,11 +2766,35 @@ func (p *pebbleSnapshot) NewMVCCIterator(
 	return maybeWrapInUnsafeIter(MVCCIterator(iter)), nil
 }
 
+// MustMVCCIterator implements the ReaderWithMustIterators interface.
+//
+// If the underlying Snapshot struct in Pebble ever starts returning errors in
+// NewIter(), this method must be removed.
+func (p *pebbleSnapshot) MustMVCCIterator(iterKind MVCCIterKind, opts IterOptions) MVCCIterator {
+	iter, err := p.NewMVCCIterator(iterKind, opts)
+	if err != nil {
+		panic(err)
+	}
+	return iter
+}
+
 // NewEngineIterator implements the Reader interface.
 func (p pebbleSnapshot) NewEngineIterator(opts IterOptions) (EngineIterator, error) {
 	return newPebbleIterator(p.snapshot, opts, StandardDurability, p.parent)
 }
 
+// MustEngineIterator implements the ReaderWithMustIterators interface.
+//
+// If the underlying Snapshot struct in Pebble ever starts returning errors in
+// NewIter(), this method must be removed.
+func (p *pebbleSnapshot) MustEngineIterator(opts IterOptions) EngineIterator {
+	iter, err := p.NewEngineIterator(opts)
+	if err != nil {
+		panic(err)
+	}
+	return iter
+}
+
 // ConsistentIterators implements the Reader interface.
 func (p pebbleSnapshot) ConsistentIterators() bool {
 	return true
diff --git a/pkg/storage/pebble_batch.go b/pkg/storage/pebble_batch.go
index e75e7a762db..7a36949e519 100644
--- a/pkg/storage/pebble_batch.go
+++ b/pkg/storage/pebble_batch.go
@@ -223,6 +223,18 @@ func (p *pebbleBatch) NewMVCCIterator(
 	return maybeWrapInUnsafeIter(iter), nil
 }
 
+// MustMVCCIterator implements the ReaderWithMustIterators interface.
+//
+// If the underlying Batch struct in Pebble ever starts returning errors in
+// NewIter(), this method must be removed.
+func (p *pebbleBatch) MustMVCCIterator(iterKind MVCCIterKind, opts IterOptions) MVCCIterator {
+	iter, err := p.NewMVCCIterator(iterKind, opts)
+	if err != nil {
+		panic(err)
+	}
+	return iter
+}
+
 // NewEngineIterator implements the Batch interface.
 func (p *pebbleBatch) NewEngineIterator(opts IterOptions) (EngineIterator, error) {
 	if p.writeOnly {
@@ -261,6 +273,18 @@ func (p *pebbleBatch) NewEngineIterator(opts IterOptions) (EngineIterator, error
 	return iter, nil
 }
 
+// MustEngineIterator implements the ReaderWithMustIterators interface.
+//
+// If the underlying Batch struct in Pebble ever starts returning errors in
+// NewIter(), this method must be removed.
+func (p *pebbleBatch) MustEngineIterator(opts IterOptions) EngineIterator {
+	iter, err := p.NewEngineIterator(opts)
+	if err != nil {
+		panic(err)
+	}
+	return iter
+}
+
 // ScanInternal implements the Reader interface.
 func (p *pebbleBatch) ScanInternal(
 	ctx context.Context,
diff --git a/pkg/upgrade/upgrades/backfill_job_info_table_migration.go b/pkg/upgrade/upgrades/backfill_job_info_table_migration.go
index 5df0d554df7..63c5875813a 100644
--- a/pkg/upgrade/upgrades/backfill_job_info_table_migration.go
+++ b/pkg/upgrade/upgrades/backfill_job_info_table_migration.go
@@ -45,9 +45,15 @@ const (
 	backfillJobInfoProgressStmt = backfillJobInfoSharedPrefix + jobs.LegacyProgressKey + `', progress` + backfillJobInfoSharedSuffix
 )
 
+// TestingSkipInfoBackfill is a testing hook.
+var TestingSkipInfoBackfill bool
+
 func backfillJobInfoTable(
 	ctx context.Context, cs clusterversion.ClusterVersion, d upgrade.TenantDeps,
 ) error {
+	if TestingSkipInfoBackfill {
+		return nil
+	}
 
 	for step, stmt := range []string{backfillJobInfoPayloadStmt, backfillJobInfoProgressStmt} {
 		var resumeAfter int
diff --git a/pkg/upgrade/upgrades/backfill_job_info_table_migration_test.go b/pkg/upgrade/upgrades/backfill_job_info_table_migration_test.go
index 4aebc75d735..c0cc6d3c2ce 100644
--- a/pkg/upgrade/upgrades/backfill_job_info_table_migration_test.go
+++ b/pkg/upgrade/upgrades/backfill_job_info_table_migration_test.go
@@ -12,7 +12,9 @@ package upgrades_test
 
 import (
 	"context"
+	"fmt"
 	"testing"
+	"time"
 
 	"github.com/cockroachdb/cockroach/pkg/base"
 	_ "github.com/cockroachdb/cockroach/pkg/ccl/backupccl"
@@ -22,6 +24,7 @@ import (
 	"github.com/cockroachdb/cockroach/pkg/jobs/jobspb"
 	"github.com/cockroachdb/cockroach/pkg/security/username"
 	"github.com/cockroachdb/cockroach/pkg/server"
+	"github.com/cockroachdb/cockroach/pkg/settings/cluster"
 	"github.com/cockroachdb/cockroach/pkg/sql"
 	"github.com/cockroachdb/cockroach/pkg/sql/isql"
 	"github.com/cockroachdb/cockroach/pkg/testutils/sqlutils"
@@ -106,3 +109,111 @@ WHERE j.id = i.job_id AND (j.payload = i.value OR j.progress = i.value) AND (j.i
 `,
 		[][]string{{"14"}})
 }
+
+var _ jobs.Resumer = &fakeJob{}
+
+type fakeJob struct {
+	job      *jobs.Job
+	ch1, ch2 chan<- string
+}
+
+func (r *fakeJob) Resume(ctx context.Context, _ interface{}) error {
+	ch := r.ch1
+	if r.job.Progress().Details.(*jobspb.Progress_Import).Import.ResumePos[0] == 2 {
+		ch = r.ch2
+	}
+	select {
+	case ch <- fmt.Sprintf("%s %v",
+		r.job.Details().(jobspb.ImportDetails).BackupPath,
+		r.job.Progress().Details.(*jobspb.Progress_Import).Import.ResumePos):
+		return nil
+	case <-ctx.Done():
+		return ctx.Err()
+	}
+}
+
+func (r *fakeJob) OnFailOrCancel(ctx context.Context, execCtx interface{}, _ error) error {
+	return nil
+}
+
+func TestIncompleteBackfill(t *testing.T) {
+	defer leaktest.AfterTest(t)()
+	defer log.Scope(t).Close(t)
+
+	clusterArgs := base.TestClusterArgs{
+		ServerArgs: base.TestServerArgs{Knobs: base.TestingKnobs{Server: &server.TestingKnobs{
+			DisableAutomaticVersionUpgrade: make(chan struct{}),
+			BootstrapVersionKeyOverride:    clusterversion.V22_2,
+			BinaryVersionOverride:          clusterversion.ByKey(clusterversion.V22_2),
+		}}},
+	}
+
+	ctx := context.Background()
+	tc := testcluster.StartTestCluster(t, 1, clusterArgs)
+	defer tc.Stopper().Stop(ctx)
+
+	r := tc.Server(0).JobRegistry().(*jobs.Registry)
+	sqlDB := sqlutils.MakeSQLRunner(tc.ServerConn(0))
+
+	ch1 := make(chan string, 1)
+	ch2 := make(chan string, 1)
+
+	jobs.RegisterConstructor(
+		jobspb.TypeImport,
+		func(job *jobs.Job, _ *cluster.Settings) jobs.Resumer { return &fakeJob{job: job, ch1: ch1, ch2: ch2} },
+		jobs.UsesTenantCostControl,
+	)
+
+	var adoptedJob, runningJob *jobs.StartableJob
+	runningID, adoptedID := jobspb.JobID(5550001), jobspb.JobID(5550002)
+	require.NoError(t, tc.Server(0).InternalDB().(isql.DB).Txn(ctx, func(
+		ctx context.Context, txn isql.Txn,
+	) (err error) {
+
+		if err := r.CreateStartableJobWithTxn(ctx, &adoptedJob, adoptedID, txn, jobs.Record{
+			Username: username.RootUserName(),
+			Details:  jobspb.ImportDetails{BackupPath: "adopted"},
+			Progress: jobspb.ImportProgress{ResumePos: []int64{1}},
+		}); err != nil {
+			return err
+		}
+
+		if err := r.CreateStartableJobWithTxn(ctx, &runningJob, runningID, txn, jobs.Record{
+			Username: username.RootUserName(),
+			Details:  jobspb.ImportDetails{BackupPath: "running"},
+			Progress: jobspb.ImportProgress{ResumePos: []int64{2}},
+		}); err != nil {
+			return err
+		}
+		return nil
+	}))
+
+	upgrades.TestingSkipInfoBackfill = true
+	defer func() {
+		upgrades.TestingSkipInfoBackfill = false
+	}()
+
+	sqlDB.Exec(t, "SET CLUSTER SETTING version = $1", clusterversion.ByKey(clusterversion.V23_1).String())
+	r.TestingForgetJob(adoptedID)
+	r.NotifyToResume(ctx, adoptedID)
+
+	require.NoError(t, runningJob.Start(ctx))
+	ctx, cancel := context.WithTimeout(ctx, time.Second*5)
+	defer cancel()
+	require.NoError(t, runningJob.AwaitCompletion(ctx))
+
+	select {
+	case res := <-ch1:
+		require.Equal(t, "adopted [1]", res)
+	case <-time.After(time.Second * 5):
+		t.Fatal("timed out waiting for job to run")
+	}
+
+	select {
+	case res := <-ch2:
+		require.Equal(t, "running [2]", res)
+	case <-time.After(time.Second * 5):
+		t.Fatal("timed out waiting for job to run")
+	}
+
+}
